# Alignment
*Here're some resources about Alignment of LLMs, especically to ethics, cultures, politicis, laws in human society*


#### RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness

tag: `RLAIF-V`

paper link: [here](https://arxiv.org/pdf/2405.17220)

github link: [here](https://github.com/RLHF-V/RLAIF-V)

citation:

```bibtex
@misc{yu2024rlaifvaligningmllmsopensource,
      title={RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness}, 
      author={Tianyu Yu and Haoye Zhang and Yuan Yao and Yunkai Dang and Da Chen and Xiaoman Lu and Ganqu Cui and Taiwen He and Zhiyuan Liu and Tat-Seng Chua and Maosong Sun},
      year={2024},
      eprint={2405.17220},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.17220}, 
}
```


#### DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (GRPO)

paper link: [here](https://arxiv.org/pdf/2402.03300.pdf)

github link: [here](https://github.com/deepseek-ai/DeepSeek-Math)


citation:

```bibtex
@misc{shao2024deepseekmath,
      title={DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}, 
      author={Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},
      year={2024},
      eprint={2402.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

#### Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models (SPIN)

paper link: [here](https://arxiv.org/pdf/2401.01335.pdf)

citation:

```bibtex
@misc{chen2024selfplay,
      title={Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models}, 
      author={Zixiang Chen and Yihe Deng and Huizhuo Yuan and Kaixuan Ji and Quanquan Gu},
      year={2024},
      eprint={2401.01335},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Zephyr: Direct Distillation of LM Alignment

paper link: [here](https://arxiv.org/pdf/2310.16944.pdf)

citation: 
```bibtex
@misc{tunstall2023zephyr,
      title={Zephyr: Direct Distillation of LM Alignment}, 
      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Cl√©mentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},
      year={2023},
      eprint={2310.16944},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained Models in Few-Shot Learning

paper link: [here](https://arxiv.org/pdf/2310.15105)

citation: 
```bibtex
@article{song2023fd,
  title={FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained Models in Few-Shot Learning},
  author={Song, Kun and Ma, Huimin and Zou, Bochao and Zhang, Huishuai and Huang, Weiran},
  journal={arXiv preprint arXiv:2310.15105},
  year={2023}
}
```

#### Group Preference Optimization: Few-Shot Alignment of Large Language Models (GPO)

paper link: [here](https://arxiv.org/pdf/2310.11523.pdf)

citation:

```bibtex
@misc{zhao2023group,
      title={Group Preference Optimization: Few-Shot Alignment of Large Language Models}, 
      author={Siyan Zhao and John Dang and Aditya Grover},
      year={2023},
      eprint={2310.11523},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment (P3O)

paper link: [here](https://arxiv.org/pdf/2310.00212.pdf)

citation:

```bibtex
@misc{wu2023pairwise,
      title={Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment}, 
      author={Tianhao Wu and Banghua Zhu and Ruoyu Zhang and Zhaojin Wen and Kannan Ramchandran and Jiantao Jiao},
      year={2023},
      eprint={2310.00212},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```



#### OpenChat: Advancing Open-source Language Models with Mixed-Quality Data (C-RLFT)

paper link: [here](https://arxiv.org/pdf/2309.11235.pdf)

citation:

```bibtex
@article{wang2023openchat,
  title={Openchat: Advancing open-source language models with mixed-quality data},
  author={Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
  journal={arXiv preprint arXiv:2309.11235},
  year={2023}
}
```


#### Fine-Tuning Language Models with Advantage-Induced Policy Alignment (APA)

paper link: [here](https://arxiv.org/abs/2306.02231)

citation:

```bibtex
@misc{zhu2023finetuning,
      title={Fine-Tuning Language Models with Advantage-Induced Policy Alignment}, 
      author={Banghua Zhu and Hiteshi Sharma and Felipe Vieira Frujeri and Shi Dong and Chenguang Zhu and Michael I. Jordan and Jiantao Jiao},
      year={2023},
      eprint={2306.02231},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
    


#### Direct preference optimization: Your language model is secretly a reward model (DPO)

paper link: [here](https://arxiv.org/pdf/2305.18290.pdf)

citation: 
```bibtex
@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}
```

#### Lima: Less is more for alignment

paper link: [here](https://arxiv.org/pdf/2305.11206)

citation: 
```bibtex
@article{zhou2023lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={arXiv preprint arXiv:2305.11206},
  year={2023}
}
```
    

#### RRHF: Rank responses to align language models with human feedback without tears

paper link: [here](https://arxiv.org/pdf/2304.05302)

citation: 
```bibtex
@article{yuan2023rrhf,
  title={Rrhf: Rank responses to align language models with human feedback without tears},
  author={Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei},
  journal={arXiv preprint arXiv:2304.05302},
  year={2023}
}
```

#### Openagi: When llm meets domain experts (RLTF)

paper link: [here](https://arxiv.org/pdf/2304.04370.pdf)

citation: 
```bibtex
@article{ge2023openagi,
  title={Openagi: When llm meets domain experts},
  author={Ge, Yingqiang and Hua, Wenyue and Ji, Jianchao and Tan, Juntao and Xu, Shuyuan and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2304.04370},
  year={2023}
}
```
    
#### Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons

paper link: [here](https://arxiv.org/pdf/2301.11270.pdf)

citation: 
```bibtex
@misc{zhu2023principled,
      title={Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons}, 
      author={Banghua Zhu and Jiantao Jiao and Michael I. Jordan},
      year={2023},a
      eprint={2301.11270},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Constitutional ai: Harmlessness from ai feedback (RLAIF)

paper link: [here](https://arxiv.org/pdf/2212.08073.pdf)

citation: 
```bibtex
@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}
```


#### Scaling Laws for Reward Model Overoptimization

paper link: [here](https://proceedings.mlr.press/v202/gao23h/gao23h.pdf)

citation: 
```bibtex
@inproceedings{gao2023scaling,
  title={Scaling laws for reward model overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={10835--10866},
  year={2023},
  organization={PMLR}
}
```


#### Illustrating Reinforcement Learning from Human Feedback (RLHF)

blog link: [here](https://huggingface.co/blog/rlhf)

citation:

```bibtex
@article{lambert2022illustrating,
  title={Illustrating reinforcement learning from human feedback (RLHF)},
  author={Lambert, Nathan and von Werra, L},
  journal={Hugging Face Blog},
  year={2022}
  url={https://huggingface.co/blog/rlhf}
}
```


#### Fine-tuning language models from human preferences (RLHF)

paper link: [here](https://arxiv.org/pdf/1909.08593.pdf)

citation: 
```bibtex
@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}
```
       


    







