# Automatic Parallelism for LLMs Training
*Here're some resources about Automatic Parallelism for LLMs Training*


#### Enabling Parallelism Hot Switching for Efficient Training of Large Language Models

tag: `Hot Switching` | `SOSP24` | `Peking University`

paper link: [here](https://dl.acm.org/doi/10.1145/3694715.3695969)

code link: [here](https://github.com/PKU-DAIR/Hetu)

citation:

```bibtex
@inproceedings{ge2024enablingparallelismhotswitching,
  author = {Ge, Hao and Fu, Fangcheng and Li, Haoyang and Wang, Xuanyu and Lin, Sheng and Wang, Yujie and Nie, Xiaonan and Zhang, Hailin and Miao, Xupeng and Cui, Bin},
  title = {Enabling Parallelism Hot Switching for Efficient Training of Large Language Models},
  year = {2024},
  isbn = {9798400712517},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3694715.3695969},
  doi = {10.1145/3694715.3695969},
  booktitle = {Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles},
  pages = {178â€“194},
  numpages = {17},
  keywords = {distributed training, large language model, parallelism strategy},
  location = {Austin, TX, USA},
  series = {SOSP '24}
}
```


#### Metis: Fast Automatic Distributed Training on Heterogeneous GPUs

tag: `Metis` | `ATC24` | `Samsung Research`

paper link: [here](https://www.usenix.org/system/files/atc24-um.pdf)

slides link: [here](https://www.usenix.org/system/files/atc24_slides-um.pdf)

citation:

```bibtex
@inproceedings {um2024metis,
  author = {Taegeon Um and Byungsoo Oh and Minyoung Kang and Woo-Yeon Lee and Goeun Kim and Dongseob Kim and Youngtaek Kim and Mohd Muzzammil and Myeongjae Jeon},
  title = {Metis: Fast Automatic Distributed Training on Heterogeneous {GPUs}},
  booktitle = {2024 USENIX Annual Technical Conference (USENIX ATC 24)},
  year = {2024},
  isbn = {978-1-939133-41-0},
  address = {Santa Clara, CA},
  pages = {563--578},
  url = {https://www.usenix.org/conference/atc24/presentation/um},
  publisher = {USENIX Association},
  month = jul
}
```


#### UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming

tag: `UniAP` | `Nanjing University`

paper link: [here](https://arxiv.org/pdf/2307.16375)

citation:

```bibtex
@misc{lin2024uniapunifyinginterintralayer,
      title={UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming}, 
      author={Hao Lin and Ke Wu and Jie Li and Jun Li and Wu-Jun Li},
      year={2024},
      eprint={2307.16375},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.16375}, 
}
```


#### Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning

tag: `Alpa` | `OSDI22` | `Google` | `AWS` | `UC Berkeley` | `SJTU`

paper link: [here](https://www.usenix.org/system/files/osdi22-zheng-lianmin.pdf)

slides link: [here](https://www.usenix.org/sites/default/files/conference/protected-files/osdi22_slides_zheng-lianmin.pdf)

citation:

```bibtex
@inproceedings {zheng2022alpa,
    author = {Lianmin Zheng and Zhuohan Li and Hao Zhang and Yonghao Zhuang and Zhifeng Chen and Yanping Huang and Yida Wang and Yuanzhong Xu and Danyang Zhuo and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica},
    title = {Alpa: Automating Inter- and {Intra-Operator} Parallelism for Distributed Deep Learning},
    booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
    year = {2022},
    isbn = {978-1-939133-28-1},
    address = {Carlsbad, CA},
    pages = {559--578},
    url = {https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin},
    publisher = {USENIX Association},
    month = jul
}
```


#### FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural Networks

tag: `FlexFlow` | `HPCA17` | `UCAS`

paper link: [here](https://ieeexplore.ieee.org/document/7920855)

citation:

```bibtex
@inproceedings{lu2017flexflow,
  author={Lu, Wenyan and Yan, Guihai and Li, Jiajun and Gong, Shijun and Han, Yinhe and Li, Xiaowei},
  booktitle={2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural Networks}, 
  year={2017},
  volume={},
  number={},
  pages={553-564},
  keywords={Parallel processing;Neurons;Computer architecture;Kernel;Clocks;Pipelines;Biological neural networks;Flexible Dataflow;Complementary Effect;Convolutional Neural Networks;Accelerator},
  doi={10.1109/HPCA.2017.29}
}
```