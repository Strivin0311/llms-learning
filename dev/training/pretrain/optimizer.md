# Optimizers for LLMs
*Here're some resources about optimizers for LLMs*


#### AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix

paper link: [here](https://arxiv.org/pdf/2312.01658)

citation:

```bibtex
@misc{yue2023agdautoswitchableoptimizerusing,
      title={AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix}, 
      author={Yun Yue and Zhiling Ye and Jiadi Jiang and Yongchao Liu and Ke Zhang},
      year={2023},
      eprint={2312.01658},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
      url={https://arxiv.org/abs/2312.01658}, 
}
```

#### AdaLomo: Low-memory Optimization with Adaptive Learning Rate

paper link: [here](https://arxiv.org/pdf/2310.10195)

citation:

```bibtex
@misc{lv2023adalomo,
      title={AdaLomo: Low-memory Optimization with Adaptive Learning Rate}, 
      author={Kai Lv and Hang Yan and Qipeng Guo and Haijun Lv and Xipeng Qiu},
      year={2023},
      eprint={2310.10195},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Full Parameter Fine-tuning for Large Language Models with Limited Resources (LOMO)

paper link: [here](https://arxiv.org/pdf/2306.09782)

citation: 
```bibtex
@article{lv2023full,
  title={Full Parameter Fine-tuning for Large Language Models with Limited Resources},
  author={Lv, Kai and Yang, Yuqing and Liu, Tengxiao and Gao, Qinghui and Guo, Qipeng and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2306.09782},
  year={2023}
}
```


#### 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed

paper link: [here](https://arxiv.org/pdf/2102.02888.pdf)

citation:

```bibtex
@misc{tang20211bit,
      title={1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed}, 
      author={Hanlin Tang and Shaoduo Gan and Ammar Ahmad Awan and Samyam Rajbhandari and Conglong Li and Xiangru Lian and Ji Liu and Ce Zhang and Yuxiong He},
      year={2021},
      eprint={2102.02888},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Large Batch Optimization for Deep Learning: Training BERT in 76 minutes (LAMB)

paper link: [here](https://arxiv.org/pdf/1904.00962.pdf)

citation:

```bibtex
@misc{you2020largebatchoptimizationdeep,
      title={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes}, 
      author={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},
      year={2020},
      eprint={1904.00962},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1904.00962}, 
}
```


#### Why AdamW matters

blog link: [here](https://towardsdatascience.com/why-adamw-matters-736223f31b5d)

citation:

```bibtex
@misc{graetz2018whyadamwmatters,
  author = {Fabio M. Graetz},
  title = {Why AdamW matters},
  year = {2018},
  howpublished = {\url{https://towardsdatascience.com/why-adamw-matters-736223f31b5d}},
}
```


#### Adafactor: Adaptive Learning Rates with Sublinear Memory Cost

paper link: [here](https://arxiv.org/pdf/1804.04235)

citation:

```bibtex
@misc{shazeer2018adafactoradaptivelearningrates,
      title={Adafactor: Adaptive Learning Rates with Sublinear Memory Cost}, 
      author={Noam Shazeer and Mitchell Stern},
      year={2018},
      eprint={1804.04235},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1804.04235}, 
}
```