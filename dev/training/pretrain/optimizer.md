# Optimization Algorithms for LLMs
*Here're some resources about Optimization Algorithms for LLMs*
*Note that most of them are widely-used in general machine learning / deep learning as well*

## Methods


#### AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix

tag: `AGD` | `NIPS23` | `AntGroup`

paper link: [here](https://arxiv.org/pdf/2312.01658)

github link: [here](https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers)

citation:

```bibtex
@misc{yue2023agdautoswitchableoptimizerusing,
      title={AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix}, 
      author={Yun Yue and Zhiling Ye and Jiadi Jiang and Yongchao Liu and Ke Zhang},
      year={2023},
      eprint={2312.01658},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
      url={https://arxiv.org/abs/2312.01658}, 
}
```

#### AdaLomo: Low-memory Optimization with Adaptive Learning Rate

tag: `AdaLomo` | `Shanghai AILab` | `Fudan University`

paper link: [here](https://arxiv.org/pdf/2310.10195)

citation:

```bibtex
@misc{lv2023adalomo,
      title={AdaLomo: Low-memory Optimization with Adaptive Learning Rate}, 
      author={Kai Lv and Hang Yan and Qipeng Guo and Haijun Lv and Xipeng Qiu},
      year={2023},
      eprint={2310.10195},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Full Parameter Fine-tuning for Large Language Models with Limited Resources

tag: `LOMO` | `OpenLMLab` | `Fudan University`

paper link: [here](https://arxiv.org/pdf/2306.09782)

github link: [here](https://github.com/OpenLMLab/LOMO)

citation:

```bibtex
@misc{lv2024parameterfinetuninglargelanguage,
      title={Full Parameter Fine-tuning for Large Language Models with Limited Resources}, 
      author={Kai Lv and Yuqing Yang and Tengxiao Liu and Qinghui Gao and Qipeng Guo and Xipeng Qiu},
      year={2024},
      eprint={2306.09782},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.09782}, 
}
```


#### 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed

tag: `1-bit Adam` | `Microsoft`

paper link: [here](https://arxiv.org/pdf/2102.02888.pdf)

github link: [here](https://github.com/microsoft/DeepSpeed)

citation:

```bibtex
@misc{tang20211bit,
      title={1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed}, 
      author={Hanlin Tang and Shaoduo Gan and Ammar Ahmad Awan and Samyam Rajbhandari and Conglong Li and Xiangru Lian and Ji Liu and Ce Zhang and Yuxiong He},
      year={2021},
      eprint={2102.02888},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Large Batch Optimization for Deep Learning: Training BERT in 76 minutes

tag: `LAMB` | `Google`

paper link: [here](https://arxiv.org/pdf/1904.00962.pdf)

git link: [here](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py)

citation:

```bibtex
@misc{you2020largebatchoptimizationdeep,
      title={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes}, 
      author={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},
      year={2020},
      eprint={1904.00962},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1904.00962}, 
}
```


#### Why AdamW matters

tag: `AdamW`

blog link: [here](https://towardsdatascience.com/why-adamw-matters-736223f31b5d)

citation:

```bibtex
@misc{graetz2018whyadamwmatters,
  author = {Fabio M. Graetz},
  title = {Why AdamW matters},
  year = {2018},
  howpublished = {\url{https://towardsdatascience.com/why-adamw-matters-736223f31b5d}},
}
```


#### Adafactor: Adaptive Learning Rates with Sublinear Memory Cost

tag: `Adafactor` | `Google Brain`

paper link: [here](https://arxiv.org/pdf/1804.04235)

citation:

```bibtex
@misc{shazeer2018adafactoradaptivelearningrates,
      title={Adafactor: Adaptive Learning Rates with Sublinear Memory Cost}, 
      author={Noam Shazeer and Mitchell Stern},
      year={2018},
      eprint={1804.04235},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1804.04235}, 
}
```


#### AdamW - Decoupled Weight Decay Regularization

tag: `AdamW` | `SGDW` | `Weight Decay`

paper link: [here](https://arxiv.org/pdf/1711.05101)

git link: [here](https://github.com/loshchil/AdamW-and-SGDW)

citation:

```bibtex
@misc{loshchilov2019decoupled,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Incorporating Nesterov Momentum into Adam

tag: `Nadam` | `NAG` | `Adam` | `ICLR16` | `Stanford University`

paper link: [here](https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf)

citation:

```bibtex
@inproceedings{dozat2016incorporating,
      title = {Incorporating {Nesterov Momentum into Adam}},
      author = {Dozat, Timothy},
      year = {2016},
      booktitle = {Proceedings of the 4th International Conference on Learning Representations},
      pages = {1--4},
}
```


#### Adam: A Method for Stochastic Optimization

tag: `Adam` | `AdaMax` | `OpenAI`

paper link: [here](https://arxiv.org/pdf/1412.6980.pdf)

citation:

```bibtex
@misc{kingma2017adammethodstochasticoptimization,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}
```


#### NeuralNetworks for Machine Learning Lecture 6a: Overview of mini-batch gradient descent

tag: `RMSProp` | `Hinton`

slides link: [here](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)

citation:

```bibtex
@misc{Hinton2012Lecture6a,
  author = {Geoffrey Hinton and Nitish Srivastava and Kevin Swersky},
  title = {Neural Networks for Machine Learning: Lecture 6a-e, Overview of mini-batch gradient descent and advanced optimization methods},
  year = {2012},
  howpublished = {\url{https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}},
  note = {Accessed: 2024-10-27}
}
```


#### ADADELTA: An Adaptive Learning Rate Method

tag: `AdaDelta` | `Google`

paper link: [here](https://arxiv.org/pdf/1212.5701)

citation:

```bibtex
@article{zeiler2012adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}
```


#### Adaptive Subgradient Methods for Online Learning and Stochastic Optimization

tag: `Adagrad` | `JMLR11`

paper link: [here](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)

citation:

```bibtex
@article{10.5555/1953048.2021068,
      author = {Duchi, John and Hazan, Elad and Singer, Yoram},
      title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
      year = {2011},
      issue_date = {2/1/2011},
      publisher = {JMLR.org},
      volume = {12},
      number = {null},
      issn = {1532-4435},
      journal = {J. Mach. Learn. Res.},
      month = jul,
      pages = {2121â€“2159},
      numpages = {39}
}
```


## Survey


#### An overview of gradient descent optimization algorithms

tag: `Gradient Descent Survey` | `GD` | `BGD` | `SGD` | `MBGD` | `Momentum` | `NAG` | `Adagrad` | `Adadelta` | `RMSprop` | `Adam` | `AdaMax` | `Nadam`
 
paper link: [here](https://arxiv.org/pdf/1609.04747)

citation:

```bibtex
@article{ruder2016overview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}
```