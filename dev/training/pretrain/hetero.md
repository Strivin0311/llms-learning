# Heterogeneous Training for LLMs
*Here're some resources about Heterogeneous Training in LLMs*
*Note that the "heterogeneous" here is a general term, including the heterogeneity of devices, models, data, etc*


#### FlashFlex: Accommodating Large Language Model Training over Heterogeneous Environment

tag: `FlashFlex` | `HKUST` | `Peking University`

paper link: [here](https://arxiv.org/pdf/2409.01143)

code link: [here](https://github.com/Relaxed-System-Lab/FlashFlex)

citation:

```bibtex
@misc{yan2024flashflexaccommodatinglargelanguage,
      title={FlashFlex: Accommodating Large Language Model Training over Heterogeneous Environment}, 
      author={Ran Yan and Youhe Jiang and Wangcheng Tao and Xiaonan Nie and Bin Cui and Binhang Yuan},
      year={2024},
      eprint={2409.01143},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2409.01143}, 
}
```


#### Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning

tag: `Fire-Flyer` | `AI-HPC` | `SC24` | `Deepseek`

paper link: [here](https://arxiv.org/pdf/2408.14158)

code link: [here](https://github.com/deepseek-ai/3FS)

citation:

```bibtex
@misc{an2024fireflyeraihpccosteffectivesoftwarehardware,
      title={Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning}, 
      author={Wei An and Xiao Bi and Guanting Chen and Shanhuang Chen and Chengqi Deng and Honghui Ding and Kai Dong and Qiushi Du and Wenjun Gao and Kang Guan and Jianzhong Guo and Yongqiang Guo and Zhe Fu and Ying He and Panpan Huang and Jiashi Li and Wenfeng Liang and Xiaodong Liu and Xin Liu and Yiyuan Liu and Yuxuan Liu and Shanghao Lu and Xuan Lu and Xiaotao Nie and Tian Pei and Junjie Qiu and Hui Qu and Zehui Ren and Zhangli Sha and Xuecheng Su and Xiaowen Sun and Yixuan Tan and Minghui Tang and Shiyu Wang and Yaohui Wang and Yongji Wang and Ziwei Xie and Yiliang Xiong and Yanhong Xu and Shengfeng Ye and Shuiping Yu and Yukun Zha and Liyue Zhang and Haowei Zhang and Mingchuan Zhang and Wentao Zhang and Yichao Zhang and Chenggang Zhao and Yao Zhao and Shangyan Zhou and Shunfeng Zhou and Yuheng Zou},
      year={2024},
      eprint={2408.14158},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2408.14158}, 
}
```


#### DistTrain: Addressing Model and Data Heterogeneity with Disaggregated Training for Multimodal Large Language Models

tag: `DistTrain` | `MMLM` | `StepFun` | `Peking University`

paper link: [here](https://arxiv.org/pdf/2408.04275)

citation:

```bibtex
@misc{zhang2024disttrainaddressingmodeldata,
      title={DistTrain: Addressing Model and Data Heterogeneity with Disaggregated Training for Multimodal Large Language Models}, 
      author={Zili Zhang and Yinmin Zhong and Ranchen Ming and Hanpeng Hu and Jianjian Sun and Zheng Ge and Yibo Zhu and Xin Jin},
      year={2024},
      eprint={2408.04275},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2408.04275}, 
}
```


#### Whale: Efficient Giant Model Training over Heterogeneous GPUs

tag: `Whale` | `ATC22` | `Alibaba Group` | `NUS` 

paper link: [here](https://www.usenix.org/system/files/atc22-jia-xianyan.pdf)

slides link: [here](https://www.usenix.org/sites/default/files/conference/protected-files/atc22_slides_jia_xianyan.pdf)

citation:

```bibtex
@inproceedings {jia2022whale,
    author = {Xianyan Jia and Le Jiang and Ang Wang and Wencong Xiao and Ziji Shi and Jie Zhang and Xinyuan Li and Langshi Chen and Yong Li and Zhen Zheng and Xiaoyong Liu and Wei Lin},
    title = {Whale: Efficient Giant Model Training over Heterogeneous {GPUs}},
    booktitle = {2022 USENIX Annual Technical Conference (USENIX ATC 22)},
    year = {2022},
    isbn = {978-1-939133-29-57},
    address = {Carlsbad, CA},
    pages = {673--688},
    url = {https://www.usenix.org/conference/atc22/presentation/jia-xianyan},
    publisher = {USENIX Association},
    month = jul
}
```


#### Heterogeneity-Aware Distributed Machine Learning Training via Partial Reduce

tag: `Partial Reduce` | `SIGMOD21` | `Peking University`

paper link: [here](https://hsword.github.io/assets/pdf/sigmod2021-preduce.pdf)

citation:

```bibtex
@inproceedings{miao2021heterogeneityaware,
      author = {Miao, Xupeng and Nie, Xiaonan and Shao, Yingxia and Yang, Zhi and Jiang, Jiawei and Ma, Lingxiao and Cui, Bin},
      title = {Heterogeneity-Aware Distributed Machine Learning Training via Partial Reduce},
      year = {2021},
      isbn = {9781450383431},
      publisher = {Association for Computing Machinery},
      address = {New York, NY, USA},
      url = {https://doi.org/10.1145/3448016.3452773},
      doi = {10.1145/3448016.3452773},
      pages = {2262â€“2270},
      numpages = {9},
      keywords = {all-reduce, distributed machine learning, heterogeneity},
      location = {Virtual Event, China},
      series = {SIGMOD '21}
}
```