# Automatic Mixed-Precision (AMP) Training for LLMs
*Here're some resources about Automatic Mixed-Precision strategies for LLMs training for LLMs*

#### eXmY: A Data Type and Technique for Arbitrary Bit Precision Quantization

paper link: [here](https://arxiv.org/pdf/2405.13938)

citation:

```bibtex
@misc{agrawal2024exmydatatypetechnique,
      title={eXmY: A Data Type and Technique for Arbitrary Bit Precision Quantization}, 
      author={Aditya Agrawal and Matthew Hedlund and Blake Hechtman},
      year={2024},
      eprint={2405.13938},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.13938}, 
}
```


#### FP8-LM: Training FP8 Large Language Models

paper link: [here](https://arxiv.org/pdf/2310.18313.pdf)

github link: [here](https://github.com/Azure/MS-AMP)

citation:

```bibtex
@misc{peng2023fp8lm,
      title={FP8-LM: Training FP8 Large Language Models}, 
      author={Houwen Peng and Kan Wu and Yixuan Wei and Guoshuai Zhao and Yuxiang Yang and Ze Liu and Yifan Xiong and Ziyue Yang and Bolin Ni and Jingcheng Hu and Ruihang Li and Miaosen Zhang and Chen Li and Jia Ning and Ruizhe Wang and Zheng Zhang and Shuguang Liu and Joe Chau and Han Hu and Peng Cheng},
      year={2023},
      eprint={2310.18313},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### BitNet: Scaling 1-bit Transformers for Large Language Models

paper link: [here](https://arxiv.org/pdf/2310.11453)

blog link: [here](https://thegenerality.com/agi/)

github link: [here](https://github.com/microsoft/unilm)

citation:

```bibtex
@misc{wang2023bitnet,
      title={BitNet: Scaling 1-bit Transformers for Large Language Models}, 
      author={Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei},
      year={2023},
      eprint={2310.11453},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### Training and inference of large language models using 8-bit floating point

paper link: [here](https://arxiv.org/pdf/2309.17224)

citation:

```bibtex
@misc{perez2023training,
      title={Training and inference of large language models using 8-bit floating point}, 
      author={Sergio P. Perez and Yan Zhang and James Briggs and Charlie Blake and Josh Levy-Kramer and Paul Balanca and Carlo Luschi and Stephen Barlow and Andrew William Fitzgibbon},
      year={2023},
      eprint={2309.17224},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Unit Scaling: Out-of-the-Box Low-Precision Training

paper link: [here](https://arxiv.org/pdf/2303.11257)

citation:

```bibtex
@misc{blake2023unitscalingoutoftheboxlowprecision,
      title={Unit Scaling: Out-of-the-Box Low-Precision Training}, 
      author={Charlie Blake and Douglas Orr and Carlo Luschi},
      year={2023},
      eprint={2303.11257},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.11257}, 
}
```


#### NVIDIA Transformer Engine: Accelerating PyTorch Training Workloads with FP8 (TE)

blog link: [here](https://towardsdatascience.com/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7)

docs link: [here](https://docs.nvidia.com/deeplearning/transformer-engine/index.html)

github link: [here](https://github.com/NVIDIA/TransformerEngine)

citation:

```bibtex
@misc{NVIDIA2023TransformerEngine,
  title={NVIDIA Transformer Engine: Accelerating PyTorch Training Workloads with FP8 (TE)},
  author={Chaim Rand, and NVIDIA},
  howpublished = {\url{https://github.com/NVIDIA/TransformerEngine}},
  year={2023},
}
```


#### FP8 Quantization: The Power of the Exponent

paper link: [here](https://arxiv.org/pdf/2208.09225.pdf)

github link: [here](https://github.com/Qualcomm-AI-research/FP8-quantization)

citation:

```bibtex
@misc{kuzmin2024fp8,
      title={FP8 Quantization: The Power of the Exponent}, 
      author={Andrey Kuzmin and Mart Van Baalen and Yuwei Ren and Markus Nagel and Jorn Peters and Tijmen Blankevoort},
      year={2024},
      eprint={2208.09225},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### 8-bit Optimizers via Block-wise Quantization

paper link: [here](https://arxiv.org/pdf/2110.02861)

citation:

```bibtex
@misc{dettmers20228bit,
      title={8-bit Optimizers via Block-wise Quantization}, 
      author={Tim Dettmers and Mike Lewis and Sam Shleifer and Luke Zettlemoyer},
      year={2022},
      eprint={2110.02861},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks (HFP8)

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2019/file/65fc9fb4897a89789352e211ca2d398f-Paper.pdf)

citation:

```bibtex
@inproceedings{NEURIPS2019_65fc9fb4,
      author = {Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi (Viji) and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
      pages = {},
      publisher = {Curran Associates, Inc.},
      title = {Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks},
      url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/65fc9fb4897a89789352e211ca2d398f-Paper.pdf},
      volume = {32},
      year = {2019}
}  
```


#### Training Deep Neural Networks with 8-bit Floating Point Numbers (FP8)

paper link: [here](https://arxiv.org/pdf/1812.08011)

citation:

```bibtex
@misc{wang2018training,
      title={Training Deep Neural Networks with 8-bit Floating Point Numbers}, 
      author={Naigang Wang and Jungwook Choi and Daniel Brand and Chia-Yu Chen and Kailash Gopalakrishnan},
      year={2018},
      eprint={1812.08011},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Mixed Precision Training (FP16)

paper link: [here](https://arxiv.org/pdf/1710.03740)

citation:

```bibtex
@misc{micikevicius2018mixed,
      title={Mixed Precision Training}, 
      author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
      year={2018},
      eprint={1710.03740},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
```