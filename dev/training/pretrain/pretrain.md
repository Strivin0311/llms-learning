# Pretraining for LLMs
*Here're some resources about pretraining methodologies for LLMs*


## Efficient Pretraining


#### OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training

paper link: [here](https://arxiv.org/pdf/2407.07852v1)

github link: [here](https://github.com/PrimeIntellect-ai/OpenDiLoCo)

citation:

```bibtex
@misc{jaghouar2024opendilocoopensourceframeworkglobally,
      title={OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training}, 
      author={Sami Jaghouar and Jack Min Ong and Johannes Hagemann},
      year={2024},
      eprint={2407.07852},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.07852}, 
}
```

#### Flash Checkpoint to Recover Large Model Training From Failure in Seconds

blog link: [here](https://github.com/intelligent-machine-learning/dlrover/blob/master/docs/blogs/flash_checkpoint.md)

github link: [here](https://github.com/intelligent-machine-learning/dlrover)

citation:

```bibtex
@misc{zhang2022flash,
      title={Flash Checkpoint to Recover Large Model Training From Failure in Seconds}, 
      author={Qinlong Wang},
      year={2024},
      howpublished={https://github.com/intelligent-machine-learning/dlrover/blob/master/docs/blogs/flash_checkpoint.md},
}
```

#### MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs

paper link: [here](https://arxiv.org/pdf/2402.15627)

github link: [here](https://github.com/volcengine/veScale)

citation:

```bibtex
@misc{jiang2024megascalescalinglargelanguage,
      title={MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs}, 
      author={Ziheng Jiang and Haibin Lin and Yinmin Zhong and Qi Huang and Yangrui Chen and Zhi Zhang and Yanghua Peng and Xiang Li and Cong Xie and Shibiao Nong and Yulu Jia and Sun He and Hongmin Chen and Zhihao Bai and Qi Hou and Shipeng Yan and Ding Zhou and Yiyao Sheng and Zhuo Jiang and Haohan Xu and Haoran Wei and Zhang Zhang and Pengfei Nie and Leqi Zou and Sida Zhao and Liang Xiang and Zherui Liu and Zhe Li and Xiaoying Jia and Jianxi Ye and Xin Jin and Xin Liu},
      year={2024},
      eprint={2402.15627},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.15627}, 
}
```


#### Efficient Online Data Mixing For Language Model Pre-Training

paper link: [here](https://arxiv.org/pdf/2312.02406.pdf)

citation:

```bibtex
@misc{albalak2023efficient,
      title={Efficient Online Data Mixing For Language Model Pre-Training}, 
      author={Alon Albalak and Liangming Pan and Colin Raffel and William Yang Wang},
      year={2023},
      eprint={2312.02406},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### DiLoCo: Distributed Low-Communication Training of Language Models

paper link: [here](https://arxiv.org/pdf/2311.08105)

open-source version: [here](https://arxiv.org/pdf/2407.07852v1)

citation:

```bibtex
@misc{douillard2023diloco,
      title={DiLoCo: Distributed Low-Communication Training of Language Models}, 
      author={Arthur Douillard and Qixuan Feng and Andrei A. Rusu and Rachita Chhaparia and Yani Donchev and Adhiguna Kuncoro and Marc'Aurelio Ranzato and Arthur Szlam and Jiajun Shen},
      year={2023},
      eprint={2311.08105},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models

paper link: [here](https://openreview.net/pdf?id=dWDEBW2raJ)

citation:

```bibtex
@inproceedings{shi2023train,
  title={Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models},
  author={Shi, Yubin and Chen, Yixuan and Dong, Mingzhi and Yang, Xiaochen and Li, Dongsheng and Wang, Yujiang and Dick, Robert P and Lv, Qin and Zhao, Yingying and Yang, Fan and others},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}
```


#### Transcending Scaling Laws with 0.1% Extra Compute (UL2R, U-PaLM)

paper link: [here](https://arxiv.org/pdf/2210.11399.pdf)

citation:

```bibtex
@misc{tay2022transcending,
      title={Transcending Scaling Laws with 0.1% Extra Compute}, 
      author={Yi Tay and Jason Wei and Hyung Won Chung and Vinh Q. Tran and David R. So and Siamak Shakeri and Xavier Garcia and Huaixiu Steven Zheng and Jinfeng Rao and Aakanksha Chowdhery and Denny Zhou and Donald Metzler and Slav Petrov and Neil Houlsby and Quoc V. Le and Mostafa Dehghani},
      year={2022},
      eprint={2210.11399},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
    

#### 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed

paper link: [here](https://arxiv.org/pdf/2102.02888.pdf)

citation:

```bibtex
@misc{tang20211bit,
      title={1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed}, 
      author={Hanlin Tang and Shaoduo Gan and Ammar Ahmad Awan and Samyam Rajbhandari and Conglong Li and Xiangru Lian and Ji Liu and Ce Zhang and Yuxiong He},
      year={2021},
      eprint={2102.02888},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### Efficient Algorithms for Device Placement of DNN Graph Operators

paper link: [here](https://arxiv.org/pdf/2006.16423)

citation:

```bibtex
@misc{tarnawski2020efficient,
      title={Efficient Algorithms for Device Placement of DNN Graph Operators}, 
      author={Jakub Tarnawski and Amar Phanishayee and Nikhil R. Devanur and Divya Mahajan and Fanny Nina Paravecino},
      year={2020},
      eprint={2006.16423},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Communication-Efficient Distributed Deep Learning: A Comprehensive Survey

paper link: [here](https://arxiv.org/pdf/2003.06307)

citation:

```bibtex
@misc{tang2023communicationefficientdistributeddeeplearning,
      title={Communication-Efficient Distributed Deep Learning: A Comprehensive Survey}, 
      author={Zhenheng Tang and Shaohuai Shi and Wei Wang and Bo Li and Xiaowen Chu},
      year={2023},
      eprint={2003.06307},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2003.06307}, 
}
```


#### The reversible residual network: Backpropagation without storing activations (RevNet)

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2017/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf)

citation:

```bibtex
@article{gomez2017reversible,
  title={The reversible residual network: Backpropagation without storing activations},
  author={Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
```


#### Training deep nets with sublinear memory cost

paper link: [here](https://arxiv.org/pdf/1604.06174)

citation:

```bibtex
@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}
```


## Effective Pretraining


#### Pre-training Small Base LMs with Fewer Tokens

paper link: [here](https://arxiv.org/pdf/2404.08634v1)

github link: [here](https://github.com/Lightning-AI/lit-gpt)

citation:

```bibtex
@misc{sanyal2024pretrainingsmallbaselms,
      title={Pre-training Small Base LMs with Fewer Tokens}, 
      author={Sunny Sanyal and Sujay Sanghavi and Alexandros G. Dimakis},
      year={2024},
      eprint={2404.08634},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.08634}, 
}
```

#### Spike No More: Stabilizing the Pre-training of Large Language Models

paper link: [here](https://arxiv.org/pdf/2312.16903)

citation:

```bibtex
@misc{takase2024spike,
      title={Spike No More: Stabilizing the Pre-training of Large Language Models}, 
      author={Sho Takase and Shun Kiyono and Sosuke Kobayashi and Jun Suzuki},
      year={2024},
      eprint={2312.16903},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### Skill-it! A data-driven skills framework for understanding and training language models

paper link: [here](https://arxiv.org/pdf/2307.14430)

citation:

```bibtex
@article{chen2023skill,
  title={Skill-it! A data-driven skills framework for understanding and training language models},
  author={Chen, Mayee F and Roberts, Nicholas and Bhatia, Kush and Wang, Jue and Zhang, Ce and Sala, Frederic and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2307.14430},
  year={2023}
}
```


#### Training compute-optimal large language models

paper link: [here](https://arxiv.org/pdf/2203.15556)

citation:

```bibtex
@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}
```
    

#### Selfdoc: Self-supervised document representation learning

paper link: [here](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_SelfDoc_Self-Supervised_Document_Representation_Learning_CVPR_2021_paper.pdf)

citation:

```bibtex
@inproceedings{li2021selfdoc,
  title={Selfdoc: Self-supervised document representation learning},
  author={Li, Peizhao and Gu, Jiuxiang and Kuen, Jason and Morariu, Vlad I and Zhao, Handong and Jain, Rajiv and Manjunatha, Varun and Liu, Hongfu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5652--5660},
  year={2021}
}
```


#### Scaling Laws for Neural Language Models

paper link: [here](https://arxiv.org/pdf/2001.08361)

citation:

```bibtex
@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}
```

#### An Empirical Model of Large-Batch Training

paper link: [here](https://arxiv.org/pdf/1812.06162)

citation:

```bibtex
@misc{mccandlish2018empirical,
      title={An Empirical Model of Large-Batch Training}, 
      author={Sam McCandlish and Jared Kaplan and Dario Amodei and OpenAI Dota Team},
      year={2018},
      eprint={1812.06162},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


## Pretraining Corpus


### Universal


#### DataComp-LM: In search of the next generation of training sets for language models (DCLM)

paper link: [here](https://arxiv.org/pdf/2406.11794v3)

github link: [here](https://github.com/mlfoundations/dclm)

citation:

```bibtex
@misc{li2024datacomplmsearchgenerationtraining,
      title={DataComp-LM: In search of the next generation of training sets for language models}, 
      author={Jeffrey Li and Alex Fang and Georgios Smyrnis and Maor Ivgi and Matt Jordan and Samir Gadre and Hritik Bansal and Etash Guha and Sedrick Keh and Kushal Arora and Saurabh Garg and Rui Xin and Niklas Muennighoff and Reinhard Heckel and Jean Mercat and Mayee Chen and Suchin Gururangan and Mitchell Wortsman and Alon Albalak and Yonatan Bitton and Marianna Nezhurina and Amro Abbas and Cheng-Yu Hsieh and Dhruba Ghosh and Josh Gardner and Maciej Kilian and Hanlin Zhang and Rulin Shao and Sarah Pratt and Sunny Sanyal and Gabriel Ilharco and Giannis Daras and Kalyani Marathe and Aaron Gokaslan and Jieyu Zhang and Khyathi Chandu and Thao Nguyen and Igor Vasiljevic and Sham Kakade and Shuran Song and Sujay Sanghavi and Fartash Faghri and Sewoong Oh and Luke Zettlemoyer and Kyle Lo and Alaaeldin El-Nouby and Hadi Pouransari and Alexander Toshev and Stephanie Wang and Dirk Groeneveld and Luca Soldaini and Pang Wei Koh and Jenia Jitsev and Thomas Kollar and Alexandros G. Dimakis and Yair Carmon and Achal Dave and Ludwig Schmidt and Vaishaal Shankar},
      year={2024},
      eprint={2406.11794},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.11794}, 
}
```

#### Does your data spark joy? Performance gains from domain upsampling at the end of training

paper link: [here](https://arxiv.org/pdf/2406.03476)

citation:

```bibtex
@misc{blakeney2024doesdatasparkjoy,
      title={Does your data spark joy? Performance gains from domain upsampling at the end of training}, 
      author={Cody Blakeney and Mansheej Paul and Brett W. Larsen and Sean Owen and Jonathan Frankle},
      year={2024},
      eprint={2406.03476},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.03476}, 
}
```

#### RedPajama: an Open Dataset for Training Large Language Models

blog link: [here](https://together.ai/blog/redpajama-data-v2)

github link: [here](https://github.com/togethercomputer/RedPajama-Data)

dataset link: [here](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)

citation:

```bibtex
@software{together2023redpajama,
  author = {Together Computer},
  title = {RedPajama: an Open Dataset for Training Large Language Models},
  month = October,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}
```


#### The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2022/file/ce9e92e3de2372a4b93353eb7f3dc0bd-Paper-Datasets_and_Benchmarks.pdf)

github link: [here](https://github.com/bigscience-workshop/data-preparation)

dataset link: [here](https://huggingface.co/bigscience-data)

citation:

```bibtex
@article{laurenccon2022bigscience,
  title={The bigscience roots corpus: A 1.6 tb composite multilingual dataset},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Villanova del Moral, Albert and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Gonz{\'a}lez Ponferrada, Eduardo and Nguyen, Huu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31809--31826},
  year={2022}
}
```


#### The Pile: An 800GB Dataset of Diverse Text for Language Modeling

paper link: [here](https://arxiv.org/pdf/2101.00027.pdf)

github link: [here](https://github.com/EleutherAI/the-pile)

dataset link: [here](https://the-eye.eu/public/AI/pile/)

citation:

```bibtex
@misc{gao2020pile,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```



### Math

#### Generative AI for Math: Part I -- MathPile: A Billion-Token-Scale Pretraining Corpus for Math

paper link: [here](https://arxiv.org/pdf/2312.17120)

github link: [here](https://github.com/GAIR-NLP/MathPile/)

dataset link: [here](https://huggingface.co/datasets/GAIR/MathPile)

citation:

```bibtex
@misc{wang2023generative,
      title={Generative AI for Math: Part I -- MathPile: A Billion-Token-Scale Pretraining Corpus for Math}, 
      author={Zengzhi Wang and Rui Xia and Pengfei Liu},
      year={2023},
      eprint={2312.17120},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### Llemma: An Open Language Model For Mathematics (Proof-Pile-2)

paper link: [here](https://arxiv.org/pdf/2310.10631.pdf)

github link: [here](https://github.com/EleutherAI/math-lm)

dataset link: [here](https://huggingface.co/datasets/EleutherAI/proof-pile-2)

citation:

```bibtex
@misc{azerbayev2023llemma,
      title={Llemma: An Open Language Model For Mathematics}, 
      author={Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck},
      year={2023},
      eprint={2310.10631},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text

paper link: [here](https://arxiv.org/pdf/2310.06786)

github link: [here](https://github.com/keirp/OpenWebMath)

dataset link: [here](https://huggingface.co/datasets/open-web-math/open-web-math)

citation:

```bibtex
@misc{paster2023openwebmath,
      title={OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text}, 
      author={Keiran Paster and Marco Dos Santos and Zhangir Azerbayev and Jimmy Ba},
      year={2023},
      eprint={2310.06786},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
```


#### MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models (MetaMathQA)

paper link: [here](https://arxiv.org/pdf/2309.12284)

github link: [here](https://github.com/meta-math/MetaMath)

dataset link: [here](https://huggingface.co/datasets/meta-math/MetaMathQA)

citation:

```bibtex
@misc{yu2023metamath,
      title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models}, 
      author={Longhui Yu and Weisen Jiang and Han Shi and Jincheng Yu and Zhengying Liu and Yu Zhang and James T. Kwok and Zhenguo Li and Adrian Weller and Weiyang Liu},
      year={2023},
      eprint={2309.12284},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```



#### Let's Verify Step by Step (PRM800K)

paper link: [here](https://arxiv.org/pdf/2305.20050.pdf)

github link: [here](https://github.com/openai/prm800k)

citation:

```bibtex
@article{lightman2023lets,
      title={Let's Verify Step by Step}, 
      author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
      journal={arXiv preprint arXiv:2305.20050},
      year={2023}
}
```



## Pretraining Objectives


#### Better & Faster Large Language Models via Multi-token Prediction

paper link: [here](https://arxiv.org/pdf/2404.19737)

citation:

```bibtex
@misc{gloeckle2024better,
      title={Better & Faster Large Language Models via Multi-token Prediction}, 
      author={Fabian Gloeckle and Badr Youbi Idrissi and Baptiste Rozière and David Lopez-Paz and Gabriel Synnaeve},
      year={2024},
      eprint={2404.19737},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```