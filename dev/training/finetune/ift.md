# Instruction Following Fine-Tuning (IFT)
*Here're some resources about Instruction Following Fine-Tuning (IFT) for LLMs*



#### Self-QA: Unsupervised Knowledge Guided Language Model Alignment

tag: `Self-QA`

paper link: [here](https://arxiv.org/pdf/2305.11952)

citation:

```bibtex
@article{zhang2023self,
  title={Self-QA: Unsupervised Knowledge Guided Language Model Alignment},
  author={Zhang, Xuanyu and Yang, Qing},
  journal={arXiv preprint arXiv:2305.11952},
  year={2023}
}
```


#### WizardLM: Empowering Large Language Models to Follow Complex Instructions

tag: `WizardLM`

paper link: [here](https://arxiv.org/pdf/2304.12244.pdf)

github link: [here](https://github.com/nlpxucan/WizardLM)

model links: 

|model name|link|
|-|-|
|WizardLM-7B-V1.0|[here](https://huggingface.co/WizardLM/WizardLM-7B-V1.0)|
|WizardLM-13B-V1.1|[here](https://huggingface.co/WizardLM/WizardLM-13B-V1.1)|
|WizardLM-13B-V1.2|[here](https://huggingface.co/WizardLM/WizardLM-13B-V1.2)|
|WizardLM-70B-V1.0|[here](https://huggingface.co/WizardLM/WizardLM-70B-V1.0)|

citation:

```bibtex
@misc{xu2023wizardlm,
      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, 
      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
      year={2023},
      eprint={2304.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### Visual Instruction Tuning

tag: `LLaVA`

paper link: [here](https://arxiv.org/pdf/2304.08485)

github link: [here](https://github.com/haotian-liu/LLaVA)

homepage link: [here](https://llava-vl.github.io/)

citation:

```bibtex
@misc{liu2023visualinstructiontuning,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.08485}, 
}
```


#### Self-Prompting Large Language Models for Zero-Shot Open-Domain QA

tag: `Self-Prompting`

paper link: [here](https://arxiv.org/pdf/2212.08635.pdf)

citation:

```bibtex
@misc{li2023selfprompting,
  title={Self-Prompting Large Language Models for Zero-Shot Open-Domain QA}, 
  author={Junlong Li and Zhuosheng Zhang and Hai Zhao},
  year={2023},
  eprint={2212.08635},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
```


#### Self-instruct: Aligning language model with self generated instructions

tag: `Self-instruct`

paper link: [here](https://arxiv.org/pdf/2212.10560)

citation:

```bibtex
@article{wang2022self,
  title={Self-instruct: Aligning language model with self generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}
```

#### Training language models to follow instructions with human feedback

tag: `InstructGPT`

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)

citation:

```bibtex
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
```


#### Scaling Instruction-Finetuned Language Models

tag: `FLAN-T5` | `FLAN-PaLM`

paper link: [here](https://arxiv.org/pdf/2210.11416.pdf)

citation:

```bibtex
@misc{chung2022scaling,
      title={Scaling Instruction-Finetuned Language Models}, 
      author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
      year={2022},
      eprint={2210.11416},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Finetuned language models are zero-shot learners

tag: `FLAN`

paper link: [here](https://arxiv.org/pdf/2109.01652)

citation:

```bibtex
@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}
```
    

