# Distillation on LLMs
*Here're some resources about Distillation strategies on LLMs*


#### Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes

tag: `Distilling Step-by-Step` | `Google` | `University of Washington`

paper link: [here](https://arxiv.org/pdf/2305.02301)

code link: [here](https://github.com/google-research/distilling-step-by-step)

citation:

```bibtex
@misc{hsieh2023distillingstepbystepoutperforminglarger,
      title={Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes}, 
      author={Cheng-Yu Hsieh and Chun-Liang Li and Chih-Kuan Yeh and Hootan Nakhost and Yasuhisa Fujii and Alexander Ratner and Ranjay Krishna and Chen-Yu Lee and Tomas Pfister},
      year={2023},
      eprint={2305.02301},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.02301}, 
}
```