# Alignment Fine-Tuning on LLMs
*Here're some resources about Alignment Fine-Tuning strategies on LLMs, especically instruction-following tuning (IFT)*


#### TÜLU 3: Pushing Frontiers in Open Language Model Post-Training

tag: `TULU 3` | `RLVR` | `Allen AI`

paper link: [here](https://allenai.org/papers/tulu-3-report.pdf)

blog link: [here](https://allenai.org/blog/tulu-3-technical)

github link: [here](https://github.com/allenai/open-instruct)

modelhub link: [here](https://huggingface.co/collections/allenai/tulu-3-models-673b8e0dc3512e30e7dc54f5)

citation:

```bibtex
@misc{lambert2024tulu3pushingfrontiers,
      title={T\"ULU 3: Pushing Frontiers in Open Language Model Post-Training}, 
      author={Nathan Lambert and Jacob Morrison and Valentina Pyatkin and Shengyi Huang and Hamish Ivison and Faeze Brahman and Lester James V. Miranda and Alisa Liu and Nouha Dziri and Shane Lyu and Yuling Gu and Saumya Malik and Victoria Graf and Jena D. Hwang and Jiangjiang Yang and Ronan Le Bras and Oyvind Tafjord and Chris Wilhelm and Luca Soldaini and Noah A. Smith and Yizhong Wang and Pradeep Dasigi and Hannaneh Hajishirzi},
      year={2024},
      eprint={2411.15124},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.15124}, 
}
```


#### Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization

tag: `DQO` | `ByteDance`

paper link: [here](https://arxiv.org/pdf/2410.09302)

citation:

```bibtex
@article{liu2024enhancing,
  title={Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization},
  author={Liu, Guanlin and Ji, Kaixuan and Zheng, Renjie and Wu, Zheng and Dun, Chen and Gu, Quanquan and Yan, Lin},
  journal={arXiv preprint arXiv:2410.09302},
  year={2024}
}
```


#### HybridFlow: A Flexible and Efficient RLHF Framework

tag: `HybridFlow` | `EuroSys25` | `ByteDance`

paper link: [here](https://arxiv.org/pdf/2409.19256)

github link: [here](https://github.com/volcengine/verl)

citation:

```bibtex
@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}
```


#### Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs

tag: `CPO` | `NIPS24` | `Sea AI Lab`

paper link: [here](https://arxiv.org/pdf/2406.09136)

github link: [here](https://github.com/sail-sg/CPO)

citation:

```bibtex
@misc{zhang2024chainpreferenceoptimizationimproving,
      title={Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs}, 
      author={Xuan Zhang and Chao Du and Tianyu Pang and Qian Liu and Wei Gao and Min Lin},
      year={2024},
      eprint={2406.09136},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.09136}, 
}
```


#### RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness

tag: `RLAIF-V` | `Tsinghua University`

paper link: [here](https://arxiv.org/pdf/2405.17220)

github link: [here](https://github.com/RLHF-V/RLAIF-V)

citation:

```bibtex
@misc{yu2024rlaifvaligningmllmsopensource,
      title={RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness}, 
      author={Tianyu Yu and Haoye Zhang and Yuan Yao and Yunkai Dang and Da Chen and Xiaoman Lu and Ganqu Cui and Taiwen He and Zhiyuan Liu and Tat-Seng Chua and Maosong Sun},
      year={2024},
      eprint={2405.17220},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.17220}, 
}
```


#### DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

tag: `DeepSeekMath` | `GRPO` | `DeepSeek AI` | `Tsinghua University` | `Peking University`

paper link: [here](https://arxiv.org/pdf/2402.03300.pdf)

github link: [here](https://github.com/deepseek-ai/DeepSeek-Math)


citation:

```bibtex
@misc{shao2024deepseekmath,
      title={DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}, 
      author={Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},
      year={2024},
      eprint={2402.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

#### Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models

tag: `SPIN` | `Self-Play` | `ICML24` | `UCLA`

paper link: [here](https://arxiv.org/pdf/2401.01335.pdf)

github link: [here](https://github.com/uclaml/SPIN)

citation:

```bibtex
@misc{chen2024selfplay,
      title={Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models}, 
      author={Zixiang Chen and Yihe Deng and Huizhuo Yuan and Kaixuan Ji and Quanquan Gu},
      year={2024},
      eprint={2401.01335},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Zephyr: Direct Distillation of LM Alignment

tag: `Zephyr` | `dDPO` | `COLM24` | `HuggingFace`

paper link: [here](https://arxiv.org/pdf/2310.16944.pdf)

github link: [here](https://github.com/huggingface/alignment-handbook)

modelhub link: [here](https://huggingface.co/collections/HuggingFaceH4/zephyr-7b-6538c6d6d5ddd1cbb1744a66)

citation:

```bibtex
@misc{tunstall2023zephyr,
      title={Zephyr: Direct Distillation of LM Alignment}, 
      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clémentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},
      year={2023},
      eprint={2310.16944},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained Models in Few-Shot Learning

tag: `FD-Align` | `NIPS23` | `USTB`

paper link: [here](https://arxiv.org/pdf/2310.15105)

github link: [here](https://github.com/skingorz/FD-Align)

citation:

```bibtex
@article{song2023fd,
  title={FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained Models in Few-Shot Learning},
  author={Song, Kun and Ma, Huimin and Zou, Bochao and Zhang, Huishuai and Huang, Weiran},
  journal={arXiv preprint arXiv:2310.15105},
  year={2023}
}
```

#### Group Preference Optimization: Few-Shot Alignment of Large Language Models

tag: `GPO` | `ICLR24` | `UCLA`

paper link: [here](https://arxiv.org/pdf/2310.11523.pdf)

github link: [here](https://github.com/jamqd/Group-Preference-Optimization)

homepage link: [here](https://siyan-zhao.github.io/llm-gpo/)

citation:

```bibtex
@misc{zhao2023group,
      title={Group Preference Optimization: Few-Shot Alignment of Large Language Models}, 
      author={Siyan Zhao and John Dang and Aditya Grover},
      year={2023},
      eprint={2310.11523},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment

tag: `P3O` | `UC Berkeley`

paper link: [here](https://arxiv.org/pdf/2310.00212.pdf)

citation:

```bibtex
@misc{wu2023pairwise,
      title={Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment}, 
      author={Tianhao Wu and Banghua Zhu and Ruoyu Zhang and Zhaojin Wen and Kannan Ramchandran and Jiantao Jiao},
      year={2023},
      eprint={2310.00212},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### OpenChat: Advancing Open-source Language Models with Mixed-Quality Data

tag: `OpenChat` | `C-RLFT` | `ICLR24` | `Tsinghua University`

paper link: [here](https://arxiv.org/pdf/2309.11235.pdf)

github link: [here](https://github.com/imoneoi/openchat)

modelhub link: [here](https://huggingface.co/collections/openchat/openchat-65110500e14eeb01d4888806)

citation:

```bibtex
@article{wang2023openchat,
  title={Openchat: Advancing open-source language models with mixed-quality data},
  author={Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
  journal={arXiv preprint arXiv:2309.11235},
  year={2023}
}
```


#### Fine-Tuning Language Models with Advantage-Induced Policy Alignment

tag: `APA` | `UC Berkeley`

paper link: [here](https://arxiv.org/pdf/2306.02231)

citation:

```bibtex
@misc{zhu2023finetuning,
      title={Fine-Tuning Language Models with Advantage-Induced Policy Alignment}, 
      author={Banghua Zhu and Hiteshi Sharma and Felipe Vieira Frujeri and Shi Dong and Chenguang Zhu and Michael I. Jordan and Jiantao Jiao},
      year={2023},
      eprint={2306.02231},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### Direct preference optimization: Your language model is secretly a reward model

tag: `DPO` | `NIPS23` | `Stanford University`

paper link: [here](https://arxiv.org/pdf/2305.18290.pdf)

citation:

```bibtex
@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}
```

#### Lima: Less is more for alignment

tag: `Lima` | `NIPS23` | `Meta` | `CMU`

paper link: [here](https://arxiv.org/pdf/2305.11206)

citation:

```bibtex
@article{zhou2023lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={arXiv preprint arXiv:2305.11206},
  year={2023}
}
```

#### Principle-driven self-alignment of language models from scratch with minimal human supervision

tag: `Self-Align` | `NIPS23` | `IBM Research` | `CMU` | `MIT`

paper link: [here](https://arxiv.org/pdf/2305.03047)

github link: [here](https://github.com/IBM/Dromedary)

citation:

```bibtex
@article{sun2023principle,
  title={Principle-driven self-alignment of language models from scratch with minimal human supervision},
  author={Sun, Zhiqing and Shen, Yikang and Zhou, Qinhong and Zhang, Hongxin and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  journal={arXiv preprint arXiv:2305.03047},
  year={2023}
}
```


#### WizardLM: Empowering Large Language Models to Follow Complex Instructions

tag: `WizardLM` | `ICLR24` | `Microsoft` | `Peking University`

paper link: [here](https://openreview.net/pdf?id=CfXh93NDgH)

github link: [here](https://github.com/nlpxucan/WizardLM)

modelhub link: [here](https://huggingface.co/WizardLMTeam)

citation:

```bibtex
@misc{xu2023wizardlm,
      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, 
      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
      year={2023},
      eprint={2304.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

#### RRHF: Rank responses to align language models with human feedback without tears

tag: `RRHF` | `NIPS23` | `DAMO Academy` | `Alibaba Group` | `Tsinghua University`

paper link: [here](https://arxiv.org/pdf/2304.05302)

github link: [here](https://github.com/GanjinZero/RRHF)

citation:

```bibtex
@article{yuan2023rrhf,
  title={Rrhf: Rank responses to align language models with human feedback without tears},
  author={Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei},
  journal={arXiv preprint arXiv:2304.05302},
  year={2023}
}
```

#### Openagi: When llm meets domain experts

tag: `OpenAGI` | `RLTF` | `NIPS23` | `Rutgers University`

paper link: [here](https://arxiv.org/pdf/2304.04370.pdf)

github link: [here](https://github.com/agiresearch/OpenAGI)

citation:

```bibtex
@article{ge2023openagi,
  title={Openagi: When llm meets domain experts},
  author={Ge, Yingqiang and Hua, Wenyue and Ji, Jianchao and Tan, Juntao and Xu, Shuyuan and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2304.04370},
  year={2023}
}
```
    
#### Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons

tag: `K-wise Comparison` | `ICML23` | `UC Berkeley`

paper link: [here](https://proceedings.mlr.press/v202/zhu23f/zhu23f.pdf)

citation:

```bibtex
@misc{zhu2023principled,
      title={Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons}, 
      author={Banghua Zhu and Jiantao Jiao and Michael I. Jordan},
      year={2023},
      eprint={2301.11270},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### Self-instruct: Aligning language model with self generated instructions

tag: `Self-instruct` | `IFT` | `Instruction Tuning` | `ACL23` | `University of Washington`

paper link: [here](https://arxiv.org/pdf/2212.10560)

github link: [here](https://github.com/yizhongw/self-instruct)

citation:

```bibtex
@article{wang2022self,
  title={Self-instruct: Aligning language model with self generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}
```


#### Self-Prompting Large Language Models for Zero-Shot Open-Domain QA

tag: `Self-Prompting` | `IFT` | `Instruction Tuning` | `NAACL24` | `SJTU`

paper link: [here](https://aclanthology.org/2024.naacl-long.17.pdf)

github link: [here](https://github.com/lockon-n/self-prompting)

citation:

```bibtex
@misc{li2023selfprompting,
  title={Self-Prompting Large Language Models for Zero-Shot Open-Domain QA}, 
  author={Junlong Li and Zhuosheng Zhang and Hai Zhao},
  year={2023},
  eprint={2212.08635},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
```


#### Constitutional ai: Harmlessness from ai feedback

tag: `RLAIF` | `Constitutional AI` | `Anthropic`

paper link: [here](https://arxiv.org/pdf/2212.08073.pdf)

github link: [here](https://github.com/anthropics/ConstitutionalHarmlessnessPaper)

citation:

```bibtex
@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}
```


#### Training language models to follow instructions with human feedback

tag: `InstructGPT` | `IFT` | `Instruction Tuning` | `NIPS22` | `OpenAI`

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)

citation:

```bibtex
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
```


#### Offline RL for Natural Language Generation with Implicit Language Q Learning

tag: `ILQL` | `ICLR23` | `UC Berkeley`

paper link: [here](https://arxiv.org/pdf/2211.11073.pdf)

github link: [here](https://github.com/Sea-Snell/Implicit-Language-Q-Learning)

homepage link: [here](https://sea-snell.github.io/ILQL_site/)

citation:

```bibtex
@misc{snell2023offlinerlnaturallanguage,
      title={Offline RL for Natural Language Generation with Implicit Language Q Learning}, 
      author={Charlie Snell and Ilya Kostrikov and Yi Su and Mengjiao Yang and Sergey Levine},
      year={2023},
      eprint={2206.11871},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.11871}, 
}
```


#### Scaling Instruction-Finetuned Language Models

tag: `FLAN-T5` | `FLAN-PaLM` | `IFT` | `Instruction Tuning` | `JMLR24` | `Google`

paper link: [here](https://www.jmlr.org/papers/volume25/23-0870/23-0870.pdf)

github link: [here](https://github.com/google-research/flan)

citation:

```bibtex
@misc{chung2022scaling,
      title={Scaling Instruction-Finetuned Language Models}, 
      author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
      year={2022},
      eprint={2210.11416},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Finetuned language models are zero-shot learners

tag: `FLAN` | `IFT` | `Instruction Tuning` | `ICLR22` | `Google`

paper link: [here](https://arxiv.org/pdf/2109.01652)

github link: [here](https://github.com/google-research/flan)

citation:

```bibtex
@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}
```


#### Fine-tuning language models from human preferences

tag: `RLHF` | `PPO` | `OpenAI`

paper link: [here](https://arxiv.org/pdf/1909.08593.pdf)

blog link: [here](https://huggingface.co/blog/rlhf)

github link: [here](https://github.com/openai/lm-human-preferences)

citation:

```bibtex
@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}
```