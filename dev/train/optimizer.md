# Optimizers for LLMs
*Here're some resources about optimizers for LLMs*


#### AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2312.01658)

citation:

```bibtex
@misc{yue2023agdautoswitchableoptimizerusing,
      title={AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix}, 
      author={Yun Yue and Zhiling Ye and Jiadi Jiang and Yongchao Liu and Ke Zhang},
      year={2023},
      eprint={2312.01658},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
      url={https://arxiv.org/abs/2312.01658}, 
}
```

#### AdaLomo: Low-memory Optimization with Adaptive Learning Rate [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2310.10195)

citation:
```bibtex
@misc{lv2023adalomo,
      title={AdaLomo: Low-memory Optimization with Adaptive Learning Rate}, 
      author={Kai Lv and Hang Yan and Qipeng Guo and Haijun Lv and Xipeng Qiu},
      year={2023},
      eprint={2310.10195},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Full Parameter Fine-tuning for Large Language Models with Limited Resources (LOMO) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2306.09782)

citation: 
```bibtex
@article{lv2023full,
  title={Full Parameter Fine-tuning for Large Language Models with Limited Resources},
  author={Lv, Kai and Yang, Yuqing and Liu, Tengxiao and Gao, Qinghui and Guo, Qipeng and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2306.09782},
  year={2023}
}
```


#### 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2102.02888.pdf)

citation:
```bibtex
@misc{tang20211bit,
      title={1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed}, 
      author={Hanlin Tang and Shaoduo Gan and Ammar Ahmad Awan and Samyam Rajbhandari and Conglong Li and Xiangru Lian and Ji Liu and Ce Zhang and Yuxiong He},
      year={2021},
      eprint={2102.02888},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Why AdamW matters [`READ`]

blog link: [here](https://towardsdatascience.com/why-adamw-matters-736223f31b5d)

citation:
```bibtex
@misc{graetz2018whyadamwmatters,
  author = {Fabio M. Graetz},
  title = {Why AdamW matters},
  year = {2018},
  howpublished = {\url{https://towardsdatascience.com/why-adamw-matters-736223f31b5d}},
}
```