# Mixture-of-Experts (MoE)
*Here're some resources about Mixture-of-Experts (MoE) structure design of LLMs*


#### DeepSeek-V2- A Strong, Economical, and Efficient Mixture-of-Experts Language Model [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2405.04434)

github link: [here](https://github.com/deepseek-ai/DeepSeek-V2)

model links:

|model name|link|
|-|-|
|DeepSeek-V2-Chat|[here](https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat)|
|DeepSeek-V2|[here](https://huggingface.co/deepseek-ai/DeepSeek-V2)|
|DeepSeek-V2-Lite-Chat|[here](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite-Chat)|
|DeepSeek-V2-Lite|[here](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite)|

citation:

```bibtex
@misc{deepseekai2024deepseekv2strongeconomicalefficient,
      title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}, 
      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bin Wang and Bingxuan Wang and Bo Liu and Chenggang Zhao and Chengqi Dengr and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Hanwei Xu and Hao Yang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jin Chen and Jingyang Yuan and Junjie Qiu and Junxiao Song and Kai Dong and Kaige Gao and Kang Guan and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruizhe Pan and Runxin Xu and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Size Zheng and T. Wang and Tian Pei and Tian Yuan and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Liu and Xin Xie and Xingkai Yu and Xinnan Song and Xinyi Zhou and Xinyu Yang and Xuan Lu and Xuecheng Su and Y. Wu and Y. K. Li and Y. X. Wei and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Zheng and Yichao Zhang and Yiliang Xiong and Yilong Zhao and Ying He and Ying Tang and Yishi Piao and Yixin Dong and Yixuan Tan and Yiyuan Liu and Yongji Wang and Yongqiang Guo and Yuchen Zhu and Yuduan Wang and Yuheng Zou and Yukun Zha and Yunxian Ma and Yuting Yan and Yuxiang You and Yuxuan Liu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhewen Hao and Zhihong Shao and Zhiniu Wen and Zhipeng Xu and Zhongyu Zhang and Zhuoshu Li and Zihan Wang and Zihui Gu and Zilin Li and Ziwei Xie},
      year={2024},
      eprint={2405.04434},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
      url={https://arxiv.org/abs/2405.04434}, 
}
```

#### DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2401.06066)

github link: [here](https://github.com/deepseek-ai/DeepSeek-MoE)

model links:

|model name|link|
|-|-|
|deepseek-moe-16b-chat|[here](https://huggingface.co/deepseek-ai/deepseek-moe-16b-chat)|
|deepseek-moe-16b-base|[here](https://huggingface.co/deepseek-ai/deepseek-moe-16b-base)|

citation:

```bibtex
@misc{dai2024deepseekmoeultimateexpertspecialization,
      title={DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models}, 
      author={Damai Dai and Chengqi Deng and Chenggang Zhao and R. X. Xu and Huazuo Gao and Deli Chen and Jiashi Li and Wangding Zeng and Xingkai Yu and Y. Wu and Zhenda Xie and Y. K. Li and Panpan Huang and Fuli Luo and Chong Ruan and Zhifang Sui and Wenfeng Liang},
      year={2024},
      eprint={2401.06066},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
      url={https://arxiv.org/abs/2401.06066}, 
}
```

#### Fast Inference of Mixture-of-Experts Language Models with Offloading [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2312.17238.pdf)

github link: [here](https://github.com/dvmazur/mixtral-offloading)

citation: 
```bibtex
@misc{eliseev2023fast,
      title={Fast Inference of Mixture-of-Experts Language Models with Offloading}, 
      author={Artyom Eliseev and Denis Mazur},
      year={2023},
      eprint={2312.17238},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2312.07987v2.pdf)

citation:

```bibtex
@article{csordas2023switchhead,
  title={SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention},
  author={Csord{\'a}s, R{\'o}bert and Pi{\k{e}}kos, Piotr and Irie, Kazuki},
  journal={arXiv preprint arXiv:2312.07987},
  year={2023}
}
```


#### Mixtral of experts: A high quality Sparse Mixture-of-Experts [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2401.04088)

blog link: [here](https://mistral.ai/news/mixtral-of-experts/)

model links: 

|model name|link|
|-|-|
|Mixtral-SlimOrca-8x7B|[here](https://huggingface.co/Open-Orca/Mixtral-SlimOrca-8x7B)|
|Mixtral-8x7B-Instruct-v0.1|[here](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)|
|Mixtral-8x7B-v0.1|[here](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)|

citation:
```bibtex
@misc{jiang2024mixtral,
      title={Mixtral of Experts}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2024},
      eprint={2401.04088},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
```


#### Memory Augmented Language Models through Mixture of Word Experts [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2311.10768)

citation:

```bibtex
@article{santos2023memory,
  title={Memory Augmented Language Models through Mixture of Word Experts},
  author={Santos, Cicero Nogueira dos and Lee-Thorp, James and Noble, Isaac and Chang, Chung-Ching and Uthus, David},
  journal={arXiv preprint arXiv:2311.10768},
  year={2023}
}
```


#### QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2310.16795.pdf)

citation:
```bibtex
@misc{frantar2023qmoe,
      title={QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models}, 
      author={Elias Frantar and Dan Alistarh},
      year={2023},
      eprint={2310.16795},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### From sparse to soft mixtures of experts (Soft MoE) [`READ`]

paper link: [here](https://arxiv.org/pdf/2308.00951)

citation:

```bibtex
@article{puigcerver2023sparse,
  title={From sparse to soft mixtures of experts},
  author={Puigcerver, Joan and Riquelme, Carlos and Mustafa, Basil and Houlsby, Neil},
  journal={arXiv preprint arXiv:2308.00951},
  year={2023}
}
```


#### OpenMoE: A family of open-sourced Mixture-of-Experts (MoE) Large Language Models [`UNREAD`]

blog link: [here](https://xuefuzhao.notion.site/Aug-2023-OpenMoE-v0-2-Release-43808efc0f5845caa788f2db52021879)

github link: [here](https://github.com/XueFuzhao/OpenMoE)

citation:
```bibtex
@misc{openmoe2023,
  author = {Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou and Yang You},
  title = {OpenMoE: Open Mixture-of-Experts Language Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/XueFuzhao/OpenMoE}},
}
```


#### Accelerating distributed {MoE} training and inference with lina [`UNREAD`]

paper link: [here](https://www.usenix.org/system/files/atc23-li-jiamin.pdf)

citation:

```bibtex
@inproceedings{li2023accelerating,
  title={Accelerating distributed $\{$MoE$\}$ training and inference with lina},
  author={Li, Jiamin and Jiang, Yimin and Zhu, Yibo and Wang, Cong and Xu, Hong},
  booktitle={2023 USENIX Annual Technical Conference (USENIX ATC 23)},
  pages={945--959},
  year={2023}
}
```


#### AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts [`UNREAD`]

paper link: [here](http://openaccess.thecvf.com/content/ICCV2023/papers/Chen_AdaMV-MoE_Adaptive_Multi-Task_Vision_Mixture-of-Experts_ICCV_2023_paper.pdf)

citation:
```bibtex
@inproceedings{chen2023adamv,
  title={AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts},
  author={Chen, Tianlong and Chen, Xuxi and Du, Xianzhi and Rashwan, Abdullah and Yang, Fan and Chen, Huizhong and Wang, Zhangyang and Li, Yeqing},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={17346--17357},
  year={2023}
}
```


#### FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2304.03946)

citation:

```bibtex
@article{nie2023flexmoe,
  title={FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement},
  author={Nie, Xiaonan and Miao, Xupeng and Wang, Zilong and Yang, Zichao and Xue, Jilong and Ma, Lingxiao and Cao, Gang and Cui, Bin},
  journal={Proceedings of the ACM on Management of Data},
  volume={1},
  number={1},
  pages={1--19},
  year={2023},
  publisher={ACM New York, NY, USA}
}
```



#### MegaBlocks: Efficient Sparse Training with Mixture-of-Experts (dMoE) [`READ`]

paper link: [here](https://arxiv.org/pdf/2211.15841.pdf)

github link: [here](https://github.com/stanford-futuredata/megablocks)

citation:
```bibtex
@misc{gale2022megablocks,
      title={MegaBlocks: Efficient Sparse Training with Mixture-of-Experts}, 
      author={Trevor Gale and Deepak Narayanan and Cliff Young and Matei Zaharia},
      year={2022},
      eprint={2211.15841},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### Tutel: Adaptive Mixture-of-Experts at Scale [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2206.03382)

citation:

```bibtex
@misc{hwang2023tutel,
      title={Tutel: Adaptive Mixture-of-Experts at Scale}, 
      author={Changho Hwang and Wei Cui and Yifan Xiong and Ziyue Yang and Ze Liu and Han Hu and Zilong Wang and Rafael Salas and Jithin Jose and Prabhat Ram and Joe Chau and Peng Cheng and Fan Yang and Mao Yang and Yongqiang Xiong},
      year={2023},
      eprint={2206.03382},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}
```

#### Evomoe: An evolutional mixture-of-experts training framework via dense-to-sparse gate [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2112.14397)

citation:

```bibtex
@article{nie2021evomoe,
  title={Evomoe: An evolutional mixture-of-experts training framework via dense-to-sparse gate},
  author={Nie, Xiaonan and Miao, Xupeng and Cao, Shijie and Ma, Lingxiao and Liu, Qibin and Xue, Jilong and Miao, Youshan and Liu, Yi and Yang, Zhi and Cui, Bin},
  journal={arXiv preprint arXiv:2112.14397},
  year={2021}
}
```


#### Mixture of Attention Heads: Selecting Attention Heads Per Token (MoA) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2210.05144)

citation:

```bibtex
@misc{zhang2022mixture,
      title={Mixture of Attention Heads: Selecting Attention Heads Per Token}, 
      author={Xiaofeng Zhang and Yikang Shen and Zeyu Huang and Jie Zhou and Wenge Rong and Zhang Xiong},
      year={2022},
      eprint={2210.05144},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

#### FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models [`UNREAD`]

paper link: [here](https://dl.acm.org/doi/pdf/10.1145/3503221.3508418)

citation:

```bibtex
@inproceedings{he2022fastermoe,
  title={FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models},
  author={He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin},
  booktitle={Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={120--134},
  year={2022}
}
```

#### Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale [`UNREAD`]

paper link: [here](https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf)

citation:

```bibtex
@inproceedings{rajbhandari2022deepspeed,
  title={Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale},
  author={Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  booktitle={International Conference on Machine Learning},
  pages={18332--18346},
  year={2022},
  organization={PMLR}
}
```

#### Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2204.07689.pdf)

citation:
```bibtex
@misc{gupta2022sparsely,
      title={Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners}, 
      author={Shashank Gupta and Subhabrata Mukherjee and Krishan Subudhi and Eduardo Gonzalez and Damien Jose and Ahmed H. Awadallah and Jianfeng Gao},
      year={2022},
      eprint={2204.07689},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Unified Scaling Laws for Routed Language Models (Sinkhorn-BASE) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2202.01169)

citation:

```bibtex
@misc{clark2022unified,
      title={Unified Scaling Laws for Routed Language Models}, 
      author={Aidan Clark and Diego de las Casas and Aurelia Guy and Arthur Mensch and Michela Paganini and Jordan Hoffmann and Bogdan Damoc and Blake Hechtman and Trevor Cai and Sebastian Borgeaud and George van den Driessche and Eliza Rutherford and Tom Hennigan and Matthew Johnson and Katie Millican and Albin Cassirer and Chris Jones and Elena Buchatskaya and David Budden and Laurent Sifre and Simon Osindero and Oriol Vinyals and Jack Rae and Erich Elsen and Koray Kavukcuoglu and Karen Simonyan},
      year={2022},
      eprint={2202.01169},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}
```

#### Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity (SMoE) [`READ`]

paper link: [here](https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf)

citation:
```bibtex
@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022},
  publisher={JMLRORG}
}
```

#### Beyond distillation: Task-level mixture-of-experts for efficient inference [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2110.03742)

citation:

```bibtex
@article{kudugunta2021beyond,
  title={Beyond distillation: Task-level mixture-of-experts for efficient inference},
  author={Kudugunta, Sneha and Huang, Yanping and Bapna, Ankur and Krikun, Maxim and Lepikhin, Dmitry and Luong, Minh-Thang and Firat, Orhan},
  journal={arXiv preprint arXiv:2110.03742},
  year={2021}
}
```


#### BASE Layers: Simplifying Training of Large, Sparse Models (BASE) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2103.16716)

updated version: [here](https://arxiv.org/pdf/2202.01169)

citation:

```bibtex
@misc{lewis2021base,
      title={BASE Layers: Simplifying Training of Large, Sparse Models}, 
      author={Mike Lewis and Shruti Bhosale and Tim Dettmers and Naman Goyal and Luke Zettlemoyer},
      year={2021},
      eprint={2103.16716},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}
```


#### Hash layers for large sparse models [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2021/file/92bf5e6240737e0326ea59846a83e076-Paper.pdf)

citation:

```bibtex
@article{roller2021hash,
  title={Hash layers for large sparse models},
  author={Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17555--17566},
  year={2021}
}
```


#### Sparse is enough in scaling transformers [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper/2021/file/51f15efdd170e6043fa02a74882f0470-Paper.pdf)

citation: 
```bibtex
@article{jaszczur2021sparse,
  title={Sparse is enough in scaling transformers},
  author={Jaszczur, Sebastian and Chowdhery, Aakanksha and Mohiuddin, Afroz and Kaiser, Lukasz and Gajewski, Wojciech and Michalewski, Henryk and Kanerva, Jonni},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9895--9907},
  year={2021}
}
```


#### Gshard: Scaling giant models with conditional computation and automatic sharding [`READ`]

paper link: [here](https://arxiv.org/pdf/2006.16668.pdf)

citation:
```bibtex
@article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}
```

#### Outrageously large neural networks: The sparsely-gated mixture-of-experts layer (Sparse MoE) [`READ`]

paper link: [here](https://arxiv.org/pdf/1701.06538.pdf)

citation:
```bibtex
@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}
```

#### Adaptive Mixture of Local Experts [`READ`]

paper link: [here](http://www.cs.utoronto.ca/~hinton/absps/jjnh91.ps)

citation:
```bibtex
@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}
```