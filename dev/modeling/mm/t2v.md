# Text to Video and Video to Text
*Here're are some resources about Text to Video and Video to Text modeling, understanding, generation in Multi-Modal LLMs*


## Method


#### Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation

tag: `SVG2` | `Sparse VideoGen2` | `ICML25` | `UC Berkeley` | `MIT` | `Standford`

paper link: [here](https://arxiv.org/pdf/2505.18875)

citation:

```bibtex
@misc{yang2025sparsevideogen2acceleratevideo,
      title={Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation}, 
      author={Shuo Yang and Haocheng Xi and Yilong Zhao and Muyang Li and Jintao Zhang and Han Cai and Yujun Lin and Xiuyu Li and Chenfeng Xu and Kelly Peng and Jianfei Chen and Song Han and Kurt Keutzer and Ion Stoica},
      year={2025},
      eprint={2505.18875},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.18875}, 
}
```

#### Scaling Diffusion Transformers Efficiently via μP

tag: `μP` | `DiT` | `MMDiT` | `U-ViT` | `PixArt-α` | `ByteDance Seed`

paper link: [here](https://www.arxiv.org/pdf/2505.15270)

code link: [here](https://github.com/ML-GSAI/Scaling-Diffusion-Transformers-muP)

citation:

```bibtex
@misc{zheng2025scalingdiffusiontransformersefficiently,
      title={Scaling Diffusion Transformers Efficiently via $\mu$P}, 
      author={Chenyu Zheng and Xinyu Zhang and Rongzhen Wang and Wei Huang and Zhi Tian and Weilin Huang and Jun Zhu and Chongxuan Li},
      year={2025},
      eprint={2505.15270},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.15270}, 
}
```

#### MAGI-1: Autoregressive Video Generation at Scale

tag: `MAGI-1` | `SandAI`

paper link: [here](https://arxiv.org/pdf/2505.13211)

code link: [here](https://github.com/SandAI-org/Magi-1)

citation:

```bibtex
@misc{ai2025magi1autoregressivevideogeneration,
      title={MAGI-1: Autoregressive Video Generation at Scale}, 
      author={Sand. ai and Hansi Teng and Hongyu Jia and Lei Sun and Lingzhi Li and Maolin Li and Mingqiu Tang and Shuai Han and Tianning Zhang and W. Q. Zhang and Weifeng Luo and Xiaoyang Kang and Yuchen Sun and Yue Cao and Yunpeng Huang and Yutong Lin and Yuxin Fang and Zewei Tao and Zheng Zhang and Zhongshu Wang and Zixun Liu and Dai Shi and Guoli Su and Hanwen Sun and Hong Pan and Jie Wang and Jiexin Sheng and Min Cui and Min Hu and Ming Yan and Shucheng Yin and Siran Zhang and Tingting Liu and Xianping Yin and Xiaoyu Yang and Xin Song and Xuan Hu and Yankai Zhang and Yuqiao Li},
      year={2025},
      eprint={2505.13211},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.13211}, 
}
```


#### Magic 1-For-1: Generating One Minute Video Clips within One Minute

tag: `Magic 1-For-1` | `Nvidia` | `Peking University`

paper link: [here](https://arxiv.org/pdf/2502.07701)

code link: [here](https://github.com/Open-Magic-Video/Magic-1-For-1)

citation:

```bibtex
@misc{yi2025magic1for1generatingminute,
      title={Magic 1-For-1: Generating One Minute Video Clips within One Minute}, 
      author={Hongwei Yi and Shitong Shao and Tian Ye and Jiantong Zhao and Qingyu Yin and Michael Lingelbach and Li Yuan and Yonghong Tian and Enze Xie and Daquan Zhou},
      year={2025},
      eprint={2502.07701},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.07701}, 
}
```


#### History-Guided Video Diffusion

tag: `DFoT` | `History Guidance` | `Diffusion Forcing` | `Meta` | `UC Berkeley`

paper link: [here](https://arxiv.org/pdf/2502.06764)

code link: [here](https://github.com/kwsong0113/diffusion-forcing-transformer)

homepage link: [here](https://boyuan.space/history-guidance/)

citation:

```bibtex
@misc{song2025historyguidedvideodiffusion,
      title={History-Guided Video Diffusion}, 
      author={Kiwhan Song and Boyuan Chen and Max Simchowitz and Yilun Du and Russ Tedrake and Vincent Sitzmann},
      year={2025},
      eprint={2502.06764},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.06764}, 
}
```


#### VideoRoPE: What Makes for Good Video Rotary Position Embedding?

tag: `VideoRoPE` | `Shanghai AI Lab` | `Fudan University`

paper link: [here](https://arxiv.org/pdf/2502.05173)

code link: [here](https://github.com/Wiselnn570/VideoRoPE)

citation:

```bibtex
@misc{wei2025videoropemakesgoodvideo,
      title={VideoRoPE: What Makes for Good Video Rotary Position Embedding?}, 
      author={Xilin Wei and Xiaoran Liu and Yuhang Zang and Xiaoyi Dong and Pan Zhang and Yuhang Cao and Jian Tong and Haodong Duan and Qipeng Guo and Jiaqi Wang and Xipeng Qiu and Dahua Lin},
      year={2025},
      eprint={2502.05173},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.05173}, 
}
```


#### Fast Video Generation with Sliding Tile Attention

tag: `STA` | `UCSD` | `Tsinghua University` | `UCB`

paper link: [here](https://www.arxiv.org/pdf/2502.04507)

citation:

```bibtex
@misc{zhang2025fastvideogenerationsliding,
      title={Fast Video Generation with Sliding Tile Attention}, 
      author={Peiyuan Zhang and Yongqi Chen and Runlong Su and Hangliang Ding and Ion Stoica and Zhenghong Liu and Hao Zhang},
      year={2025},
      eprint={2502.04507},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.04507}, 
}
```

#### Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity

tag: `SVG` | `Sparse VideoGen` | `ICML25` | `UC Berkeley` | `MIT` | `Nvidia` | `Tsinghua University`

paper link: [here](https://arxiv.org/pdf/2502.01776)

code link: [here](https://github.com/svg-project/Sparse-VideoGen)

follow-up work: [here](https://arxiv.org/pdf/2505.18875)

citation:

```bibtex
@misc{xi2025sparsevideogenacceleratingvideo,
      title={Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity}, 
      author={Haocheng Xi and Shuo Yang and Yilong Zhao and Chenfeng Xu and Muyang Li and Xiuyu Li and Yujun Lin and Han Cai and Jintao Zhang and Dacheng Li and Jianfei Chen and Ion Stoica and Kurt Keutzer and Song Han},
      year={2025},
      eprint={2502.01776},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.01776}, 
}
```


#### VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models

tag: `VideoJAM` | `GenAI` | `Meta`

paper link: [here](https://hila-chefer.github.io/videojam-paper.github.io/VideoJAM_arxiv.pdf)

homepage link: [here](https://hila-chefer.github.io/videojam-paper.github.io/)

citation:

```bibtex
@article{chefer2024videojam,
  author = {Hila Chefer and Uriel Singer and Amit Zohar and Yuval Kirstain and Adam Polyak and Yaniv Taigman and Lior Wolf and Shelly Sheynin},
  title  = {VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models},
  year= {2025},
  url = {https://hila-chefer.github.io/videojam-paper.github.io/VideoJAM_arxiv.pdf}
}
```


#### Autoregressive Video Generation without Vector Quantization

tag: `NOVA` | `ICLR25` | `BAAI` | `BUPT`

paper link: [here](https://arxiv.org/pdf/2412.14169)

code link: [here](https://github.com/baaivision/NOVA)

citation:

```bibtex
@misc{deng2025autoregressivevideogenerationvector,
      title={Autoregressive Video Generation without Vector Quantization}, 
      author={Haoge Deng and Ting Pan and Haiwen Diao and Zhengxiong Luo and Yufeng Cui and Huchuan Lu and Shiguang Shan and Yonggang Qi and Xinlong Wang},
      year={2025},
      eprint={2412.14169},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.14169}, 
}
```


#### State-of-the-art video and image generation with Veo 2 and Imagen 3

tag: `Veo2` | `Imagen3` | `Google DeepMind`

blog link: [here](https://blog.google/technology/google-labs/video-image-generation-update-december-2024/)

homepage link: [Veo3](https://deepmind.google/models/veo/) | [Veo2](https://deepmind.google/technologies/veo/veo-2/) | [Imagen3](https://deepmind.google/technologies/imagen-3/)


citation:

```bibtex
@misc{google2024veo2imagen3,
  author = {Google DeepMind},
  title = {State-of-the-art video and image generation with Veo 2 and Imagen 3},
  howpublished = {https://blog.google/technology/google-labs/video-image-generation-update-december-2024/},
  month = {December},
  year = {2024}
}
```


#### Movie Gen: A Cast of Media Foundation Models

tag: `Movie Gen` | `Meta`

paper link: [here](https://ai.meta.com/static-resource/movie-gen-research-paper)

blog link: [here](https://ai.meta.com/blog/movie-gen-media-foundation-models-generative-ai-video/)

citation: 

```bibtex
@misc{polyak2024moviegen,
      title={Movie Gen: A Cast of Media Foundation Models}, 
      author={Adam Polyak and Amit Zohar and Andrew Brown and Andros Tjandra and Animesh Sinha and Ann Lee and Apoorv Vyas and Bowen Shi and Chih-Yao Ma and Ching-Yao Chuang and David Yan and Dhruv Choudhary and Dingkang Wang and Geet Sethi and Guan Pang and Haoyu Ma and Ishan Misra and Ji Hou and Jialiang Wang and Kiran Jagadeesh and Kunpeng Li and Luxin Zhang and Mannat Singh and Mary Williamson and Matt Le and Matthew Yu and Mitesh Kumar Singh and Peizhao Zhang and Peter Vajda and Quentin Duval and Rohit Girdhar and Roshan Sumbaly and Sai Saketh Rambhatla and Sam Tsai and Samaneh Azadi and Samyak Datta and Sanyuan Chen and Sean Bell and Sharadh Ramaswamy and Shelly Sheynin and Siddharth Bhattacharya and Simran Motwani and Tao Xu and Tianhe Li and Tingbo Hou and Wei-Ning Hsu and Xi Yin and Xiaoliang Dai and Yaniv Taigman and Yaqiao Luo and Yen-Cheng Liu and Yi-Chiao Wu and Yue Zhao and Yuval Kirstain and Zecheng He and Zijian He and Albert Pumarola and Ali Thabet and Artsiom Sanakoyeu and Arun Mallya and Baishan Guo and Boris Araya and Breena Kerr and Carleigh Wood and Ce Liu and Cen Peng and Dimitry Vengertsev and Edgar Schonfeld and Elliot Blanchard and Felix Juefei-Xu and Fraylie Nord and Jeff Liang and John Hoffman and Jonas Kohler and Kaolin Fire and Karthik Sivakumar and Lawrence Chen and Licheng Yu and Luya Gao and Markos Georgopoulos and Rashel Moritz and Sara K. Sampson and Shikai Li and Simone Parmeggiani and Steve Fine and Tara Fowler and Vladan Petrovic and Yuming Du},
      year={2024},
      eprint={2410.13720},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.13720}, 
}
```

#### Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation

tag: `Hallo2` | `Fudan University`

paper link: [here](https://arxiv.org/pdf/2410.07718)

code link: [here](https://github.com/fudan-generative-vision/hallo2)

citation:

```bibtex
@misc{cui2024hallo2,
      title={Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation}, 
      author={Jiahao Cui and Hui Li and Yao Yao and Hao Zhu and Hanlin Shang and Kaihui Cheng and Hang Zhou and Siyu Zhu and Jingdong Wang},
      year={2024},
      eprint={2410.07718},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.07718}, 
}
```


#### mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models

tag: `mPLUG-Owl 3` | `Alibaba Group`

paper link: [here](https://arxiv.org/pdf/2408.04840)

code link: [here](https://github.com/X-PLUG/mPLUG-Owl)

citation:

```bibtex
@misc{ye2024mplugowl3,
      title={mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models}, 
      author={Jiabo Ye and Haiyang Xu and Haowei Liu and Anwen Hu and Ming Yan and Qi Qian and Ji Zhang and Fei Huang and Jingren Zhou},
      year={2024},
      eprint={2408.04840},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.04840}, 
}
```


#### Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion

tag: `DF` | `Diffusion Forcing` | `Teacher Forcing` | `NIPS24` | `MIT`

paper link: [here](https://openreview.net/pdf?id=yDo1ynArjj)

code link: [here](https://github.com/buoyancy99/diffusion-forcing)

citation:

```bibtex
@misc{chen2024diffusionforcingnexttokenprediction,
      title={Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion}, 
      author={Boyuan Chen and Diego Marti Monso and Yilun Du and Max Simchowitz and Russ Tedrake and Vincent Sitzmann},
      year={2024},
      eprint={2407.01392},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.01392}, 
}
```


#### Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation

tag: `Hallo` | `Fudan University`

paper link: [here](https://arxiv.org/pdf/2406.08801)

code link: [here](https://github.com/fudan-generative-vision/hallo)

homepage link: [here](https://fudan-generative-vision.github.io/hallo/#/)

follow-up work: [here](https://arxiv.org/pdf/2410.07718)

citation:

```bibtex
@misc{xu2024hallo,
      title={Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation}, 
      author={Mingwang Xu and Hui Li and Qingkun Su and Hanlin Shang and Liwei Zhang and Ce Liu and Jingdong Wang and Yao Yao and Siyu Zhu},
      year={2024},
      eprint={2406.08801},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.08801}, 
}
```


#### Video generation models as world simulators

tag: `Sora` | `OpenAI`

blog link: [here](https://openai.com/index/video-generation-models-as-world-simulators/)

homepage link: [here](https://openai.com/index/sora/)

citation:

```bibtex
@article{brooks2024video,
  author = {Brooks, Peebles, et al.},
  title = {Video Generation Models as World Simulators},
  url = {https://openai.com/index/video-generation-models-as-world-simulators/},
  year = {2024}
}
```


#### NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation

tag: `NUWA-XL` | `ACL23` | `Microsoft` | `USTC`

paper link: [here](https://arxiv.org/pdf/2303.12346)

code link: [here](https://github.com/microsoft/NUWA)

homepage link: [here](https://msra-nuwa.azurewebsites.net/)

citation:

```bibtex
@misc{yin2023nuwaxl,
      title={NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation}, 
      author={Shengming Yin and Chenfei Wu and Huan Yang and Jianfeng Wang and Xiaodong Wang and Minheng Ni and Zhengyuan Yang and Linjie Li and Shuguang Liu and Fan Yang and Jianlong Fu and Gong Ming and Lijuan Wang and Zicheng Liu and Houqiang Li and Nan Duan},
      year={2023},
      eprint={2303.12346},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.12346}, 
}
```



## Benchmark


#### Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis

tag: `Video-MME` | `USTC`

paper link: [here](https://arxiv.org/pdf/2405.21075)

code link: [here](https://github.com/BradyFU/Video-MME)

homepage link: [here](https://video-mme.github.io/)

dataset link: [here](https://github.com/BradyFU/Video-MME)

citation:

```bibtex
@misc{fu2024videomme,
      title={Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis}, 
      author={Chaoyou Fu and Yuhan Dai and Yongdong Luo and Lei Li and Shuhuai Ren and Renrui Zhang and Zihan Wang and Chenyu Zhou and Yunhang Shen and Mengdan Zhang and Peixian Chen and Yanwei Li and Shaohui Lin and Sirui Zhao and Ke Li and Tong Xu and Xiawu Zheng and Enhong Chen and Rongrong Ji and Xing Sun},
      year={2024},
      eprint={2405.21075},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.21075}, 
}
```


## Empirical Study


#### From Image to Video: An Empirical Study of Diffusion Representations

tag: `Diffusion` | `Google DeepMind`

paper link: [here](https://arxiv.org/pdf/2502.07001)

citation:

```bibtex
@misc{vélez2025imagevideoempiricalstudy,
      title={From Image to Video: An Empirical Study of Diffusion Representations}, 
      author={Pedro Vélez and Luisa F. Polanía and Yi Yang and Chuhan Zhang and Rishab Kabra and Anurag Arnab and Mehdi S. M. Sajjadi},
      year={2025},
      eprint={2502.07001},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.07001}, 
}
```