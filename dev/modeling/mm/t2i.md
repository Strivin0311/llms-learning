# Text-to-Image and Image-to-Text
*Here're are some resources about Text-to-Image and Image-to-Text modeling, understanding, generation in Multi-Modal LLMs*

## Methods


#### Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation

tag: `Janus`

paper link: [here](https://arxiv.org/pdf/2410.13848v1)

github link: [here](https://github.com/deepseek-ai/Janus)

citation:

```bibtex
@misc{wu2024janusdecouplingvisualencoding,
      title={Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation}, 
      author={Chengyue Wu and Xiaokang Chen and Zhiyu Wu and Yiyang Ma and Xingchao Liu and Zizheng Pan and Wen Liu and Zhenda Xie and Xingkai Yu and Chong Ruan and Ping Luo},
      year={2024},
      eprint={2410.13848},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.13848}, 
}
```


#### Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders

tag: `Eagle`

paper link: [here](https://arxiv.org/pdf/2408.15998)

github link: [here](https://github.com/NVlabs/Eagle)

model-hub link: [here](https://huggingface.co/NVEagle)

citation:

```bibtex
@misc{shi2024eagleexploringdesignspace,
      title={Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders}, 
      author={Min Shi and Fuxiao Liu and Shihao Wang and Shijia Liao and Subhashree Radhakrishnan and De-An Huang and Hongxu Yin and Karan Sapra and Yaser Yacoob and Humphrey Shi and Bryan Catanzaro and Andrew Tao and Jan Kautz and Zhiding Yu and Guilin Liu},
      year={2024},
      eprint={2408.15998},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.15998}, 
}
```


#### Show-o: One Single Transformer to Unify Multimodal Understanding and Generation

tag: `Show-o`

paper link: [here](https://arxiv.org/pdf/2408.12528)

github link: [here](https://github.com/showlab/Show-o)

homepage link: [here](https://showlab.github.io/Show-o/)

model link: [here](https://huggingface.co/showlab/show-o)

citation:

```bibtex
@misc{xie2024showosingletransformerunify,
      title={Show-o: One Single Transformer to Unify Multimodal Understanding and Generation}, 
      author={Jinheng Xie and Weijia Mao and Zechen Bai and David Junhao Zhang and Weihao Wang and Kevin Qinghong Lin and Yuchao Gu and Zhijie Chen and Zhenheng Yang and Mike Zheng Shou},
      year={2024},
      eprint={2408.12528},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.12528}, 
}
```


#### RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness

tag: `RLAIF-V`

paper link: [here](https://arxiv.org/pdf/2405.17220)

github link: [here](https://github.com/RLHF-V/RLAIF-V)

citation:

```bibtex
@misc{yu2024rlaifvaligningmllmsopensource,
      title={RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness}, 
      author={Tianyu Yu and Haoye Zhang and Yuan Yao and Yunkai Dang and Da Chen and Xiaoman Lu and Ganqu Cui and Taiwen He and Zhiyuan Liu and Tat-Seng Chua and Maosong Sun},
      year={2024},
      eprint={2405.17220},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.17220}, 
}
```


#### mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration

tag: `mPLUG-Owl 2`

paper link: [here](https://arxiv.org/pdf/2311.04257)

github link: [here](https://github.com/X-PLUG/mPLUG-Owl)

follow-up work: [here](https://arxiv.org/pdf/2408.04840)

citation:

```bibtex
@misc{ye2023mplugowl2revolutionizingmultimodallarge,
      title={mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration}, 
      author={Qinghao Ye and Haiyang Xu and Jiabo Ye and Ming Yan and Anwen Hu and Haowei Liu and Qi Qian and Ji Zhang and Fei Huang and Jingren Zhou},
      year={2023},
      eprint={2311.04257},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.04257}, 
}
```


#### AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts

tag: `AdaMV-MoE`

paper link: [here](http://openaccess.thecvf.com/content/ICCV2023/papers/Chen_AdaMV-MoE_Adaptive_Multi-Task_Vision_Mixture-of-Experts_ICCV_2023_paper.pdf)

citation:

```bibtex
@inproceedings{chen2023adamv,
  title={AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts},
  author={Chen, Tianlong and Chen, Xuxi and Du, Xianzhi and Rashwan, Abdullah and Yang, Fan and Chen, Huizhong and Wang, Zhangyang and Li, Yeqing},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={17346--17357},
  year={2023}
}
```

#### mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality

tag: `mPLUG-Owl`

paper link: [here](https://arxiv.org/pdf/2304.14178)

github link: [here](https://github.com/X-PLUG/mPLUG-Owl)

follow-up work: [here](https://arxiv.org/pdf/2311.04257)

citation:

```bibtex
@misc{ye2024mplugowlmodularizationempowerslarge,
      title={mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality}, 
      author={Qinghao Ye and Haiyang Xu and Guohai Xu and Jiabo Ye and Ming Yan and Yiyang Zhou and Junyang Wang and Anwen Hu and Pengcheng Shi and Yaya Shi and Chenliang Li and Yuanhong Xu and Hehong Chen and Junfeng Tian and Qi Qian and Ji Zhang and Fei Huang and Jingren Zhou},
      year={2024},
      eprint={2304.14178},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.14178}, 
}
```


#### Visual Instruction Tuning

tag: `LLaVA`

paper link: [here](https://arxiv.org/pdf/2304.08485)

github link: [here](https://github.com/haotian-liu/LLaVA)

homepage link: [here](https://llava-vl.github.io/)

citation:

```bibtex
@misc{liu2023visualinstructiontuning,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.08485}, 
}
```


#### Scalable Diffusion Models with Transformers

tag: `DiT`

paper link: [here](https://arxiv.org/pdf/2212.09748)

github link: [here](https://www.wpeebles.com/DiT)

homepage link: [here](https://www.wpeebles.com/DiT)

citation:

```bibtex
@misc{peebles2023scalablediffusionmodelstransformers,
      title={Scalable Diffusion Models with Transformers}, 
      author={William Peebles and Saining Xie},
      year={2023},
      eprint={2212.09748},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2212.09748}, 
}
```


#### Reproducible scaling laws for contrastive language-image learning

tag: `OpenCLIP`

paper link: [here](https://arxiv.org/pdf/2212.07143)

github link: [here](https://github.com/LAION-AI/scaling-laws-openclip)

citation:

```bibtex
@inproceedings{Cherti_2023,
   title={Reproducible Scaling Laws for Contrastive Language-Image Learning},
   url={http://dx.doi.org/10.1109/CVPR52729.2023.00276},
   DOI={10.1109/cvpr52729.2023.00276},
   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
   year={2023},
   month=jun }

```


#### Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise

tag: `Cold Diffusion`

paper link: [here](https://arxiv.org/pdf/2208.09392)

github link: [here](https://github.com/arpitbansal297/Cold-Diffusion-Models)

citation:

```bibtex
@misc{bansal2022colddiffusioninvertingarbitrary,
      title={Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise}, 
      author={Arpit Bansal and Eitan Borgnia and Hong-Min Chu and Jie S. Li and Hamid Kazemi and Furong Huang and Micah Goldblum and Jonas Geiping and Tom Goldstein},
      year={2022},
      eprint={2208.09392},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2208.09392}, 
}
```


#### Autoregressive Image Generation using Residual Quantization

tag: `RQ-Transformer` | `RQ-VAE`

paper link: [here](https://arxiv.org/pdf/2203.01941)

github link: [here](https://github.com/kakaobrain/rq-vae-transformer)

citation:

```bibtex
@misc{lee2022autoregressiveimagegenerationusing,
      title={Autoregressive Image Generation using Residual Quantization}, 
      author={Doyup Lee and Chiheon Kim and Saehoon Kim and Minsu Cho and Wook-Shin Han},
      year={2022},
      eprint={2203.01941},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2203.01941}, 
}
```


#### High-Resolution Image Synthesis with Latent Diffusion Models

tag: `Latent Diffusion` | `LDM`

paper link: [here](https://arxiv.org/pdf/2112.10752)

github link: [here](https://github.com/CompVis/latent-diffusion)

citation:

```bibtex
@misc{rombach2022highresolutionimagesynthesislatent,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2022},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.10752}, 
}
```


### SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations

tag: `SDEdit`

paper link: [here](https://arxiv.org/pdf/2108.01073)

github link: [here](https://github.com/ermongroup/SDEdit)

homepage link: [here](https://sde-image-editing.github.io/)

citation:

```bibtex
@misc{meng2022sdeditguidedimagesynthesis,
      title={SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations}, 
      author={Chenlin Meng and Yutong He and Yang Song and Jiaming Song and Jiajun Wu and Jun-Yan Zhu and Stefano Ermon},
      year={2022},
      eprint={2108.01073},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2108.01073}, 
}
```


#### Diffusion Models Beat GANs on Image Synthesis

tag: `Guided Diffusion`

paper link: [here](https://arxiv.org/pdf/2105.05233)

github link: [here](https://github.com/openai/guided-diffusion)

citation:

```bibtex
@misc{dhariwal2021diffusionmodelsbeatgans,
      title={Diffusion Models Beat GANs on Image Synthesis}, 
      author={Prafulla Dhariwal and Alex Nichol},
      year={2021},
      eprint={2105.05233},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2105.05233}, 
}
```


#### Swin Transformer: Hierarchical Vision Transformer using Shifted Windows

tag: `Swin Transformer`

paper link: [here](https://arxiv.org/pdf/2103.14030)

github link: [here](https://github.com/microsoft/Swin-Transformer)

citation:

```bibtex
@misc{liu2021swintransformerhierarchicalvision,
      title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}, 
      author={Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
      year={2021},
      eprint={2103.14030},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.14030}, 
}
```


#### Learning Transferable Visual Models From Natural Language Supervision

tag: `CLIP`

paper link: [here](https://arxiv.org/pdf/2103.00020)

github link: [here](https://github.com/OpenAI/CLIP)

citation:

```bibtex
@misc{radford2021learningtransferablevisualmodels,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}
```

#### Zero-Shot Text-to-Image Generation

tag: `DALL-E`

paper link: [here](https://arxiv.org/pdf/2102.12092)

github link: [here](https://github.com/openai/DALL-E)

citation:

```bibtex
@misc{ramesh2021zeroshottexttoimagegeneration,
      title={Zero-Shot Text-to-Image Generation}, 
      author={Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
      year={2021},
      eprint={2102.12092},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2102.12092}, 
}
```


#### Taming Transformers for High-Resolution Image Synthesis

tag: `Taming Transformer` | `VQGAN`

paper link: [here](https://arxiv.org/pdf/2012.09841)

github link: [here](https://github.com/CompVis/taming-transformers)

citation:

```bibtex
@misc{esser2021tamingtransformershighresolutionimage,
      title={Taming Transformers for High-Resolution Image Synthesis}, 
      author={Patrick Esser and Robin Rombach and Björn Ommer},
      year={2021},
      eprint={2012.09841},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2012.09841}, 
}
```

#### Score-Based Generative Modeling through Stochastic Differential Equations

tag: `SDE`

paper link: [here](https://arxiv.org/pdf/2011.13456)

github link: [here](https://github.com/CompVis/latent-diffusion)

citation:

```bibtex
@misc{song2021scorebasedgenerativemodelingstochastic,
      title={Score-Based Generative Modeling through Stochastic Differential Equations}, 
      author={Yang Song and Jascha Sohl-Dickstein and Diederik P. Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
      year={2021},
      eprint={2011.13456},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2011.13456}, 
}
```


#### An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale

tag: `ViT` | `Vision Transformer`

paper link: [here](https://arxiv.org/pdf/2010.11929)

github link: [here](https://github.com/google-research/vision_transformer)

citation:

```bibtex
@misc{dosovitskiy2021imageworth16x16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}
```


#### Denoising Diffusion Implicit Models

tag: `DDIM`

paper link: [here](https://arxiv.org/pdf/2010.02502)

citation:

```bibtex
@misc{song2022denoisingdiffusionimplicitmodels,
      title={Denoising Diffusion Implicit Models}, 
      author={Jiaming Song and Chenlin Meng and Stefano Ermon},
      year={2022},
      eprint={2010.02502},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2010.02502}, 
}
```


#### Denoising Diffusion Probabilistic Models

tag: `DDPM`

paper link: [here](https://arxiv.org/pdf/2006.11239)

github link: [here](https://github.com/hojonathanho/diffusion)

citation:

```bibtex
@misc{ho2020denoisingdiffusionprobabilisticmodels,
      title={Denoising Diffusion Probabilistic Models}, 
      author={Jonathan Ho and Ajay Jain and Pieter Abbeel},
      year={2020},
      eprint={2006.11239},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.11239}, 
}
```


#### Generative Modeling by Estimating Gradients of the Data Distribution

tag: `NCSN` | `SMLD` | `Score matching` | `Score Function`

paper link: [here](https://arxiv.org/pdf/1907.05600)

citation:

```bibtex
@misc{song2020generativemodelingestimatinggradients,
      title={Generative Modeling by Estimating Gradients of the Data Distribution}, 
      author={Yang Song and Stefano Ermon},
      year={2020},
      eprint={1907.05600},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1907.05600}, 
}
```


## Benchmark


#### Are We on the Right Way for Evaluating Large Vision-Language Models?

tag: `MMStar`

paper link: [here](https://arxiv.org/pdf/2403.20330)

github link: [here](https://github.com/MMStar-Benchmark/MMStar)

homepage link: [here](https://mmstar-benchmark.github.io/)

dataset link: [here](https://huggingface.co/datasets/Lin-Chen/MMStar)

citation:

```bibtex
@misc{chen2024rightwayevaluatinglarge,
      title={Are We on the Right Way for Evaluating Large Vision-Language Models?}, 
      author={Lin Chen and Jinsong Li and Xiaoyi Dong and Pan Zhang and Yuhang Zang and Zehui Chen and Haodong Duan and Jiaqi Wang and Yu Qiao and Dahua Lin and Feng Zhao},
      year={2024},
      eprint={2403.20330},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.20330}, 
}
```

#### MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI

tag: `MMMU`

paper link: [here](https://arxiv.org/pdf/2311.16502)

github link: [here](https://github.com/MMMU-Benchmark/MMMU)

homepage link: [here](https://mmmu-benchmark.github.io/)

dataset link: [here](https://huggingface.co/datasets/MMMU/MMMU)

citation:

```bibtex
@misc{yue2024mmmumassivemultidisciplinemultimodal,
      title={MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI}, 
      author={Xiang Yue and Yuansheng Ni and Kai Zhang and Tianyu Zheng and Ruoqi Liu and Ge Zhang and Samuel Stevens and Dongfu Jiang and Weiming Ren and Yuxuan Sun and Cong Wei and Botao Yu and Ruibin Yuan and Renliang Sun and Ming Yin and Boyuan Zheng and Zhenzhu Yang and Yibo Liu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
      year={2024},
      eprint={2311.16502},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.16502}, 
}
```


#### MMBench: Is Your Multi-modal Model an All-around Player?

tag: `MMBench`

paper link: [here](https://arxiv.org/pdf/2307.06281)

github link: [here](https://github.com/open-compass/VLMEvalKit)

dataset link: [here](https://github.com/open-compass/mmbench/)

citation:

```bibtex
@misc{liu2024mmbenchmultimodalmodelallaround,
      title={MMBench: Is Your Multi-modal Model an All-around Player?}, 
      author={Yuan Liu and Haodong Duan and Yuanhan Zhang and Bo Li and Songyang Zhang and Wangbo Zhao and Yike Yuan and Jiaqi Wang and Conghui He and Ziwei Liu and Kai Chen and Dahua Lin},
      year={2024},
      eprint={2307.06281},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.06281}, 
}
```


## Empirical Study


#### A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise

tag: `GPT-4V` | `Gemini` | `Awesome Multi-Modal LLMs`

paper link: [here](https://arxiv.org/pdf/2312.12436)

github link: [here](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)

citation:

```bibtex
@misc{fu2023challengergpt4vearlyexplorations,
      title={A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise}, 
      author={Chaoyou Fu and Renrui Zhang and Zihan Wang and Yubo Huang and Zhengye Zhang and Longtian Qiu and Gaoxiang Ye and Yunhang Shen and Mengdan Zhang and Peixian Chen and Sirui Zhao and Shaohui Lin and Deqiang Jiang and Di Yin and Peng Gao and Ke Li and Hongsheng Li and Xing Sun},
      year={2023},
      eprint={2312.12436},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.12436}, 
}
```


## Survey

#### Diffusion Models: A Comprehensive Survey of Methods and Applications

tag: `Diffusion Survey`

paper link: [here](https://arxiv.org/pdf/2209.00796)

github link: [here](https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy)

citation:

```bibtex
@misc{yang2024diffusionmodelscomprehensivesurvey,
      title={Diffusion Models: A Comprehensive Survey of Methods and Applications}, 
      author={Ling Yang and Zhilong Zhang and Yang Song and Shenda Hong and Runsheng Xu and Yue Zhao and Wentao Zhang and Bin Cui and Ming-Hsuan Yang},
      year={2024},
      eprint={2209.00796},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.00796}, 
}
```