# Activation Functions
*Here're some resources about Activation Functions designed for Transformer-based LLMs beyond softmax*


#### Primer: Searching for Efficient Transformers for Language Modeling (Squared ReLU)

paper link: [here](https://arxiv.org/pdf/2109.08668.pdf)

citation:
```bibtex
@misc{so2022primer,
      title={Primer: Searching for Efficient Transformers for Language Modeling}, 
      author={David R. So and Wojciech Ma≈Ñke and Hanxiao Liu and Zihang Dai and Noam Shazeer and Quoc V. Le},
      year={2022},
      eprint={2109.08668},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Glu variants improve transformer (ReGLU)

paper link: [here](https://arxiv.org/pdf/2002.05202.pdf)

citation:
```bibtex
@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}
```


#### Deep Learning using Rectified Linear Units (ReLU)

paper link: [here](https://arxiv.org/pdf/1803.08375.pdf)

citation:
```bibtex
@misc{agarap2019deep,
      title={Deep Learning using Rectified Linear Units (ReLU)}, 
      author={Abien Fred Agarap},
      year={2019},
      eprint={1803.08375},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
```


#### Searching for activation functions (Swish)

paper link: [here](https://arxiv.org/pdf/1710.05941.pdf)

citation:
```bibtex
@article{ramachandran2017searching,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
}
```


#### Gaussian Error Linear Units (GELUs)

paper link: [here](https://arxiv.org/pdf/1606.08415.pdf)

citation:
```bibtex
@misc{hendrycks2023gaussian,
      title={Gaussian Error Linear Units (GELUs)}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2023},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)

paper link: [here](https://arxiv.org/pdf/1511.07289.pdf)

citation:
```bibtex
@article{clevert2015fast,
  title={Fast and accurate deep network learning by exponential linear units (elus)},
  author={Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:1511.07289},
  year={2015}
}
```