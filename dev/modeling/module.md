# Submodule Design of LLMs
*Here're some resources about submodule design of Large Language Models*

## Activation Functions

#### Primer: Searching for Efficient Transformers for Language Modeling (Squared ReLU)

paper link: [here](https://arxiv.org/pdf/2109.08668.pdf)

citation:
```bibtex
@misc{so2022primer,
      title={Primer: Searching for Efficient Transformers for Language Modeling}, 
      author={David R. So and Wojciech Ma≈Ñke and Hanxiao Liu and Zihang Dai and Noam Shazeer and Quoc V. Le},
      year={2022},
      eprint={2109.08668},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Glu variants improve transformer (ReGLU)

paper link: [here](https://arxiv.org/pdf/2002.05202.pdf)

citation:
```bibtex
@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}
```


#### Deep Learning using Rectified Linear Units (ReLU)

paper link: [here](https://arxiv.org/pdf/1803.08375.pdf)

citation:
```bibtex
@misc{agarap2019deep,
      title={Deep Learning using Rectified Linear Units (ReLU)}, 
      author={Abien Fred Agarap},
      year={2019},
      eprint={1803.08375},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
```


#### Searching for activation functions (SwiGLU)

paper link: [here](https://arxiv.org/pdf/1710.05941.pdf)

citation:
```bibtex
@article{ramachandran2017searching,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
}
```


#### Gaussian Error Linear Units (GELUs)

paper link: [here](https://arxiv.org/pdf/1606.08415.pdf)

citation:
```bibtex
@misc{hendrycks2023gaussian,
      title={Gaussian Error Linear Units (GELUs)}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2023},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Deep Learning with S-shaped Rectified Linear Activation Units (SReLU)

paper link: [here](https://arxiv.org/pdf/1512.07030)

citation:

```bibtex
@misc{jin2015deeplearningsshapedrectified,
      title={Deep Learning with S-shaped Rectified Linear Activation Units}, 
      author={Xiaojie Jin and Chunyan Xu and Jiashi Feng and Yunchao Wei and Junjun Xiong and Shuicheng Yan},
      year={2015},
      eprint={1512.07030},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.07030}, 
}
```


#### Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)

paper link: [here](https://arxiv.org/pdf/1511.07289.pdf)

citation:
```bibtex
@article{clevert2015fast,
  title={Fast and accurate deep network learning by exponential linear units (elus)},
  author={Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:1511.07289},
  year={2015}
}
```

## Normalization

#### DeepNet: Scaling Transformers to 1,000 Layers (DeepNorm)

paper link: [here](https://arxiv.org/pdf/2203.00555)

github link: [here](https://github.com/microsoft/unilm)

citation:

```bibtex
@misc{wang2022deepnetscalingtransformers1000,
      title={DeepNet: Scaling Transformers to 1,000 Layers}, 
      author={Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Dongdong Zhang and Furu Wei},
      year={2022},
      eprint={2203.00555},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.00555}, 
}
```


#### On Layer Normalization in the Transformer Architecture

paper link: [here](https://arxiv.org/pdf/2002.04745)

citation:

```bibtex
@misc{xiong2020layernormalizationtransformerarchitecture,
      title={On Layer Normalization in the Transformer Architecture}, 
      author={Ruibin Xiong and Yunchang Yang and Di He and Kai Zheng and Shuxin Zheng and Chen Xing and Huishuai Zhang and Yanyan Lan and Liwei Wang and Tie-Yan Liu},
      year={2020},
      eprint={2002.04745},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.04745}, 
}
```
