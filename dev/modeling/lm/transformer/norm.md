# Normalization
*Here're some resources about Normalization modules in Transformers*


#### SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization

tag: `SLAB` | `SLA` | `PRepBN` | `ICML24` | `Huawei Noahâ€™s Ark Lab`

paper link: [here](https://arxiv.org/pdf/2405.11582)

code link: [here](https://github.com/xinghaochen/SLAB)

citation:

```bibtex
@misc{guo2024slabefficienttransformerssimplified,
      title={SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization}, 
      author={Jialong Guo and Xinghao Chen and Yehui Tang and Yunhe Wang},
      year={2024},
      eprint={2405.11582},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.11582}, 
}
```


#### DeepNet: Scaling Transformers to 1,000 Layers

tag: `DeepNorm` | `DeepNet` | `Microsoft`

paper link: [here](https://arxiv.org/pdf/2203.00555)

code link: [here](https://github.com/microsoft/unilm)

citation:

```bibtex
@misc{wang2022deepnetscalingtransformers1000,
      title={DeepNet: Scaling Transformers to 1,000 Layers}, 
      author={Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Dongdong Zhang and Furu Wei},
      year={2022},
      eprint={2203.00555},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.00555}, 
}
```


#### On Layer Normalization in the Transformer Architecture

tag: `LayerNorm` | `LN` | `ICML20` | `MSRA`

paper link: [here](https://arxiv.org/pdf/2002.04745)

citation:

```bibtex
@misc{xiong2020layernormalizationtransformerarchitecture,
      title={On Layer Normalization in the Transformer Architecture}, 
      author={Ruibin Xiong and Yunchang Yang and Di He and Kai Zheng and Shuxin Zheng and Chen Xing and Huishuai Zhang and Yanyan Lan and Liwei Wang and Tie-Yan Liu},
      year={2020},
      eprint={2002.04745},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.04745}, 
}
```


#### Understanding and Improving Layer Normalization

tag: `AdaNorm` | `NIPS19` | `Peking University`

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2019/file/2f4fe03d77724a7217006e5d16728874-Paper.pdf)

citation:

```bibtex
@misc{xu2019understandingimprovinglayernormalization,
      title={Understanding and Improving Layer Normalization}, 
      author={Jingjing Xu and Xu Sun and Zhiyuan Zhang and Guangxiang Zhao and Junyang Lin},
      year={2019},
      eprint={1911.07013},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1911.07013}, 
}
```


#### Root Mean Square Layer Normalization

tag: `RMSNorm` | `NIPS19`

paper link: [here](https://arxiv.org/pdf/1910.07467)

code link: [here](https://github.com/bzhangGo/rmsnorm)

citation:

```bibtex
@misc{zhang2019rootmeansquarelayer,
      title={Root Mean Square Layer Normalization}, 
      author={Biao Zhang and Rico Sennrich},
      year={2019},
      eprint={1910.07467},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.07467}, 
}
```


#### Group Normalization

tag: `GroupNorm` | `GN` | `ECCV18` | `Meta`

paper link: [here](https://openaccess.thecvf.com/content_ECCV_2018/papers/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.pdf)

code link: [here](https://github.com/facebookresearch/Detectron/tree/main/projects/GN)

citation:

```bibtex
@misc{wu2018groupnormalization,
      title={Group Normalization}, 
      author={Yuxin Wu and Kaiming He},
      year={2018},
      eprint={1803.08494},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1803.08494}, 
}
```


#### Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift

tag: `BatchNorm` | `BN` | `Internal Covariate Shift` | `ICML15` | `Google`

paper link: [here](https://arxiv.org/pdf/1502.03167)

citation:

```bibtex
@misc{ioffe2015batchnormalizationacceleratingdeep,
      title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
      author={Sergey Ioffe and Christian Szegedy},
      year={2015},
      eprint={1502.03167},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1502.03167}, 
}
```