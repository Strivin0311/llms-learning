# Miscellaneous Architectures for language/sequence modeling
*Here're some resources about Miscellaneous Architectures for language/sequence modeling*


#### You Only Cache Once: Decoder-Decoder Architectures for Language Models

tag: `YOCO` | `Microsoft` | `Tsinghua University`

paper link: [here](https://arxiv.org/pdf/2405.05254)

github link: [here](https://github.com/microsoft/unilm/tree/master/YOCO)

citation:

```bibtex
@misc{sun2024cacheoncedecoderdecoderarchitectures,
      title={You Only Cache Once: Decoder-Decoder Architectures for Language Models}, 
      author={Yutao Sun and Li Dong and Yi Zhu and Shaohan Huang and Wenhui Wang and Shuming Ma and Quanlu Zhang and Jianyong Wang and Furu Wei},
      year={2024},
      eprint={2405.05254},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.05254}, 
}
```

#### Kolmogorov-Arnold Transformer

tag: `KAT` | `NUS`

paper link: [here](https://arxiv.org/pdf/2409.10594)

github link: [here](https://github.com/Adamdad/kat)

citation:

```bibtex
@misc{yang2024kolmogorovarnoldtransformer,
      title={Kolmogorov-Arnold Transformer}, 
      author={Xingyi Yang and Xinchao Wang},
      year={2024},
      eprint={2409.10594},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.10594}, 
}
```


#### Scalable MatMul-free Language Modeling

tag: `Matmul-free LM` | `UCSC`

paper link: [here](https://arxiv.org/pdf/2406.02528)

github link: [here](https://github.com/ridgerchu/matmulfreellm)

citation:

```bibtex
@misc{zhu2024scalablematmulfreelanguagemodeling,
      title={Scalable MatMul-free Language Modeling}, 
      author={Rui-Jie Zhu and Yu Zhang and Ethan Sifferman and Tyler Sheaves and Yiqiao Wang and Dustin Richmond and Peng Zhou and Jason K. Eshraghian},
      year={2024},
      eprint={2406.02528},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
      url={https://arxiv.org/abs/2406.02528}, 
}
```

#### Addition is All You Need for Energy-efficient Language Models

tag: `L-Mul` | `BitEnergy AI`

paper link: [here](https://arxiv.org/pdf/2410.00907)

citation:

```bibtex
@misc{luo2024additionneedenergyefficientlanguage,
      title={Addition is All You Need for Energy-efficient Language Models}, 
      author={Hongyin Luo and Wei Sun},
      year={2024},
      eprint={2410.00907},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.00907}, 
}
```


#### Monarch Mixer: A simple sub-quadratic GEMM-based architecture

tag: `M2` | `Monarch Mixer` | `Standford University`

paper link: [here](https://arxiv.org/pdf/2310.12109)

citation: 

```bibtex
@article{fu2023monarch,
  title={Monarch Mixer: A simple sub-quadratic GEMM-based architecture},
  author={Fu, Daniel Y and Arora, Simran and Grogan, Jessica and Johnson, Isys and Eyuboglu, Sabri and Thomas, Armin W and Spector, Benjamin and Poli, Michael and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2310.12109},
  year={2023}
}
```


#### An Attention Free Transformer

tag: `AFT` | `Apple`

paper link: [here](https://arxiv.org/pdf/2105.14103)

citation:

```bibtex
@misc{zhai2021attentionfreetransformer,
      title={An Attention Free Transformer}, 
      author={Shuangfei Zhai and Walter Talbott and Nitish Srivastava and Chen Huang and Hanlin Goh and Ruixiang Zhang and Josh Susskind},
      year={2021},
      eprint={2105.14103},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2105.14103}, 
}
```


#### The reversible residual network: Backpropagation without storing activations

tag: `RevNet` | `NIPS17`

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2017/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf)

github link: [here](https://github.com/renmengye/revnet-public)

citation:

```bibtex
@article{gomez2017reversible,
  title={The reversible residual network: Backpropagation without storing activations},
  author={Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
```