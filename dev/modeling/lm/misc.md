# Miscellaneous Architectures for language/sequence modeling
*Here're some resources about Miscellaneous Architectures for language/sequence modeling*


#### Byte Latent Transformer: Patches Scale Better Than Tokens

tag: `BLT` | `Meta` | `University of Washington`

paper link: [here](https://arxiv.org/pdf/2412.09871)

code link: [here](https://github.com/facebookresearch/blt)

citation:

```bibtex
@misc{pagnoni2024bytelatenttransformerpatches,
      title={Byte Latent Transformer: Patches Scale Better Than Tokens}, 
      author={Artidoro Pagnoni and Ram Pasunuru and Pedro Rodriguez and John Nguyen and Benjamin Muller and Margaret Li and Chunting Zhou and Lili Yu and Jason Weston and Luke Zettlemoyer and Gargi Ghosh and Mike Lewis and Ari Holtzman and Srinivasan Iyer},
      year={2024},
      eprint={2412.09871},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.09871}, 
}
```


#### FlashRNN: Optimizing Traditional RNNs on Modern Hardware

tag: `FlashRNN` | `NXAI Lab` | `JKU`

paper link: [here](https://arxiv.org/pdf/2412.07752)

code link: [here](https://github.com/NX-AI/flashrnn)

citation:

```bibtex
@misc{pöppel2024flashrnnoptimizingtraditionalrnns,
      title={FlashRNN: Optimizing Traditional RNNs on Modern Hardware}, 
      author={Korbinian Pöppel and Maximilian Beck and Sepp Hochreiter},
      year={2024},
      eprint={2412.07752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.07752}, 
}
```


#### TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters

tag: `TokenFormer` | `Pattention` | `Google` | `Peking University`

paper link: [here](https://arxiv.org/pdf/2410.23168)

code link: [here](https://github.com/Haiyang-W/TokenFormer)

citation:

```bibtex
@misc{wang2024tokenformerrethinkingtransformerscaling,
      title={TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters}, 
      author={Haiyang Wang and Yue Fan and Muhammad Ferjad Naeem and Yongqin Xian and Jan Eric Lenssen and Liwei Wang and Federico Tombari and Bernt Schiele},
      year={2024},
      eprint={2410.23168},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.23168}, 
}
```

#### An Evolved Universal Transformer Memory

tag: `NAMMs` | `EMA` | `BAM` | `Sakana AI`

paper link: [here](https://arxiv.org/pdf/2410.13166)

code link: [here](https://github.com/SakanaAI/evo-memory)

citation:

```bibtex
@misc{cetin2024evolveduniversaltransformermemory,
      title={An Evolved Universal Transformer Memory}, 
      author={Edoardo Cetin and Qi Sun and Tianyu Zhao and Yujin Tang},
      year={2024},
      eprint={2410.13166},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.13166}, 
}
```


#### LoLCATs: On Low-Rank Linearizing of Large Language Models

tag: `LoLCATs` | `Attention Transfer` | `LoRA` | `Together AI` | `Standford University` | `MIT`

paper link: [here](https://arxiv.org/pdf/2410.10254)

code link: [here](https://github.com/HazyResearch/lolcats)

citation:

```bibtex
@misc{zhang2024lolcatslowranklinearizinglarge,
      title={LoLCATs: On Low-Rank Linearizing of Large Language Models}, 
      author={Michael Zhang and Simran Arora and Rahul Chalamala and Alan Wu and Benjamin Spector and Aaryan Singhal and Krithik Ramesh and Christopher Ré},
      year={2024},
      eprint={2410.10254},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.10254}, 
}
```


#### FAN: Fourier Analysis Networks

tag: `FAN` | `ByteDance` | `Peking University`

paper link: [here](https://arxiv.org/pdf/2410.02675)

code link: [here](https://github.com/YihongDong/FAN)

citation:

```bibtex
@misc{dong2024fanfourieranalysisnetworks,
      title={FAN: Fourier Analysis Networks}, 
      author={Yihong Dong and Ge Li and Yongding Tao and Xue Jiang and Kechi Zhang and Jia Li and Jing Su and Jun Zhang and Jingjing Xu},
      year={2024},
      eprint={2410.02675},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.02675}, 
}
```


#### Model Comparisons: XNet Outperforms KAN

tag: `XNet` | `KAN`

paper link: [here](https://arxiv.org/pdf/2410.02033)

citation:

```bibtex
@misc{li2024modelcomparisonsxnetoutperforms,
      title={Model Comparisons: XNet Outperforms KAN}, 
      author={Xin Li and Zhihong Jeff Xia and Xiaotao Zheng},
      year={2024},
      eprint={2410.02033},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.02033}, 
}
```


#### Addition is All You Need for Energy-efficient Language Models

tag: `L-Mul` | `BitEnergy AI`

paper link: [here](https://arxiv.org/pdf/2410.00907)

citation:

```bibtex
@misc{luo2024additionneedenergyefficientlanguage,
      title={Addition is All You Need for Energy-efficient Language Models}, 
      author={Hongyin Luo and Wei Sun},
      year={2024},
      eprint={2410.00907},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.00907}, 
}
```


#### Cauchy activation function and XNet

tag: `XNet` | `Cauchy Activation Function` | `Cauchy Integral Theorem`

paper link: [here](https://arxiv.org/pdf/2409.19221)

follow-up work: [here](https://arxiv.org/pdf/2410.02033)

citation:

```bibtex
@misc{li2024cauchyactivationfunctionxnet,
      title={Cauchy activation function and XNet}, 
      author={Xin Li and Zhihong Xia and Hongkun Zhang},
      year={2024},
      eprint={2409.19221},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.19221}, 
}
```


#### Scalable MatMul-free Language Modeling

tag: `Matmul-free LM` | `UCSC`

paper link: [here](https://arxiv.org/pdf/2406.02528)

code link: [here](https://github.com/ridgerchu/matmulfreellm)

citation:

```bibtex
@misc{zhu2024scalablematmulfreelanguagemodeling,
      title={Scalable MatMul-free Language Modeling}, 
      author={Rui-Jie Zhu and Yu Zhang and Ethan Sifferman and Tyler Sheaves and Yiqiao Wang and Dustin Richmond and Peng Zhou and Jason K. Eshraghian},
      year={2024},
      eprint={2406.02528},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
      url={https://arxiv.org/abs/2406.02528}, 
}
```


#### You Only Cache Once: Decoder-Decoder Architectures for Language Models

tag: `YOCO` | `Decoder-Decoder` | `NIPS24` | `Microsoft` | `Tsinghua University`

paper link: [here](https://openreview.net/pdf?id=25Ioxw576r)

code link: [here](https://github.com/microsoft/unilm/tree/master/YOCO)

citation:

```bibtex
@misc{sun2024cacheoncedecoderdecoderarchitectures,
      title={You Only Cache Once: Decoder-Decoder Architectures for Language Models}, 
      author={Yutao Sun and Li Dong and Yi Zhu and Shaohan Huang and Wenhui Wang and Shuming Ma and Quanlu Zhang and Jianyong Wang and Furu Wei},
      year={2024},
      eprint={2405.05254},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.05254}, 
}
```


#### xLSTM: Extended Long Short-Term Memory

tag: `xLSTM` | `NIPS24` | `LIT AI Lab` | `NXAI Lab` | `JKU`

paper link: [here](https://openreview.net/pdf?id=ARAxPPIAhq)

code link: [here](https://github.com/NX-AI/xlstm)

citation:

```bibtex
@misc{beck2024xlstmextendedlongshortterm,
      title={xLSTM: Extended Long Short-Term Memory}, 
      author={Maximilian Beck and Korbinian Pöppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael Kopp and Günter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
      year={2024},
      eprint={2405.04517},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.04517}, 
}
```


#### Monarch Mixer: A simple sub-quadratic GEMM-based architecture

tag: `M2` | `Monarch Mixer` | `NIPS23` | `Standford University`

paper link: [here](https://openreview.net/pdf?id=cB0BImqSS9)

code link: [here](https://github.com/HazyResearch/m2)

citation: 

```bibtex
@article{fu2023monarch,
  title={Monarch Mixer: A simple sub-quadratic GEMM-based architecture},
  author={Fu, Daniel Y and Arora, Simran and Grogan, Jessica and Johnson, Isys and Eyuboglu, Sabri and Thomas, Armin W and Spector, Benjamin and Poli, Michael and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2310.12109},
  year={2023}
}
```


#### An Attention Free Transformer

tag: `AFT` | `Apple`

paper link: [here](https://arxiv.org/pdf/2105.14103)

citation:

```bibtex
@misc{zhai2021attentionfreetransformer,
      title={An Attention Free Transformer}, 
      author={Shuangfei Zhai and Walter Talbott and Nitish Srivastava and Chen Huang and Hanlin Goh and Ruixiang Zhang and Josh Susskind},
      year={2021},
      eprint={2105.14103},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2105.14103}, 
}
```


#### The reversible residual network: Backpropagation without storing activations

tag: `RevNet` | `NIPS17` | `University of Toronto`

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2017/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf)

code link: [here](https://github.com/renmengye/revnet-public)

citation:

```bibtex
@article{gomez2017reversible,
  title={The reversible residual network: Backpropagation without storing activations},
  author={Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
```