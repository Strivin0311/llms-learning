# Model Editing for LLMs
*Here're some resources about Model Editing for LLMs, including (De)composition, Fusion / Stacking / Ensembling, and even Unlearning*


#### Large Language Model Unlearning [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2310.10683.pdf)

citation:
```bibtex
@misc{yao2023large,
      title={Large Language Model Unlearning}, 
      author={Yuanshun Yao and Xiaojun Xu and Yang Liu},
      year={2023},
      eprint={2310.10683},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition [`READ`]

paper link: [here](https://arxiv.org/pdf/2307.13269)

github link: [here](https://github.com/sail-sg/lorahub)

hfhub link: [here](https://huggingface.co/lorahub)

citation:
```bibtex
@misc{huang2024lorahub,
      title={LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition}, 
      author={Chengsong Huang and Qian Liu and Bill Yuchen Lin and Tianyu Pang and Chao Du and Min Lin},
      year={2024},
      eprint={2307.13269},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```