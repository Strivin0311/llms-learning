# Pruning on LLMs
*Here're some resources about Pruning on LLMs*


#### A Simple and Effective Pruning Approach for Large Language Models (Wanda) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2306.11695.pdf)

citation:
```bibtex
@misc{sun2023simple,
      title={A Simple and Effective Pruning Approach for Large Language Models}, 
      author={Mingjie Sun and Zhuang Liu and Anna Bair and J. Zico Kolter},
      year={2023},
      eprint={2306.11695},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

#### GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints [`READ`]

paper link: [here](https://arxiv.org/pdf/2305.13245)

citation:

```bibtex
@article{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}
```


#### LLM-Pruner: On the Structural Pruning of Large Language Models [`READ`]

paper link: [here](https://arxiv.org/pdf/2305.11627.pdf)

citation:
```bibtex
@misc{ma2023llmpruner,
      title={LLM-Pruner: On the Structural Pruning of Large Language Models}, 
      author={Xinyin Ma and Gongfan Fang and Xinchao Wang},
      year={2023},
      eprint={2305.11627},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

#### Fast transformer decoding: One write-head is all you need (MQA) [`READ`]

paper link: [here](https://arxiv.org/pdf/1911.02150.pdf?trk=public_post_comment-text)

citation:

```bibtex
@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}
```


#### Are sixteen heads really better than one? [`READ`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf)

citation:

```bibtex
@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
```