# GLM
*Here're some resources about GLM*


#### Glm-130b: An open bilingual pre-trained model

paper link: [here](https://arxiv.org/pdf/2210.02414)

github link: [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B) | [ChatGLM3](https://github.com/THUDM/ChatGLM3)

model links: 

|model name|link|
|-|-|
|chatglm3-6b-128k|[here](https://huggingface.co/THUDM/chatglm3-6b-128k)|
|chatglm3-6b-32k|[here](https://huggingface.co/THUDM/chatglm3-6b-32k)|
|chatglm3-6b-base|[here](https://huggingface.co/THUDM/chatglm3-6b-base)|
|chatglm3-6b|[here](https://huggingface.co/THUDM/chatglm3-6b)|
|chatglm2-6b-32k-int4|[here](https://huggingface.co/THUDM/chatglm2-6b-32k-int4)|
|chatglm2-6b-32k|[here](https://huggingface.co/THUDM/chatglm2-6b-32k)|
|chatglm2-6b-int4|[here](https://huggingface.co/THUDM/chatglm2-6b-int4)|
|chatglm2-6b|[here](https://huggingface.co/THUDM/chatglm2-6b)|

citation: 
```bibtex
@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}
```
    


#### Glm: General language model pretraining with autoregressive blank infilling

paper link: [here](https://arxiv.org/pdf/2103.10360)

github link: [ChatGLM](https://github.com/THUDM/ChatGLM-6B)

model links: 

|model name|link|
|-|-|
|chatglm-6b-int4-qe|[here](https://huggingface.co/THUDM/chatglm-6b-int4-qe)|
|chatglm-6b-int4|[here](https://huggingface.co/THUDM/chatglm-6b-int4)|
|chatglm-6b-int8|[here](https://huggingface.co/THUDM/chatglm-6b-int8)|
|chatglm-6b|[here](https://huggingface.co/THUDM/chatglm-6b)|

citation: 
```bibtex
@article{du2021glm,
  title={Glm: General language model pretraining with autoregressive blank infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10360},
  year={2021}
}
```
    