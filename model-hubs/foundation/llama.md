# Llama
*Here're some resources about Llama*
    

#### Llama 2: Open foundation and fine-tuned chat models [`READ`]

paper link: [here](https://arxiv.org/pdf/2307.09288.pdf)

model links: 

|model name|link|
|-|-|
|Llama2-70b-chat-hf|[here](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)|
|Llama2-70b-hf|[here](https://huggingface.co/meta-llama/Llama-2-70b-hf)|
|Llama2-13b-chat-hf|[here](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)|
|Llama2-13b-hf|[here](https://huggingface.co/meta-llama/Llama-2-13b-hf)|
|Llama2-7b-chat-hf|[here](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)|
|Llama2-7b-hf|[here](https://huggingface.co/meta-llama/Llama-2-7b-hf)|

citation: 
```bibtex
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
```


#### Lima: Less is more for alignment [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2305.11206)

citation: 
```bibtex
@article{zhou2023lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={arXiv preprint arXiv:2305.11206},
  year={2023}
}
```


#### WizardLM: Empowering Large Language Models to Follow Complex Instructions [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2304.12244.pdf)

github link: [here](https://github.com/nlpxucan/WizardLM)

model links: 

|model name|link|
|-|-|
|WizardLM-7B-V1.0|[here](https://huggingface.co/WizardLM/WizardLM-7B-V1.0)|
|WizardLM-13B-V1.1|[here](https://huggingface.co/WizardLM/WizardLM-13B-V1.1)|
|WizardLM-13B-V1.2|[here](https://huggingface.co/WizardLM/WizardLM-13B-V1.2)|
|WizardLM-70B-V1.0|[here](https://huggingface.co/WizardLM/WizardLM-70B-V1.0)|

citation:
```bibtex
@misc{xu2023wizardlm,
      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, 
      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
      year={2023},
      eprint={2304.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
    

#### Llama: Open and efficient foundation language models [`READ`]

paper link: [here](https://arxiv.org/pdf/2302.13971)

citation: 
```bibtex
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
```
    