# Llama
*Here're some resources about Llama*
    

#### Introducing Meta Llama 3: The most capable openly available LLM to date

blog link: [here](https://ai.meta.com/blog/meta-llama-3/)

model card link: [here](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)

github link: [here](https://github.com/meta-llama/llama3)

model links:

|model name|link|
|-|-|
|Meta-Llama-3-70B-Instruct|[here](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)|
|Meta-Llama-3-70B|[here](https://huggingface.co/meta-llama/Meta-Llama-3-70B)|
|Meta-Llama-3-8B-Instruct|[here](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)|
|Meta-Llama-3-8B|[here](https://huggingface.co/meta-llama/Meta-Llama-3-8B)|

citation:

```bibtex
@article{llama3modelcard,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}
```

#### Llama 2: Open foundation and fine-tuned chat models

paper link: [here](https://arxiv.org/pdf/2307.09288.pdf)

model links: 

|model name|link|
|-|-|
|Llama2-70b-chat-hf|[here](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)|
|Llama2-70b-hf|[here](https://huggingface.co/meta-llama/Llama-2-70b-hf)|
|Llama2-13b-chat-hf|[here](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)|
|Llama2-13b-hf|[here](https://huggingface.co/meta-llama/Llama-2-13b-hf)|
|Llama2-7b-chat-hf|[here](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)|
|Llama2-7b-hf|[here](https://huggingface.co/meta-llama/Llama-2-7b-hf)|

citation: 
```bibtex
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
```


#### WizardLM: Empowering Large Language Models to Follow Complex Instructions

paper link: [here](https://arxiv.org/pdf/2304.12244.pdf)

github link: [here](https://github.com/nlpxucan/WizardLM)

model links: 

|model name|link|
|-|-|
|WizardLM-7B-V1.0|[here](https://huggingface.co/WizardLM/WizardLM-7B-V1.0)|
|WizardLM-13B-V1.1|[here](https://huggingface.co/WizardLM/WizardLM-13B-V1.1)|
|WizardLM-13B-V1.2|[here](https://huggingface.co/WizardLM/WizardLM-13B-V1.2)|
|WizardLM-70B-V1.0|[here](https://huggingface.co/WizardLM/WizardLM-70B-V1.0)|

citation:
```bibtex
@misc{xu2023wizardlm,
      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, 
      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
      year={2023},
      eprint={2304.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
    

#### Llama: Open and efficient foundation language models

paper link: [here](https://arxiv.org/pdf/2302.13971)

citation: 
```bibtex
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
```
    