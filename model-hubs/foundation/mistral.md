# Mistral
*Here're some resources about Mistral*


#### Starling-7B- Increasing LLM Helpfulness & Harmlessness with RLAIF [`UNREAD`]

blog link: [here](https://starling.cs.berkeley.edu/)

model links: 

|model name|link|
|-|-|
|Starling-LM-7B-alpha|[here](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha)|
|Starling-RM-7B-alpha|[here](https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha)|


citation: 
```bibtex
@misc{starling2023,
    title = {Starling-7B: Improving LLM Helpfulness & Harmlessness with RLAIF},
    author = {Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Jiao, Jiantao},
    month = {November},
    year = {2023}
}
```


#### Mixtral of experts: A high quality Sparse Mixture-of-Experts [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2401.04088)

blog link: [here](https://mistral.ai/news/mixtral-of-experts/)

model links: 

|model name|link|
|-|-|
|Mixtral-SlimOrca-8x7B|[here](https://huggingface.co/Open-Orca/Mixtral-SlimOrca-8x7B)|
|Mixtral-8x7B-Instruct-v0.1|[here](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)|
|Mixtral-8x7B-v0.1|[here](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)|

citation:
```bibtex
@misc{jiang2024mixtral,
      title={Mixtral of Experts}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2024},
      eprint={2401.04088},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
```


#### Mistral 7B [`READ`]

paper link: [here](https://arxiv.org/pdf/2310.06825.pdf)

model links: 

|model name|link|
|-|-|
|Mistral-7B-OpenOrca|[here](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca)|
|Zephyr-7B-beta|[here](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)|
|OpenHermes-2.5-Mistral-7B|[here](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)|
|OpenHermes-2.5-neural-chat-7b-v3-1-7B|[here](https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-7b-v3-1-7B)|
|Neural-Chat-7b-v3-1|[here](https://huggingface.co/Intel/neural-chat-7b-v3-1)|
|Mistral-7B-Instruct-v0.1|[here](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)|
|Mistral-7B-v0.1|[here](https://huggingface.co/mistralai/Mistral-7B-v0.1)|



citation: 
```bibtex
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
```


#### OpenChat: Advancing Open-source Language Models with Mixed-Quality Data [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2309.11235.pdf)

model link: 

|model name|link|
|-|-|
|OpenChat3.5|[here](https://huggingface.co/openchat/openchat_3.5)|
|OpenChat3.5-1210|[here](https://huggingface.co/openchat/openchat-3.5-1210)|
|||


citation:
```bibtex
@article{wang2023openchat,
  title={Openchat: Advancing open-source language models with mixed-quality data},
  author={Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
  journal={arXiv preprint arXiv:2309.11235},
  year={2023}
}
```
    