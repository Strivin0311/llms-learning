# Retrieval-Augmented Generation (RAG)
*Here're some resources about Retrieval-Augmented Generation with LLMs*

## Methods

#### MemoRAG

#### RQ-RAG

#### Self-RAG

#### Precise Zero-Shot Dense Retrieval without Relevance Labels (HyDE)

paper link: [here](https://arxiv.org/pdf/2212.10496.pdf)

citation:

```bibtex
@misc{gao2022precise,
      title={Precise Zero-Shot Dense Retrieval without Relevance Labels}, 
      author={Luyu Gao and Xueguang Ma and Jimmy Lin and Jamie Callan},
      year={2022},
      eprint={2212.10496},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}
```


#### Generate rather than retrieve: Large language models are strong context generators (GenRead)

paper link: [here](https://arxiv.org/pdf/2209.10063)

tutorial links:

|tutorial name|public date|main-lib version|notebook link|
|-|-|-|-|
|tutorial_llamaindex_rag_api|2024.01|llama-index=0.9.26|[here](../notebooks/tutorial_llamaindex_rag_api.ipynb)|
|tutorial_langchain|2023.12|langchain=0.0.352, openai=1.6.1|[here](../notebooks/tutorial_langchain.ipynb)|

citation: 
```bibtex
@article{yu2022generate,
  title={Generate rather than retrieve: Large language models are strong context generators},
  author={Yu, Wenhao and Iter, Dan and Wang, Shuohang and Xu, Yichong and Ju, Mingxuan and Sanyal, Soumya and Zhu, Chenguang and Zeng, Michael and Jiang, Meng},
  journal={arXiv preprint arXiv:2209.10063},
  year={2022}
}
```


#### Pseudo Relevance Feedback with Deep Language Models and Dense Retrievers: Successes and Pitfalls (PRF)

paper link: [here](https://arxiv.org/pdf/2108.11044.pdf)

citation:

```bibtex
@misc{li2022pseudo,
      title={Pseudo Relevance Feedback with Deep Language Models and Dense Retrievers: Successes and Pitfalls}, 
      author={Hang Li and Ahmed Mourad and Shengyao Zhuang and Bevan Koopman and Guido Zuccon},
      year={2022},
      eprint={2108.11044},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}
```


#### Retrieval-augmented generation for knowledge-intensive nlp tasks (RAG)

paper link: [here](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf)

citation: 
```bibtex
@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}
```



## Benchmarks

#### Benchmarking Large Language Models in Retrieval-Augmented Generation (RGB)

paper link: [here](https://arxiv.org/pdf/2309.01431)

github link: [here](https://github.com/chen700564/RGB)

citation:

```bibtex
@misc{chen2023benchmarking,
      title={Benchmarking Large Language Models in Retrieval-Augmented Generation}, 
      author={Jiawei Chen and Hongyu Lin and Xianpei Han and Le Sun},
      year={2023},
      eprint={2309.01431},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


## Survey


#### Retrieval-Augmented Generation for Large Language Models: A Survey

paper link: [here](https://arxiv.org/pdf/2312.10997.pdf)

github link: [here](https://github.com/Tongji-KGLLM/RAG-Survey)

citation:

```bibtex
@misc{gao2024retrievalaugmented,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Qianyu Guo and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
