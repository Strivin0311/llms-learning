{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXtGVjlDYFK6"
      },
      "source": [
        "# **Tutorial** - Keyword Extraction with KeyBERT\n",
        "(last updated 10-05-2021)\n",
        "\n",
        "In this tutorial we will be exploring how to use KeyBERT to create keywords from documents. The most frequent use-cases and methods are discussed together with important parameters to keep a look out for.\n",
        "\n",
        "\n",
        "## KeyBERT\n",
        "KeyBERT is a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings to create keywords and keyphrases that are most similar to a document.\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaartenGr/KeyBERT/master/images/logo.png\" width=\"50%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxaVXeEgYfZD"
      },
      "source": [
        "# Enabling the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "[Reference](https://colab.research.google.com/notebooks/gpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CmaBZsSYinW"
      },
      "source": [
        "# **Installing KeyBERT**\n",
        "\n",
        "We start by installing KeyBERT from PyPi:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc0V1Zd7-RFu"
      },
      "source": [
        "%%capture\n",
        "# !pip install keybert[all]\n",
        "!pip install keybert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q707beopYq33"
      },
      "source": [
        "**NOTE**: If you choose to use `keybert[all]` to install all embedding backends, then this may take a while as it needs to install Spacy, Torch, Gensim, USE, etc. If you only want to use sentence-transformers, then I would advise you to use `pip install keybert`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3l7D_iWZAo5"
      },
      "source": [
        "## Restart the Notebook\n",
        "After installing KeyBERT, some packages that were already loaded were updated and in order to correctly use them, we should now restart the notebook.\n",
        "\n",
        "From the Menu:\n",
        "\n",
        "Runtime â†’ Restart Runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waOvFU2fZKRJ"
      },
      "source": [
        "# **KeyBERT**\n",
        "Using KeyBERT is rather straightforward, we simply choose a document that we want keywords/keyphrases from and pass it through our keyword model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAaiKnQVdA00"
      },
      "source": [
        "from keybert import KeyBERT\n",
        "\n",
        "doc = \"\"\"\n",
        "         Supervised learning is the machine learning task of learning a function that\n",
        "         maps an input to an output based on example input-output pairs.[1] It infers a\n",
        "         function from labeled training data consisting of a set of training examples.[2]\n",
        "         In supervised learning, each example is a pair consisting of an input object\n",
        "         (typically a vector) and a desired output value (also called the supervisory signal).\n",
        "         A supervised learning algorithm analyzes the training data and produces an inferred function,\n",
        "         which can be used for mapping new examples. An optimal scenario will allow for the\n",
        "         algorithm to correctly determine the class labels for unseen instances. This requires\n",
        "         the learning algorithm to generalize from the training data to unseen situations in a\n",
        "         'reasonable' way (see inductive bias).\n",
        "      \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJUDiB9ogUkF",
        "outputId": "aec1e090-c42d-4a5f-d957-7e3fa0a4f479"
      },
      "source": [
        "kw_model = KeyBERT()\n",
        "kw_model.extract_keywords(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('learning', 0.4605),\n",
              " ('algorithm', 0.4556),\n",
              " ('training', 0.4488),\n",
              " ('class', 0.4087),\n",
              " ('mapping', 0.3701)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cKMnkkGdWkH"
      },
      "source": [
        "**NOTE**: Use `model=\"xlm-r-bert-base-nli-stsb-mean-tokens\"` to select a model that support 50+ languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQag6gocdo_Y"
      },
      "source": [
        "## Keyphrase Length\n",
        "You can set `keyphrase_ngram_range` to set the length of the resulting keywords/keyphrases:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2b4QBQvdutp",
        "outputId": "7ad405e3-a3c8-4702-f775-ddee12c99a74"
      },
      "source": [
        "kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('learning', 0.4605),\n",
              " ('algorithm', 0.4556),\n",
              " ('training', 0.4488),\n",
              " ('class', 0.4087),\n",
              " ('mapping', 0.3701)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyG2V-EFdvF3"
      },
      "source": [
        "To extract keyphrases, simply set `keyphrase_ngram_range` to (1, 2) or higher depending on the number of words you would like in the resulting keyphrases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQVD6plodxoR",
        "outputId": "810dd535-36aa-4265-8b6a-4cf03ebe1bd9"
      },
      "source": [
        "kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('algorithm generalize training', 0.7727),\n",
              " ('learning algorithm analyzes', 0.7588),\n",
              " ('learning machine learning', 0.7577),\n",
              " ('learning algorithm generalize', 0.7515),\n",
              " ('algorithm analyzes training', 0.7504)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCOgtzTefG7N"
      },
      "source": [
        "Note that the stop_words are set by default to `\"english\"` so if you set this to None, then some of the stopwords will still be included in longer keyphrases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeS3kei7fPt-",
        "outputId": "596fc01c-fed4-49b6-88b7-9665ff323aad"
      },
      "source": [
        "kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 3), stop_words=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('learning algorithm analyzes', 0.7588),\n",
              " ('supervised learning algorithm', 0.7503),\n",
              " ('the learning algorithm', 0.7272),\n",
              " ('learning algorithm to', 0.7107),\n",
              " ('learning algorithm', 0.6979)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMLu1O7aeDBH"
      },
      "source": [
        "## Max Sum Similarity\n",
        "To diversity the results, we take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv25t8UyeilQ",
        "outputId": "b81795ea-12a4-4957-8f19-a8d7a1542c4f"
      },
      "source": [
        "kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 3),\n",
        "                          use_maxsum=True, nr_candidates=20, top_n=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('training examples supervised', 0.4556),\n",
              " ('machine learning', 0.7504),\n",
              " ('analyzes training data', 0.7727),\n",
              " ('requires learning algorithm', 0.5011),\n",
              " ('supervised learning algorithm', 0.279)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGikEABZeewO"
      },
      "source": [
        "## Maximal Marginal Relevance\n",
        "\n",
        "To diversify the results, we can use Maximal Margin Relevance (MMR) to create keywords / keyphrases which is also based on cosine similarity. The results with **high** diversity:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiHfsKYGeHJf",
        "outputId": "eb197dfc-6b88-43f9-9252-b1ce89c68e1d"
      },
      "source": [
        "kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 3),\n",
        "                          use_mmr=True, diversity=0.7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('algorithm generalize training', 0.7727),\n",
              " ('labels unseen', 0.089),\n",
              " ('mapping new', 0.3573),\n",
              " ('algorithm correctly', 0.3867),\n",
              " ('pairs infers function', 0.3827)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL-BhtBCeQ5x"
      },
      "source": [
        "The results with **low diversity**:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pn8m2p5eRBW",
        "outputId": "89f395fd-d961-46de-cf5c-4647657cfb41"
      },
      "source": [
        "kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 3),\n",
        "                              use_mmr=True, diversity=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('algorithm generalize training', 0.7727),\n",
              " ('supervised learning algorithm', 0.7503),\n",
              " ('learning machine learning', 0.7577),\n",
              " ('learning algorithm analyzes', 0.7588),\n",
              " ('learning algorithm generalize', 0.7515)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMLYu7HtZM9q"
      },
      "source": [
        "# **Embedding Models**\n",
        "In this section, we will go through all embedding models and backends that are supported in KeyBERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqQlRlqVZTMv"
      },
      "source": [
        "## Sentence Transformers\n",
        "You can select any model from sentence-transformers [here](https://www.sbert.net/docs/pretrained_models.html) and pass it through KeyBERT with `model`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKgz_m2ZZYLd",
        "outputId": "591174a4-634b-4b15-b3c9-e716c8f8e8c7"
      },
      "source": [
        "kw_model = KeyBERT(model=\"xlm-r-bert-base-nli-stsb-mean-tokens\")\n",
        "kw_model.extract_keywords(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('learning', 0.6026),\n",
              " ('training', 0.518),\n",
              " ('algorithm', 0.471),\n",
              " ('analyzes', 0.4646),\n",
              " ('supervised', 0.4624)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_b2G8IuaQg-"
      },
      "source": [
        "Or we can select a SentenceTransformer model with our own parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz4Nn_DUaUmE"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sentence_model = SentenceTransformer(\"xlm-r-bert-base-nli-stsb-mean-tokens\", device=\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyHnkkuGfdyd",
        "outputId": "fd10e788-5030-41b3-ab74-e3ddc3110e0c"
      },
      "source": [
        "kw_model = KeyBERT(model=sentence_model)\n",
        "kw_model.extract_keywords(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('learning', 0.6026),\n",
              " ('training', 0.518),\n",
              " ('algorithm', 0.471),\n",
              " ('analyzes', 0.4646),\n",
              " ('supervised', 0.4624)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SPQyllzabGM"
      },
      "source": [
        "## Flair\n",
        "Flair allows you to choose almost any embedding model that is publicly available.  \n",
        "Flair can be used as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf_xF-9Kae50"
      },
      "source": [
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "roberta = TransformerDocumentEmbeddings('roberta-base')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B02LWjHAfsJl",
        "outputId": "0cb9a8fc-8ac1-4fb6-c357-dedea4e421e3"
      },
      "source": [
        "kw_model = KeyBERT(model=roberta)\n",
        "kw_model.extract_keywords(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('algorithm', 0.9289),\n",
              " ('inferred', 0.9286),\n",
              " ('output', 0.9286),\n",
              " ('supervised', 0.9285),\n",
              " ('desired', 0.9284)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PjZdivWak4x"
      },
      "source": [
        "You can select any ðŸ¤— transformers model [here](https://huggingface.co/models).\n",
        "\n",
        "Moreover, you can also use Flair to use word embeddings and pool them to create document embeddings. Under the hood, Flair simply averages all word embeddings in a document. Then, we can easily pass it to KeyBERT in order to use those word embeddings as document embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4xsiz1daoj_"
      },
      "source": [
        "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n",
        "\n",
        "glove_embedding = WordEmbeddings('crawl')\n",
        "document_glove_embeddings = DocumentPoolEmbeddings([glove_embedding])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ2tctpDfv8H",
        "outputId": "b0817523-74dc-4e1a-880c-3d80e7a616f5"
      },
      "source": [
        "kw_model = KeyBERT(model=document_glove_embeddings)\n",
        "kw_model.extract_keywords(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('function', 0.4896),\n",
              " ('output', 0.4621),\n",
              " ('data', 0.4577),\n",
              " ('learning', 0.4538),\n",
              " ('input', 0.4524)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWycFAEnavoc"
      },
      "source": [
        "## Spacy\n",
        "Spacy has shown great promise over the last years and is now slowly transitioning into transformer-based techniques which makes it interesting to use in KeyBERT.\n",
        "\n",
        "We start by using a non-transformer-based model which we will have to download first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBmWZffha-jG"
      },
      "source": [
        "%%capture\n",
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJUnZbgGbATx"
      },
      "source": [
        "Next, simply load the model into a Spacy nlp instance and pass it through KeyBERT:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y583H7cdbE5S"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\", exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj_EMIYVfzZ9",
        "outputId": "e73b084c-a102-4525-eaea-5907f6cead46"
      },
      "source": [
        "kw_model = KeyBERT(model=nlp)\n",
        "kw_model.extract_keywords(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('example', 0.7436),\n",
              " ('way', 0.7313),\n",
              " ('determine', 0.6889),\n",
              " ('allow', 0.6621),\n",
              " ('used', 0.6432)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cJwnvCUbH83"
      },
      "source": [
        "We can also use their transformer-based models which we also have to download first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-Vh8PyHbK8L"
      },
      "source": [
        "%%capture\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q5xLMi7bMTK"
      },
      "source": [
        "As before, we simply load the model and pass it through KeyBERT. Note that we exclude a bunch of features as they are not used in KeyBERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqY1ykD0bVK7"
      },
      "source": [
        "import spacy\n",
        "from thinc.api import set_gpu_allocator, require_gpu\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_trf\", exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n",
        "set_gpu_allocator(\"pytorch\")\n",
        "require_gpu(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvCC8LIGf1Cd"
      },
      "source": [
        "kw_model = KeyBERT(model=nlp)\n",
        "kw_model.extract_keywords(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XM283A3bax8"
      },
      "source": [
        "## Universal Sentence Encoder (USE)\n",
        "The Universal Sentence Encoder encodes text into high dimensional vectors that are used here for embedding the documents. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC_ri-ribdfn"
      },
      "source": [
        "import tensorflow_hub\n",
        "embedding_model = tensorflow_hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GEOalvvb0Gg",
        "outputId": "0d46bcbc-2253-4ddf-ef94-49f2b650afc1"
      },
      "source": [
        "kw_model = KeyBERT(model=embedding_model)\n",
        "kw_model.extract_keywords(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('training', 0.2549),\n",
              " ('learning', 0.2264),\n",
              " ('algorithm', 0.2092),\n",
              " ('data', 0.1952),\n",
              " ('pairs', 0.1859)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_7IBBNcbhwa"
      },
      "source": [
        "## Gensim\n",
        "For Gensim, KeyBERT supports its `gensim.downloader` module. Here, we can download any model word embedding model to be used in KeyBERT. Note that Gensim is primarily used for Word Embedding models. This works typically best for short documents since the word embeddings are pooled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD4h-saAb4Ez"
      },
      "source": [
        "import gensim.downloader as api\n",
        "ft = api.load('fasttext-wiki-news-subwords-300')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cle4pMO5b5xc",
        "outputId": "7340f20b-7c14-462b-884c-c5540689dffb"
      },
      "source": [
        "kw_model = KeyBERT(model=ft)\n",
        "kw_model.extract_keywords(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('way', 0.698),\n",
              " ('new', 0.6955),\n",
              " ('based', 0.6899),\n",
              " ('set', 0.6824),\n",
              " ('object', 0.6574)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPI-dgukcAqX"
      },
      "source": [
        "## Custom Backend\n",
        "If your backend or model cannot be found in the ones currently available, you can use the BaseEmbedder class to create your own backend. Below, you will find an example of creating a SentenceTransformer backend for KeyBERT:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGjqG_m6cDF8"
      },
      "source": [
        "from keybert.backend import BaseEmbedder\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class CustomEmbedder(BaseEmbedder):\n",
        "    def __init__(self, embedding_model):\n",
        "        super().__init__()\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "    def embed(self, documents, verbose=False):\n",
        "        embeddings = self.embedding_model.encode(documents, show_progress_bar=verbose)\n",
        "        return embeddings\n",
        "\n",
        "# Create custom backend\n",
        "distilbert = SentenceTransformer(\"distilbert-base-nli-stsb-mean-tokens\")\n",
        "custom_embedder = CustomEmbedder(embedding_model=distilbert)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8XTB0i-cEmd",
        "outputId": "19c385a1-0b22-4b7f-854a-37db8c48233e"
      },
      "source": [
        "kw_model = KeyBERT(model=custom_embedder)\n",
        "kw_model.extract_keywords(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('learning', 0.5199),\n",
              " ('algorithm', 0.4292),\n",
              " ('supervised', 0.4265),\n",
              " ('training', 0.3835),\n",
              " ('class', 0.3147)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSeVVzcZaGeo"
      },
      "source": [
        "# **Candidates**\n",
        "In some cases, one might want to be using candidate keywords generated by other keyword algorithms or retrieved from a select list of possible keywords/keyphrases. In KeyBERT, you can easily use those candidate keywords to perform keyword extraction. We are going to create these candidates with [YAKE](https://github.com/LIAAD/yake), another great tool for extracting keywords.\n",
        "\n",
        "We start by installing yake:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knvOSqDeif_F"
      },
      "source": [
        "%%capture\n",
        "!pip install yake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svemPC3Ni0WW"
      },
      "source": [
        "Next, we will create 20 candidate keywords with YAKE:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJev2NH8GiM9"
      },
      "source": [
        "import yake\n",
        "\n",
        "kw_extractor = yake.KeywordExtractor(top=20)\n",
        "candidates = kw_extractor.extract_keywords(doc)\n",
        "candidates = [candidate[0] for candidate in candidates]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukq3eLsvi5Vc"
      },
      "source": [
        "Finally, we are going to pass these candidates to KeyBERT and use MMR to select the top 5 keywords/keyphrases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9KKHi_Cctb6",
        "outputId": "afb14dbf-7a1f-475d-d75e-1e24f7f5d1f0"
      },
      "source": [
        "kw_model = KeyBERT()\n",
        "kw_model.extract_keywords(doc, candidates, use_mmr=True, diversity=0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('supervised learning algorithm', 0.7503),\n",
              " ('training data consisting', 0.5419),\n",
              " ('machine learning', 0.6306),\n",
              " ('learning algorithm', 0.6979),\n",
              " ('input-output pairs.', 0.3598)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}