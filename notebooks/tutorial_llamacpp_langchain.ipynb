{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Tutorial on loading GGUF llama model within langchain in a CPU-only environment"]},{"cell_type":"markdown","metadata":{},"source":["This tutorial is based on the official llamacpp documentation [here](https://python.langchain.com/docs/integrations/llms/llamacpp) and the one in llama docker [here](https://github.com/penkow/llama-docker/blob/main/llama_cpu.py)"]},{"cell_type":"markdown","metadata":{},"source":["This tutorial runs successfully with **langchain==0.0.354** and **llama-cpp-python=0.2.27**"]},{"cell_type":"markdown","metadata":{},"source":["#### step0. prepare the environment with llama_cpp and langchain"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["from llama_cpp import Llama\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n","from langchain.chains import LLMChain\n","from langchain.prompts import PromptTemplate\n","from langchain_community.llms import LlamaCpp"]},{"cell_type":"markdown","metadata":{},"source":["#### step1. load the gguf model with llama_cpp"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./model/llama2-7b-chat-Q4KM/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n","llama_model_loader: - kv  10:                          general.file_type u32              = 15\n","llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q4_K:  193 tensors\n","llama_model_loader: - type q6_K:   33 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V2\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 4096\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 32\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 1\n","llm_load_print_meta: n_embd_k_gqa     = 4096\n","llm_load_print_meta: n_embd_v_gqa     = 4096\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 11008\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 4096\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: model type       = 7B\n","llm_load_print_meta: model ftype      = Q4_K - Medium\n","llm_load_print_meta: model params     = 6.74 B\n","llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n","llm_load_print_meta: general.name     = LLaMA v2\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size       =    0.11 MiB\n","llm_load_tensors: system memory used  = 3891.35 MiB\n","..................................................................................................\n","llama_new_context_with_model: n_ctx      = 512\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n","llama_build_graph: non-view tensors processed: 676/676\n","llama_new_context_with_model: compute buffer total size = 73.69 MiB\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"]}],"source":["model_path = \"./model/llama2-7b-chat-Q4KM/llama-2-7b-chat.Q4_K_M.gguf\"\n","\n","llama_gguf = Llama(model_path=model_path, verbose=True)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n"]},{"name":"stdout","output_type":"stream","text":["{'id': 'cmpl-85ea91f4-1183-46aa-b30a-55306523c9b9', 'object': 'text_completion', 'created': 1704456582, 'model': './model/llama2-7b-chat-Q4KM/llama-2-7b-chat.Q4_K_M.gguf', 'choices': [{'text': \"<s>[INST] <<SYS>>\\nYou are a helpful assistant\\n<</SYS>>\\nGenerate a list of 5 funny dog names [/INST]  Of course, I'd be happy to help you with that! Here are five funny dog names that might make you and your furry friend smile:\\n\\n1. Captain Fluffy Pants - This name is perfect for a dog that's as fluffy and adorable as it is adventurous and playful.\\n2. Sir Bark-a-Lot - For the dog that loves to bark at everything, this name is sure to bring a smile to your face.\\n3. Puddles McSplashy - This name is perfect for a dog that loves to splash around in puddles and get wet and muddy.\\n4. Barky Boo Boo - For the dog that's always up for a good cuddle, this name is sure to bring a smile to your face.\\n5. Sir Whiskerface von Fluffypants - This name is perfect for a dog with a fluffy beard and whiskers, and it's sure to make you and your friends laugh.\\nI hope these suggestions help inspire you to find the perfect funny dog name for your new furry friend!\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 37, 'completion_tokens': 256, 'total_tokens': 293}}\n"]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =     223.86 ms\n","llama_print_timings:      sample time =      46.48 ms /   257 runs   (    0.18 ms per token,  5528.67 tokens per second)\n","llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n","llama_print_timings:        eval time =    6831.30 ms /   257 runs   (   26.58 ms per token,    37.62 tokens per second)\n","llama_print_timings:       total time =    7370.01 ms\n"]}],"source":["# create the llama-style prompt\n","system_message = \"You are a helpful assistant\"\n","user_message = \"Generate a list of 5 funny dog names\"\n","\n","prompt = f\"\"\"<s>[INST] <<SYS>>\n","{system_message}\n","<</SYS>>\n","{user_message} [/INST]\"\"\"\n","\n","# Model parameters\n","max_tokens = 1000\n","\n","# Run the model\n","output = llama_gguf(prompt, max_tokens=max_tokens, echo=True)\n","print(output)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<s>[INST] <<SYS>>\n","You are a helpful assistant\n","<</SYS>>\n","Generate a list of 5 funny dog names [/INST]  Of course, I'd be happy to help you with that! Here are five funny dog names that might make you and your furry friend smile:\n","\n","1. Captain Fluffy Pants - This name is perfect for a dog that's as fluffy and adorable as it is adventurous and playful.\n","2. Sir Bark-a-Lot - For the dog that loves to bark at everything, this name is sure to bring a smile to your face.\n","3. Puddles McSplashy - This name is perfect for a dog that loves to splash around in puddles and get wet and muddy.\n","4. Barky Boo Boo - For the dog that's always up for a good cuddle, this name is sure to bring a smile to your face.\n","5. Sir Whiskerface von Fluffypants - This name is perfect for a dog with a fluffy beard and whiskers, and it's sure to make you and your friends laugh.\n","I hope these suggestions help inspire you to find the perfect funny dog name for your new furry friend!\n"]}],"source":["# the total generated output text with prompt\n","print(output['choices'][0]['text'])"]},{"cell_type":"markdown","metadata":{},"source":["#### step2. load the model with langchain built-in LlamaCpp"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Callbacks support token-wise streaming\n","callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./model/llama2-7b-chat-Q4KM/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n","llama_model_loader: - kv  10:                          general.file_type u32              = 15\n","llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q4_K:  193 tensors\n","llama_model_loader: - type q6_K:   33 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V2\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 4096\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 32\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 1\n","llm_load_print_meta: n_embd_k_gqa     = 4096\n","llm_load_print_meta: n_embd_v_gqa     = 4096\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 11008\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 4096\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: model type       = 7B\n","llm_load_print_meta: model ftype      = Q4_K - Medium\n","llm_load_print_meta: model params     = 6.74 B\n","llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n","llm_load_print_meta: general.name     = LLaMA v2\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size       =    0.11 MiB\n","llm_load_tensors: system memory used  = 3891.35 MiB\n","..................................................................................................\n","llama_new_context_with_model: n_ctx      = 512\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n","llama_build_graph: non-view tensors processed: 676/676\n","llama_new_context_with_model: compute buffer total size = 4.29 MiB\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"]}],"source":["llm_cpp = LlamaCpp(\n","    model_path=model_path,\n","    temperature=0.75,\n","    max_tokens=2000,\n","    top_p=1,\n","    callback_manager=callback_manager,\n","    verbose=True,  # Verbose is required to pass to the callback manager\n",")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["LlamaCpp(callbacks=<langchain_core.callbacks.manager.CallbackManager object at 0x7f2094564be0>, client=<llama_cpp.llama.Llama object at 0x7f2094566530>, model_path='./model/llama2-7b-chat-Q4KM/llama-2-7b-chat.Q4_K_M.gguf', max_tokens=2000, temperature=0.75, top_p=1.0)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["llm_cpp"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# build a prompt template\n","template = \"\"\"Question: {question}\n","\n","Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n","\n","prompt = PromptTemplate(template=template, input_variables=[\"question\"])"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# build a chain with the gguf model and the prompt\n","llm_chain = LLMChain(prompt=prompt, llm=llm_cpp)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Justin Bieber was born on March 1, 1994. The NBA (National Basketball Association) season typically runs from October to June of the following year. So if we look at the NBA champions for each year from 1994 to present, we can find out which team won the championship in the year Justin Bieber was born.\n","Here are the NBA champions for each year since 1994:\n","* 1994: Chicago Bulls\n","So, the NBA team that won the championship in the year Justin Bieber was born (1994) is the Chicago Bulls!"]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =      74.52 ms\n","llama_print_timings:      sample time =      23.16 ms /   136 runs   (    0.17 ms per token,  5871.69 tokens per second)\n","llama_print_timings: prompt eval time =     418.31 ms /    44 tokens (    9.51 ms per token,   105.19 tokens per second)\n","llama_print_timings:        eval time =    4210.45 ms /   135 runs   (   31.19 ms per token,    32.06 tokens per second)\n","llama_print_timings:       total time =    5001.16 ms\n"]},{"data":{"text/plain":["'\\nJustin Bieber was born on March 1, 1994. The NBA (National Basketball Association) season typically runs from October to June of the following year. So if we look at the NBA champions for each year from 1994 to present, we can find out which team won the championship in the year Justin Bieber was born.\\nHere are the NBA champions for each year since 1994:\\n* 1994: Chicago Bulls\\nSo, the NBA team that won the championship in the year Justin Bieber was born (1994) is the Chicago Bulls!'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# run the chain with the prompt filled with the user question\n","question = \"What NBA team won the champion in the year Justin Bieber was born?\"\n","llm_chain.run(question)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" Here are the NBA teams"]},{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n"]},{"name":"stdout","output_type":"stream","text":[" and their championships since 1970 when Justin Bieber was born:\n","\n","* 1970 - Seattle Super Sonics (won championship)\n","* 1980 - Los Angeles Lakers (won championship)\n","* 1990 - Detroit Pistons (won championship)\n","* 2000 - Los Angeles Lakers (won championship)\n","* 2010 - Dallas Mavericks (won championship)\n","\n","So, since Justin Bieber was born in 1994, the NBA team that won the championship in the year he was born is:\n","Detroit Pistons."]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =      74.52 ms\n","llama_print_timings:      sample time =      24.44 ms /   145 runs   (    0.17 ms per token,  5933.14 tokens per second)\n","llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n","llama_print_timings:        eval time =    4467.86 ms /   145 runs   (   30.81 ms per token,    32.45 tokens per second)\n","llama_print_timings:       total time =    4843.94 ms\n"]},{"data":{"text/plain":["' Here are the NBA teams and their championships since 1970 when Justin Bieber was born:\\n\\n* 1970 - Seattle Super Sonics (won championship)\\n* 1980 - Los Angeles Lakers (won championship)\\n* 1990 - Detroit Pistons (won championship)\\n* 2000 - Los Angeles Lakers (won championship)\\n* 2010 - Dallas Mavericks (won championship)\\n\\nSo, since Justin Bieber was born in 1994, the NBA team that won the championship in the year he was born is:\\nDetroit Pistons.'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# or just run the gguf model directly\n","llm_cpp(prompt.format(question=question))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"llama","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
