{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "computer_issu_or_question_list = ''' \\\n",
    "a computer software, \\\n",
    "Windows, \\\n",
    "Printer, \\\n",
    "Mouse, \\\n",
    "Micrphone, \\\n",
    "Heaset, \\\n",
    "Sound, \\\n",
    "Battery issues, \\\n",
    "PC heating up, \\\n",
    "Game not working, \\\n",
    "Game makes computer slow, \\\n",
    "Display and Appearance issues, \\\n",
    "Sound and Audio issues, \\\n",
    "Network and Internet issues, \\\n",
    "Power and Battery, \\\n",
    "Privacy and Security, \\\n",
    "Accessibility, \\\n",
    "Keyboard and Mouse issues, \\\n",
    "System Updates and Maintenance issues, \\\n",
    "File and Folder, \\\n",
    "Applications and Software issues, \\\n",
    "Slow performance ,\\\n",
    "Computer crashes or freezes ,\\\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"A model that takes in a description of a computer related issue and how to use {computer_issue_list}, and other general computer issues doesn't show in here \\\n",
    "or feature in English, and responds with the troubleshooting guide and guide or tutorial in English.\"\n",
    "\n",
    "prompt = prompt_template.format(computer_issue_list=computer_issu_or_question_list)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = .4\n",
    "# temperature = .2\n",
    "\n",
    "number_of_examples = 50\n",
    "# number_of_examples = 10\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import random\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example(prompt, prev_examples, temperature=.5):\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are generating data which will be used to train \\\n",
    "a machine learning model.\\n\\nYou will be given a high-level description of the model \\\n",
    "we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\\n",
    "\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\\n",
    "\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\\n",
    "\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\\n",
    "\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\\n",
    "\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    if len(prev_examples) > 0:\n",
    "        if len(prev_examples) > 10:\n",
    "            prev_examples = random.sample(prev_examples, 10)\n",
    "        for example in prev_examples:\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": example\n",
    "            })\n",
    "\n",
    "    '''\n",
    "    ** gpt-3.5-turbo-16k-0613\n",
    "    Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, \n",
    "    this model will not receive updates, and will be deprecated 3 months after a new version is released.\n",
    "    max token: 16,385 tokens\n",
    "    '''\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k-0613\", # gpt-4\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=4096, # 1354 , 500, max : 4096\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message['content']\n",
    "\n",
    "# Generate examples\n",
    "prev_examples = []\n",
    "for i in range(number_of_examples):\n",
    "    print(f'Generating example {i}')\n",
    "    example = generate_example(prompt, prev_examples, temperature)\n",
    "    prev_examples.append(example)\n",
    "    # print index of example and example\n",
    "    print(f'Example {i} generated: {example}')\n",
    "\n",
    "print(prev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_system_message(prompt):\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k-0613\", # gpt-4\n",
    "        messages=[\n",
    "          {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will be given a high-level description of the model we are training, \\\n",
    "and from that, you will generate a simple system prompt for that model to use. \\\n",
    "Remember, you are not generating the system message for data generation -- \\\n",
    "you are generating the system message to use for inference. \\\n",
    "A good format to follow is `Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO.`.\\\n",
    "\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\\n",
    "\\n\\nFor example, never write: `\\\"$SYSTEM_PROMPT_HERE\\\"`.\\n\\nIt should be like: `$SYSTEM_PROMPT_HERE`.\"\n",
    "          },\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt.strip(),\n",
    "          }\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message['content']\n",
    "\n",
    "system_message = generate_system_message(prompt)\n",
    "\n",
    "print(f'The system message is: `{system_message}`. \\nFeel free to re-run this cell if you want a better result.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store prompts and responses\n",
    "prompts = []\n",
    "responses = []\n",
    "\n",
    "# Parse out prompts and responses from examples\n",
    "for example in prev_examples:\n",
    "  try:\n",
    "    split_example = example.split('-----------')\n",
    "    prompts.append(split_example[1].strip())\n",
    "    responses.append(split_example[3].strip())\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'prompt': prompts,\n",
    "    'response': responses\n",
    "})\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print('There are ' + str(len(df)) + ' successfully-generated examples. Here are the first few:')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "df = df.rename(columns={\n",
    "    'prompt': 'Question',\n",
    "    'response': 'Solution'\n",
    "})\n",
    "\n",
    "# Save DataFrame to CSV without headers first\n",
    "file_path = 'data/gpt-gen-data-llm.csv'\n",
    "df.to_csv(file_path, header=False, index=False)\n",
    "\n",
    "df = df.rename(columns={\n",
    "    'prompt': 'Question',\n",
    "    'response': 'Solution'\n",
    "})\n",
    "\n",
    "# save df to csv with headers. need to add First column is 'prompt', second column is 'response'\n",
    "df.to_csv('data/gpt-gen-data-llm.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets, with 90% in the train set\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "# Save the dataframes to .jsonl files\n",
    "train_df.to_json('data/train.jsonl', orient='records', lines=True)\n",
    "test_df.to_json('data/test.jsonl', orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
