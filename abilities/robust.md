# Robustness Abilities of LLMs
*Here're some resources about Robustness Abilities of LLMs*


### Adversarial Attack


#### Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation [`READ`]

paper link: [here](https://arxiv.org/pdf/2311.03348.pdf)

citation:
```bibtex
@misc{shah2023scalable,
      title={Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation}, 
      author={Rusheb Shah and Quentin Feuillade--Montixi and Soroush Pour and Arush Tagade and Stephen Casper and Javier Rando},
      year={2023},
      eprint={2311.03348},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game [`READ`]

paper link: [here](https://arxiv.org/pdf/2311.01011.pdf)

citation:
```bibtex
@misc{toyer2023tensor,
      title={Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game}, 
      author={Sam Toyer and Olivia Watkins and Ethan Adrian Mendes and Justin Svegliato and Luke Bailey and Tiffany Wang and Isaac Ong and Karim Elmaaroufi and Pieter Abbeel and Trevor Darrell and Alan Ritter and Stuart Russell},
      year={2023},
      eprint={2311.01011},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks [`READ`]

paper link: [here](https://arxiv.org/pdf/2310.16955.pdf)

citation:
```bibtex
@misc{sinha2024break,
      title={Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks}, 
      author={Aradhana Sinha and Ananth Balashankar and Ahmad Beirami and Thi Avrahami and Jilin Chen and Alex Beutel},
      year={2024},
      eprint={2310.16955},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models [`READ`]

paper link: [here](https://arxiv.org/pdf/2310.15140.pdf)

citation:
```bibtex
@misc{zhu2023autodan,
      title={AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models}, 
      author={Sicheng Zhu and Ruiyi Zhang and Bang An and Gang Wu and Joe Barrow and Zichao Wang and Furong Huang and Ani Nenkova and Tong Sun},
      year={2023},
      eprint={2310.15140},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}
```


#### Jailbreaking Black Box Large Language Models in Twenty Queries [`READ`]

paper link: [here](https://arxiv.org/pdf/2310.08419.pdf)

citation:
```bibtex
@misc{chao2023jailbreaking,
      title={Jailbreaking Black Box Large Language Models in Twenty Queries}, 
      author={Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong},
      year={2023},
      eprint={2310.08419},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### Low-Resource Languages Jailbreak GPT-4 [`READ`]

paper link: [here](https://arxiv.org/pdf/2310.02446.pdf)

citation:
```bibtex
@misc{yong2024lowresource,
      title={Low-Resource Languages Jailbreak GPT-4}, 
      author={Zheng-Xin Yong and Cristina Menghini and Stephen H. Bach},
      year={2024},
      eprint={2310.02446},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### Universal and Transferable Adversarial Attacks on Aligned Language Models [`READ`]

paper link: [here](https://arxiv.org/pdf/2307.15043.pdf)

citation:
```bibtex
@misc{zou2023universal,
      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, 
      author={Andy Zou and Zifan Wang and Nicholas Carlini and Milad Nasr and J. Zico Kolter and Matt Fredrikson},
      year={2023},
      eprint={2307.15043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```



### Adversarial Defence


#### Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks [`READ`]

paper link: [here](https://arxiv.org/pdf/2401.17263.pdf)

citation:
```bibtex
@misc{zhou2024robust,
      title={Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks}, 
      author={Andy Zhou and Bo Li and Haohan Wang},
      year={2024},
      eprint={2401.17263},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### Weakly Supervised Detection of Hallucinations in LLM Activations [`READ`]

paper link: [here](https://arxiv.org/pdf/2312.02798.pdf)

citation:
```bibtex
@misc{rateike2023weakly,
      title={Weakly Supervised Detection of Hallucinations in LLM Activations}, 
      author={Miriam Rateike and Celia Cintas and John Wamburu and Tanya Akumu and Skyler Speakman},
      year={2023},
      eprint={2312.02798},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization [`READ`]

paper link: [here](https://arxiv.org/pdf/2311.09096.pdf)

citation:
```bibtex
@misc{zhang2023defending,
      title={Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization}, 
      author={Zhexin Zhang and Junxiao Yang and Pei Ke and Minlie Huang},
      year={2023},
      eprint={2311.09096},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks [`READ`]

paper link: [here](https://arxiv.org/pdf/2310.03684.pdf)

citation:
```bibtex
@misc{robey2023smoothllm,
      title={SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks}, 
      author={Alexander Robey and Eric Wong and Hamed Hassani and George J. Pappas},
      year={2023},
      eprint={2310.03684},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### Can LLM-Generated Misinformation Be Detected? [`READ`]

paper link: [here](https://arxiv.org/pdf/2309.13788.pdf)

citation:
```bibtex
@misc{chen2023llmgenerated,
      title={Can LLM-Generated Misinformation Be Detected?}, 
      author={Canyu Chen and Kai Shu},
      year={2023},
      eprint={2309.13788},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### Baseline Defenses for Adversarial Attacks Against Aligned Language Models [`READ`]

paper link: [here](https://arxiv.org/pdf/2309.00614.pdf)

citation:
```bibtex
@misc{jain2023baseline,
      title={Baseline Defenses for Adversarial Attacks Against Aligned Language Models}, 
      author={Neel Jain and Avi Schwarzschild and Yuxin Wen and Gowthami Somepalli and John Kirchenbauer and Ping-yeh Chiang and Micah Goldblum and Aniruddha Saha and Jonas Geiping and Tom Goldstein},
      year={2023},
      eprint={2309.00614},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### Detecting language model attacks with perplexity [`READ`]

paper link: [here](https://arxiv.org/pdf/2308.14132.pdf)

citation:
```bibtex
@misc{alon2023detecting,
      title={Detecting Language Model Attacks with Perplexity}, 
      author={Gabriel Alon and Michael Kamfonas},
      year={2023},
      eprint={2308.14132},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks [`READ`]

paper link: [here](https://arxiv.org/pdf/2305.14965.pdf)

citation:
```bibtex
@misc{rao2024tricking,
      title={Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks}, 
      author={Abhinav Rao and Sachin Vashistha and Atharva Naik and Somak Aditya and Monojit Choudhury},
      year={2024},
      eprint={2305.14965},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```



### Robustness Evaluation


#### Quantifying Uncertainty in Natural Language Explanations of Large Language Models [`READ`]

paper link: [here](https://arxiv.org/pdf/2311.03533.pdf)

citation:
```bibtex
@misc{tanneru2023quantifying,
      title={Quantifying Uncertainty in Natural Language Explanations of Large Language Models}, 
      author={Sree Harsha Tanneru and Chirag Agarwal and Himabindu Lakkaraju},
      year={2023},
      eprint={2311.03533},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

#### Estimating Uncertainty in Multimodal Foundation Models using Public Internet Data [`READ`]

paper link: [here](https://arxiv.org/pdf/2310.09926.pdf)

citation:
```bibtex
@misc{dutta2023estimating,
      title={Estimating Uncertainty in Multimodal Foundation Models using Public Internet Data}, 
      author={Shiladitya Dutta and Hongbo Wei and Lars van der Laan and Ahmed M. Alaa},
      year={2023},
      eprint={2310.09926},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
```


#### Risk Assessment and Statistical Significance in the Age of Foundation Models [`READ`]

paper link: [here](https://arxiv.org/pdf/2310.07132.pdf)

citation:
```bibtex
@misc{nitsure2024risk,
      title={Risk Assessment and Statistical Significance in the Age of Foundation Models}, 
      author={Apoorva Nitsure and Youssef Mroueh and Mattia Rigotti and Kristjan Greenewald and Brian Belgodere and Mikhail Yurochkin and Jiri Navratil and Igor Melnyk and Jerret Ross},
      year={2024},
      eprint={2310.07132},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Evaluating Superhuman Models with Consistency Checks [`READ`]

paper link: [here](https://arxiv.org/pdf/2306.09983.pdf)

citation:
```bibtex
@misc{fluri2023evaluating,
      title={Evaluating Superhuman Models with Consistency Checks}, 
      author={Lukas Fluri and Daniel Paleka and Florian Tram√®r},
      year={2023},
      eprint={2306.09983},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```



### Emiprical Study


#### Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models [`READ`]

paper link: [here](https://arxiv.org/pdf/2402.17671.pdf)

citation:
```bibtex
@misc{huang2024securing,
      title={Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models}, 
      author={Yunpeng Huang and Yaonan Gu and Jingwei Xu and Zhihong Zhu and Zhaorun Chen and Xiaoxing Ma},
      year={2024},
      eprint={2402.17671},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Are Large Language Models Really Robust to Word-Level Perturbations? [`READ`]

paper link: [here](https://arxiv.org/pdf/2309.11166.pdf)

citation:
```bibtex
@misc{wang2023large,
      title={Are Large Language Models Really Robust to Word-Level Perturbations?}, 
      author={Haoyu Wang and Guozheng Ma and Cong Yu and Ning Gui and Linrui Zhang and Zhiqi Huang and Suwei Ma and Yongzhe Chang and Sen Zhang and Li Shen and Xueqian Wang and Peilin Zhao and Dacheng Tao},
      year={2023},
      eprint={2309.11166},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### Jailbroken: How does llm safety training fail? [`READ`]

paper link: [here](https://arxiv.org/pdf/2307.02483.pdf)

citation:
```bibtex
@misc{wei2023jailbroken,
      title={Jailbroken: How Does LLM Safety Training Fail?}, 
      author={Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
      year={2023},
      eprint={2307.02483},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study [`READ`]

paper link: [here](https://arxiv.org/pdf/2305.13860.pdf)

citation:
```bibtex
@misc{liu2023jailbreaking,
      title={Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study}, 
      author={Yi Liu and Gelei Deng and Zhengzi Xu and Yuekang Li and Yaowen Zheng and Ying Zhang and Lida Zhao and Tianwei Zhang and Yang Liu},
      year={2023},
      eprint={2305.13860},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
```


#### On Evaluating Adversarial Robustness of Large Vision-Language Models [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2023/file/a97b58c4f7551053b0512f92244b0810-Paper-Conference.pdf)

citation:
```bibtex
@article{zhao2024evaluating,
  title={On evaluating adversarial robustness of large vision-language models},
  author={Zhao, Yunqing and Pang, Tianyu and Du, Chao and Yang, Xiao and Li, Chongxuan and Cheung, Ngai-Man Man and Lin, Min},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
```




