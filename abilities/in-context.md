# In-context Learning on LLMs
*Here's some resources about In-context / Prompt Learning on LLMs*


### Method

#### Plum: Prompt Learning using Metaheuristic [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2311.08364)

citation: 
```bibtex
@article{pan2023plum,
  title={Plum: Prompt Learning using Metaheuristic},
  author={Pan, Rui and Xing, Shuo and Diao, Shizhe and Liu, Xiang and Shum, Kashun and Zhang, Jipeng and Zhang, Tong},
  journal={arXiv preprint arXiv:2311.08364},
  year={2023}
}
```


#### Graph of thoughts: Solving elaborate problems with large language models (GoT) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2308.09687.pdf)

citation: 
```bibtex
@article{besta2023graph,
  title={Graph of thoughts: Solving elaborate problems with large language models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Michal and Niewiadomski, Hubert and Nyczyk, Piotr and others},
  journal={arXiv preprint arXiv:2308.09687},
  year={2023}
}
```


#### Tree of thoughts: Deliberate problem solving with large language models (ToT) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2305.10601)

citation: 
```bibtex
@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2305.10601},
  year={2023}
}
```
    
    
    

#### Progprompt: Generating situated robot task plans using large language models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2209.11302)

citation: 
```bibtex
@inproceedings{singh2023progprompt,
  title={Progprompt: Generating situated robot task plans using large language models},
  author={Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={11523--11530},
  year={2023},
  organization={IEEE}
}
```
    


#### Maple: Multi-modal prompt learning [`UNREAD`]

paper link: [here](https://openaccess.thecvf.com/content/CVPR2023/papers/Khattak_MaPLe_Multi-Modal_Prompt_Learning_CVPR_2023_paper.pdf)

citation: 
```bibtex
@inproceedings{khattak2023maple,
  title={Maple: Multi-modal prompt learning},
  author={Khattak, Muhammad Uzair and Rasheed, Hanoona and Maaz, Muhammad and Khan, Salman and Khan, Fahad Shahbaz},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19113--19122},
  year={2023}
}
```

#### Structured prompting: Scaling in-context learning to 1,000 examples [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2212.06713)

citation: 
```bibtex
@article{hao2022structured,
  title={Structured prompting: Scaling in-context learning to 1,000 examples},
  author={Hao, Yaru and Sun, Yutao and Dong, Li and Han, Zhixiong and Gu, Yuxian and Wei, Furu},
  journal={arXiv preprint arXiv:2212.06713},
  year={2022}
}
```
    
#### Large language models can self-improve [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2210.11610.pdf)

citation: 
```bibtex
@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}
```


#### Large language models are human-level prompt engineers [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2211.01910.pdf?trk=public_post_comment-text)

citation: 
```bibtex
@article{zhou2022large,
  title={Large language models are human-level prompt engineers},
  author={Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  journal={arXiv preprint arXiv:2211.01910},
  year={2022}
}
```

#### Large language models can self-improve [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2210.11610.pdf)

citation: 
```bibtex
@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}
```
    


#### Conditional prompt learning for vision-language models [`UNREAD`]

paper link: [here](http://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf)

citation: 
```bibtex
@inproceedings{zhou2022conditional,
  title={Conditional prompt learning for vision-language models},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16816--16825},
  year={2022}
}
```
    


#### Chain-of-thought prompting elicits reasoning in large language models (CoT) [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf)

citation: 
```bibtex
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
```

#### Self-consistency improves chain of thought reasoning in language models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2203.11171.pdf?utm_campaign=Neural%20Newsletter%26utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_bjTPob0bX6S_mnCLCnCAmqpWItQ7B7OQaIGTCC1yZATezVlHdH2i8Yt9tLWSTopo7Qn)

citation: 
```bibtex
@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}
```
    


#### Calibrate before use: Improving few-shot performance of language models [`UNREAD`]

paper link: [here](http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf)

citation: 
```bibtex
@inproceedings{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={International Conference on Machine Learning},
  pages={12697--12706},
  year={2021},
  organization={PMLR}
}
```
    


#### Multitask prompted training enables zero-shot task generalization [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2110.08207)

citation: 
```bibtex
@article{sanh2021multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={arXiv preprint arXiv:2110.08207},
  year={2021}
}
```
    

### Survey


#### A survey for in-context learning [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2301.00234)

citation: 
```bibtex
@article{dong2022survey,
  title={A survey for in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}
```
    

#### Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing [`READ`]

paper link: [here](https://dl.acm.org/doi/pdf/10.1145/3560815?trk=public_post_comment-text)

citation: 
```bibtex
@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}
```
    
    