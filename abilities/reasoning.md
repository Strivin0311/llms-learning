# Reasoning Abilities of LLMs
*Here's some resources about Reasoning Abilities of LLMs*


#### ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2309.17452)

citation: 
```bibtex
@article{gou2023tora,
  title={ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving},
  author={Gou, Zhibin and Shao, Zhihong and Gong, Yeyun and Yang, Yujiu and Huang, Minlie and Duan, Nan and Chen, Weizhu and others},
  journal={arXiv preprint arXiv:2309.17452},
  year={2023}
}
```
    


#### Visual programming: Compositional visual reasoning without training [`READ`]

paper link: [here](https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf)

citation: 
```bibtex
@inproceedings{gupta2023visual,
  title={Visual programming: Compositional visual reasoning without training},
  author={Gupta, Tanmay and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14953--14962},
  year={2023}
}
```
    

#### Progprompt: Generating situated robot task plans using large language models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2209.11302)

citation: 
```bibtex
@inproceedings{singh2023progprompt,
  title={Progprompt: Generating situated robot task plans using large language models},
  author={Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={11523--11530},
  year={2023},
  organization={IEEE}
}
```

#### Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2210.11694)

citation: 
```bibtex
@article{zhang2022multi,
  title={Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem},
  author={Zhang, Wenqi and Shen, Yongliang and Ma, Yanna and Cheng, Xiaoxia and Tan, Zeqi and Nong, Qingpeng and Lu, Weiming},
  journal={arXiv preprint arXiv:2210.11694},
  year={2022}
}
```
    

#### Language models as zero-shot planners: Extracting actionable knowledge for embodied agents [`UNREAD`]

paper link: [here](https://proceedings.mlr.press/v162/huang22a/huang22a.pdf)

citation: 
```bibtex
@inproceedings{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={International Conference on Machine Learning},
  pages={9118--9147},
  year={2022},
  organization={PMLR}
}
```
    


#### Large language models can self-improve [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2210.11610.pdf)

citation: 
```bibtex
@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}
```



#### Language models show human-like content effects on reasoning [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2207.07051.pdf&nbsp;%3C/p%3E)

citation: 
```bibtex
@article{dasgupta2022language,
  title={Language models show human-like content effects on reasoning},
  author={Dasgupta, Ishita and Lampinen, Andrew K and Chan, Stephanie CY and Creswell, Antonia and Kumaran, Dharshan and McClelland, James L and Hill, Felix},
  journal={arXiv preprint arXiv:2207.07051},
  year={2022}
}
```