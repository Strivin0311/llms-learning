# Reasoning Abilities of LLMs
*Here's some resources about Reasoning Abilities of LLMs*


#### ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2309.17452)

citation: 
```bibtex
@article{gou2023tora,
  title={ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving},
  author={Gou, Zhibin and Shao, Zhihong and Gong, Yeyun and Yang, Yujiu and Huang, Minlie and Duan, Nan and Chen, Weizhu and others},
  journal={arXiv preprint arXiv:2309.17452},
  year={2023}
}
```

#### Why think step-by-step? Reasoning emerges from the locality of experience [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2304.03843)

citation: 
```bibtex
@article{prystawski2023think,
  title={Why think step-by-step? Reasoning emerges from the locality of experience},
  author={Prystawski, Ben and Goodman, Noah D},
  journal={arXiv preprint arXiv:2304.03843},
  year={2023}
}
```



#### Pal: Program-aided language models [`UNREAD`]

paper link: [here](https://proceedings.mlr.press/v202/gao23f/gao23f.pdf)

citation: 
```bibtex
@inproceedings{gao2023pal,
  title={Pal: Program-aided language models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={International Conference on Machine Learning},
  pages={10764--10799},
  year={2023},
  organization={PMLR}
}
```
    


#### Visual programming: Compositional visual reasoning without training [`READ`]

paper link: [here](https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf)

citation: 
```bibtex
@inproceedings{gupta2023visual,
  title={Visual programming: Compositional visual reasoning without training},
  author={Gupta, Tanmay and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14953--14962},
  year={2023}
}
```
    

#### Progprompt: Generating situated robot task plans using large language models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2209.11302)

citation: 
```bibtex
@inproceedings{singh2023progprompt,
  title={Progprompt: Generating situated robot task plans using large language models},
  author={Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={11523--11530},
  year={2023},
  organization={IEEE}
}
```

#### Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2210.11694)

citation: 
```bibtex
@article{zhang2022multi,
  title={Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem},
  author={Zhang, Wenqi and Shen, Yongliang and Ma, Yanna and Cheng, Xiaoxia and Tan, Zeqi and Nong, Qingpeng and Lu, Weiming},
  journal={arXiv preprint arXiv:2210.11694},
  year={2022}
}
```
    

#### Language models as zero-shot planners: Extracting actionable knowledge for embodied agents [`UNREAD`]

paper link: [here](https://proceedings.mlr.press/v162/huang22a/huang22a.pdf)

citation: 
```bibtex
@inproceedings{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={International Conference on Machine Learning},
  pages={9118--9147},
  year={2022},
  organization={PMLR}
}
```
    


#### Large language models can self-improve [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2210.11610.pdf)

citation: 
```bibtex
@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}
```

#### React: Synergizing reasoning and acting in language models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2210.03629.pdf)

citation: 
```bibtex
@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}
```



#### Language models show human-like content effects on reasoning [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2207.07051.pdf)

citation: 
```bibtex
@article{dasgupta2022language,
  title={Language models show human-like content effects on reasoning},
  author={Dasgupta, Ishita and Lampinen, Andrew K and Chan, Stephanie CY and Creswell, Antonia and Kumaran, Dharshan and McClelland, James L and Hill, Felix},
  journal={arXiv preprint arXiv:2207.07051},
  year={2022}
}
```


#### Self-consistency improves chain of thought reasoning in language models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2203.11171.pdf)

citation: 
```bibtex
@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}
```
    