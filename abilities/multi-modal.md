# Multi-Modal Representation Abilities of LLMs
*Here're some resources about improving Multi-Modal Representation Abilities of LLMs*

### Audio

#### Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2311.07919.pdf)

|model name|link|
|-|-|
|Qwen-Audio-Chat|[here](https://huggingface.co/Qwen/Qwen-Audio-Chat)|
|Qwen-Audio|[here](https://huggingface.co/Qwen/Qwen-Audio)|

citation:
```bibtex
@misc{chu2023qwenaudio,
      title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models}, 
      author={Yunfei Chu and Jin Xu and Xiaohuan Zhou and Qian Yang and Shiliang Zhang and Zhijie Yan and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2311.07919},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}
```


### Video

#### CAST: Cross-Attention in Space and Time for Video Action Recognition [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2311.18825.pdf)

citation: 
```bibtex
@misc{lee2023cast,
      title={CAST: Cross-Attention in Space and Time for Video Action Recognition}, 
      author={Dongho Lee and Jongseo Lee and Jinwoo Choi},
      year={2023},
      eprint={2311.18825},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

#### Free-bloom: Zero-shot text-to-video generator with llm director and ldm animator [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2309.14494)

citation: 
```bibtex
@article{huang2023free,
  title={Free-bloom: Zero-shot text-to-video generator with llm director and ldm animator},
  author={Huang, Hanzhuo and Feng, Yufan and Shi, Cheng and Xu, Lan and Yu, Jingyi and Yang, Sibei},
  journal={arXiv preprint arXiv:2309.14494},
  year={2023}
}
```
    


### Image


#### CogAgent: A Visual Language Model for GUI Agents [`UNREAD`]

paper link: [here](https://arxiv.org/abs/2312.08914)

github link: [here](https://github.com/THUDM/CogVLM)

citation: 
```bibtex
@misc{hong2023cogagent,
      title={CogAgent: A Visual Language Model for GUI Agents}, 
      author={Wenyi Hong and Weihan Wang and Qingsong Lv and Jiazheng Xu and Wenmeng Yu and Junhui Ji and Yan Wang and Zihan Wang and Yuxiao Dong and Ming Ding and Jie Tang},
      year={2023},
      eprint={2312.08914},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

#### CogVLM: Visual Expert for Pretrained Language Models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2311.03079.pdf)

github link: [here](https://github.com/THUDM/CogVLM)

model links: 

|model name|link|
|-|-|
|CogVLM|[here](https://huggingface.co/THUDM/CogVLM)|

citation: 
```bibtex
@misc{wang2023cogvlm,
      title={CogVLM: Visual Expert for Pretrained Language Models}, 
      author={Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},
      year={2023},
      eprint={2311.03079},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

#### Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2308.12966.pdf)

github link: [here](https://github.com/QwenLM/Qwen-VL)

model links: 

|model name|link|
|-|-|
|Qwen-VL-Chat-Int4|[here](https://huggingface.co/Qwen/Qwen-VL-Chat-Int4)|
|Qwen-VL-Chat|[here](https://huggingface.co/Qwen/Qwen-VL-Chat)|
|Qwen-VL|[here](https://huggingface.co/Qwen/Qwen-VL)|


citation: 
```bibtex
@misc{bai2023qwenvl,
      title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond}, 
      author={Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2308.12966},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### Improved Baselines with Visual Instruction Tuning (LLaVA-v1.5) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2310.03744.pdf)

blog link: [here](https://llava-vl.github.io/)

github links: 

|repo name|link|
|-|-|
|BakLLaVA|[here](https://github.com/SkunkworksAI/BakLLaVA)|
|LLaVA|[here](https://github.com/haotian-liu/LLaVA)|

model links: 

|model name|link|
|-|-|
|BakLLaVA-1|[here](https://huggingface.co/SkunkworksAI/BakLLaVA-1)|
|LLaVA-v1.5-13b|[here](https://huggingface.co/liuhaotian/llava-v1.5-13b)|
|LLaVA-v1.5-7b|[here](https://huggingface.co/liuhaotian/llava-v1.5-7b)|

citation: 
```bibtex
@misc{liu2023improvedllava,
      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
      title={Improved Baselines with Visual Instruction Tuning}, 
      publisher={arXiv:2310.03744},
      year={2023},
}
```


#### Visual instruction tuning (LLaVA) [`READ`]

paper link: [here](https://arxiv.org/pdf/2304.08485)

blog link: [here](https://llava-vl.github.io/)

github link: [here](https://github.com/haotian-liu/LLaVA)


citation: 
```bibtex
@inproceedings{liu2023llava,
    author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
    title = {Visual Instruction Tuning},
    booktitle = {NeurIPS},
    year = {2023}
}
```
    

#### Visual programming: Compositional visual reasoning without training [`READ`]

paper link: [here](https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf)

citation: 
```bibtex
@inproceedings{gupta2023visual,
  title={Visual programming: Compositional visual reasoning without training},
  author={Gupta, Tanmay and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14953--14962},
  year={2023}
}
```


#### Learning transferable visual models from natural language supervision (CLIP) [`UNREAD`]

paper link: [here](http://proceedings.mlr.press/v139/radford21a/radford21a.pdf)

citation: 
```bibtex
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
```