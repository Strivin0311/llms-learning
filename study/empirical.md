# Empirical Study on LLMs
*Here're some resources about Empirical Research on LLMs, especially surveys*


## Surveys

#### Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems

paper link: [here](https://arxiv.org/pdf/2312.15234.pdf)

citation:

```bibtex
@misc{miao2023efficient,
      title={Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems}, 
      author={Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Hongyi Jin and Tianqi Chen and Zhihao Jia},
      year={2023},
      eprint={2312.15234},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### Retrieval-Augmented Generation for Large Language Models: A Survey

paper link: [here](https://arxiv.org/pdf/2312.10997.pdf)

github link: [here](https://github.com/Tongji-KGLLM/RAG-Survey)

citation:

```bibtex
@misc{gao2024retrievalaugmented,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Qianyu Guo and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey

paper link: [here](https://arxiv.org/pdf/2311.12351.pdf)

github link: [here](https://github.com/Strivin0311/long-llms-learning)

citation:

```bibtex
@misc{huang2024advancing,
      title={Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey}, 
      author={Yunpeng Huang and Jingwei Xu and Junyu Lai and Zixu Jiang and Taolue Chen and Zenan Li and Yuan Yao and Xiaoxing Ma and Lijuan Yang and Hao Chen and Shupeng Li and Penghao Zhao},
      year={2024},
      eprint={2311.12351},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

#### Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models

paper link: [here](https://arxiv.org/pdf/2311.03687)

citation:

```bibtex
@misc{zhang2023dissectingruntimeperformancetraining,
      title={Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models}, 
      author={Longteng Zhang and Xiang Liu and Zeyu Li and Xinglin Pan and Peijie Dong and Ruibo Fan and Rui Guo and Xin Wang and Qiong Luo and Shaohuai Shi and Xiaowen Chu},
      year={2023},
      eprint={2311.03687},
      archivePrefix={arXiv},
      primaryClass={cs.PF},
      url={https://arxiv.org/abs/2311.03687}, 
}
```


#### Challenges and applications of large language models

paper link: [here](https://arxiv.org/pdf/2307.10169)

citation: 
```bibtex
@article{kaddour2023challenges,
  title={Challenges and applications of large language models},
  author={Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  journal={arXiv preprint arXiv:2307.10169},
  year={2023}
}
```
    


#### A comprehensive overview of large language models

paper link: [here](https://arxiv.org/pdf/2307.06435)

citation: 
```bibtex
@article{naveed2023comprehensive,
  title={A comprehensive overview of large language models},
  author={Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Barnes, Nick and Mian, Ajmal},
  journal={arXiv preprint arXiv:2307.06435},
  year={2023}
}
```
    


#### ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope

paper link: [here](https://www.sciencedirect.com/science/article/pii/S266734522300024X)

citation: 
```bibtex
@article{ray2023chatgpt,
  title={ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope},
  author={Ray, Partha Pratim},
  journal={Internet of Things and Cyber-Physical Systems},
  year={2023},
  publisher={Elsevier}
}
```
    


#### Augmented language models: a survey

paper link: [here](https://arxiv.org/pdf/2302.07842)

citation: 
```bibtex
@article{mialon2023augmented,
  title={Augmented language models: a survey},
  author={Mialon, Gr{\'e}goire and Dess{\`\i}, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozi{\`e}re, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and others},
  journal={arXiv preprint arXiv:2302.07842},
  year={2023}
}
```

#### Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning

blog link: [here](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/)

citation:

```bibtex
@misc{Dettmers2023WhichGPU,
  author = {Tim Dettmers},
  title = {Which GPU for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning},
  year = {2023},
  month = {Jan},
  howpublished = {\url{https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/}},
}
```