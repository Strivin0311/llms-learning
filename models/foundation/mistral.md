# Mistral
*Here's some resources about Mistral*


#### Starling-7B- Increasing LLM Helpfulness & Harmlessness with RLAIF [`UNREAD`]

blog link: [here](https://starling.cs.berkeley.edu/)

model links: 

|model name|link|
|-|-|
|Starling-LM-7B-alpha|[here](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha)|
|Starling-RM-7B-alpha|[here](https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha)|


citation: 
```bibtex
@misc{starling2023,
    title = {Starling-7B: Improving LLM Helpfulness & Harmlessness with RLAIF},
    author = {Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Jiao, Jiantao},
    month = {November},
    year = {2023}
}
```

#### Mixtral of experts: A high quality Sparse Mixture-of-Experts [`UNREAD`]

blog link: [here](https://mistral.ai/news/mixtral-of-experts/)

model links: 

|model name|link|
|-|-|
|Mixtral-SlimOrca-8x7B|[here](https://huggingface.co/Open-Orca/Mixtral-SlimOrca-8x7B)|
|Mixtral-8x7B-Instruct-v0.1|[here](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)|
|Mixtral-8x7B-v0.1|[here](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)|

citation:
```bibtex
@online{mixtral_model,
  author = {Mistral AI},
  title = {Mixtral of Experts: A High-Quality Sparse Mixture-of-Experts},
  year = {2023},
  url = {\url{https://mistral.ai/news/mixtral-of-experts/}}
}
```


#### Mistral 7B [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2310.06825.pdf)

model links: 

|model name|link|
|-|-|
|Mistral-7B-OpenOrca|[here](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca)|
|Zephyr-7B-beta|[here](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)|
|OpenHermes-2.5-Mistral-7B|[here](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)|
|OpenHermes-2.5-neural-chat-7b-v3-1-7B|[here](https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-7b-v3-1-7B)|
|Neural-Chat-7b-v3-1|[here](https://huggingface.co/Intel/neural-chat-7b-v3-1)|
|Mistral-7B-Instruct-v0.1|[here](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)|
|Mistral-7B-v0.1|[here](https://huggingface.co/mistralai/Mistral-7B-v0.1)|



citation: 
```bibtex
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
```


#### OpenChat: Advancing Open-source Language Models with Mixed-Quality Data [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2309.11235.pdf)

model link: 

|model name|link|
|-|-|
|OpenChat3.5|[here](https://huggingface.co/openchat/openchat_3.5)|
|OpenChat3.5-1210|[here](https://huggingface.co/openchat/openchat-3.5-1210)|
|||


citation:
```bibtex
@article{wang2023openchat,
  title={Openchat: Advancing open-source language models with mixed-quality data},
  author={Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
  journal={arXiv preprint arXiv:2309.11235},
  year={2023}
}
```
    