# GLM
*Here're some resources about GLM*


#### ChatGLM3 - 6B

github link: [here](https://github.com/THUDM/ChatGLM3)

hf-repo link: [here](https://huggingface.co/THUDM/chatglm3-6b)


#### ChatGLM2 - 6B

github link: [here](https://github.com/THUDM/ChatGLM2-6B)

hf-repo link: [here](https://huggingface.co/THUDM/chatglm2-6b)


#### ChatGLM - 6B

github link: [here](https://github.com/THUDM/ChatGLM-6B)

hf-repo link: [here](https://huggingface.co/THUDM/chatglm-6b)




#### Glm-130b: An open bilingual pre-trained model [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2210.02414)

citation: 
```bibtex
@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}
```
    


#### Glm: General language model pretraining with autoregressive blank infilling [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2103.10360)

citation: 
```bibtex
@article{du2021glm,
  title={Glm: General language model pretraining with autoregressive blank infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10360},
  year={2021}
}
```
    