# Miscellaneous Models
*Here're some resources about Miscellaneous Models*



#### Grok-1: 314 billion parameter Mixture-of-Experts model trained from scratch by xAI [`READ`]

blog link: [here](https://x.ai/blog/grok-os)

github link: [here](https://github.com/xai-org/grok-1)

model link: [here](https://huggingface.co/xai-org/grok-1)

citation:
```bibtex
@misc{grok2024xai,
  title={Grok-1: 314 billion parameter Mixture-of-Experts model trained from scratch by xAI},
  author={XAI},
  howpublished={\url{https://github.com/xai-org/grok-1}},
  year={2024},
}

```


#### Sora: Video generation models as world simulators [`READ`]

blog link: [here](https://openai.com/research/video-generation-models-as-world-simulators)

homepage link: [here](https://openai.com/sora)

citation:
```bibtex
@article{videoworldsimulators2024,
  title={Video generation models as world simulators},
  author={Tim Brooks and Bill Peebles and Connor Holmes and Will DePue and Yufei Guo and Li Jing and David Schnurr and Joe Taylor and Troy Luhman and Eric Luhman and Clarence Ng and Ricky Wang and Aditya Ramesh},
  year={2024},
  url={https://openai.com/research/video-generation-models-as-world-simulators},
}
```


#### Phi-2: The surprising power of small language models [`READ`]

blog link: [here](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)

model links:

|model name|link|
|-|-|
|Phi-2|[here](https://huggingface.co/microsoft/phi-2)|

citation:
```bibtex
@misc{Javaheripi2023Phi2,
  author = {Javaheripi, Mojan and Bubeck, SÃ©bastien},
  title = {Phi-2: The Surprising Power of Small Language Models},
  year = {2023},
  month = {December},
  howpublished = {\url{https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/}},
}
```

#### Textbooks Are All You Need II: phi-1.5 technical report [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2309.05463.pdf)

model links:

|model name|link|
|-|-|
|Phi-1.5|[here](https://huggingface.co/microsoft/phi-1_5)|

citation:
```bibtex
@article{li2023textbooks,
  title={Textbooks Are All You Need II: \textbf{phi-1.5} technical report},
  author={Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
  journal={arXiv preprint arXiv:2309.05463},
  year={2023}
}

```


#### Textbooks Are All You Need (Phi-1) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2306.11644.pdf)

model links:

|model name|link|
|-|-|
|Phi-1|[here](https://huggingface.co/microsoft/phi-1)|

citation:
```bibtex
@article{gunasekar2023textbooks,
  title={Textbooks Are All You Need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}
```



#### Orca 2: Teaching Small Language Models How to Reason [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2311.11045.pdf)

model links: 

|model name|link|
|-|-|
|Orca-2-13b|[here](https://huggingface.co/microsoft/Orca-2-13b)|
|Orca-2-7b|[here](https://huggingface.co/microsoft/Orca-2-7b)|

citation:
```bibtex
@misc{mitra2023orca,
      title={Orca 2: Teaching Small Language Models How to Reason}, 
      author={Arindam Mitra and Luciano Del Corro and Shweti Mahajan and Andres Codas and Clarisse Simoes and Sahaj Agarwal and Xuxi Chen and Anastasia Razdaibiedina and Erik Jones and Kriti Aggarwal and Hamid Palangi and Guoqing Zheng and Corby Rosset and Hamed Khanpour and Ahmed Awadallah},
      year={2023},
      eprint={2311.11045},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
```


#### Anima: the first open source 33B Chinese LLM [`READ`]

github link: [here](https://github.com/lyogavin/Anima)

model links: 

|model name|link|
|-|-|
|Anima-7B-100K|[here](https://huggingface.co/lyogavin/Anima-7B-100K)|
|Anima-33B-merged|[here](https://huggingface.co/lyogavin/Anima33B-merged)|
|Anima-33B|[here](https://huggingface.co/lyogavin/Anima33B)|

citation:
```bibtex
@misc{anima2023github,
  author = {Gavin Li, Ikko Eltociear Ashimine, Naozumi},
  title = {Anima: the first open source 33B Chinese LLM},
  year = {2023},
  howpublished = {\url{https://github.com/lyogavin/Anima}},
}
```


#### Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality [`READ`]

blog link: [here](https://lmsys.org/blog/2023-03-30-vicuna/)

github link: [here](https://github.com/lm-sys/FastChat)

model links: 

|model name|link|
|-|-|
|Vicuna-33b-v1.3|[here](https://huggingface.co/lmsys/vicuna-33b-v1.3)|
|Vicuna-13b-v1.5-16k|[here](https://huggingface.co/lmsys/vicuna-13b-v1.5-16k)|
|Vicuna-13b-v1.5|[here](https://huggingface.co/lmsys/vicuna-13b-v1.5)|
|Vicuna-7b-v1.5-16k|[here](https://huggingface.co/lmsys/vicuna-7b-v1.5-16k)|
|Vicuna-7b-v1.5|[here](https://huggingface.co/lmsys/vicuna-7b-v1.5)|

citation:
```bibtex
@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}
```

#### Baize: An open-source chat model with parameter-efficient tuning on self-chat data [`READ`]

paper link: [here](https://arxiv.org/pdf/2304.01196)

citation: 
```bibtex
@article{xu2023baize,
  title={Baize: An open-source chat model with parameter-efficient tuning on self-chat data},
  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  journal={arXiv preprint arXiv:2304.01196},
  year={2023}
}
```


#### Flamingo: a visual language model for few-shot learning [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf)

citation: 
```bibtex
@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}
```


#### OPT: Open pre-trained transformer language models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR1_0YiQKgxIsy8unzoLvL9E2OA41_kze-H0YvhoCzIQUp_gk-MR9dUs2ZE)

model links: 

|model name|link|
|-|-|
|OPT-30b|[here](https://huggingface.co/facebook/opt-30b)|
|OPT-13b|[here](https://huggingface.co/facebook/opt-13b)|
|OPT-6.7b|[here](https://huggingface.co/facebook/opt-6.7b)|
|OPT-2.7b|[here](https://huggingface.co/facebook/opt-2.7b)|
|OPT-1.3b|[here](https://huggingface.co/facebook/opt-1.3b)|

citation: 
```bibtex
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
```


#### Learning transferable visual models from natural language supervision (CLIP) [`READ`]

paper link: [here](http://proceedings.mlr.press/v139/radford21a/radford21a.pdf)

citation: 
```bibtex
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
```
    

#### LaMDA: Language Models for Dialog Applications [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2201.08239.pdf)

blog link: [here](https://blog.google/technology/ai/lamda/)

citation:
```bibtex
@misc{thoppilan2022lamda,
      title={LaMDA: Language Models for Dialog Applications}, 
      author={Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and YaGuang Li and Hongrae Lee and Huaixiu Steven Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Vincent Zhao and Yanqi Zhou and Chung-Ching Chang and Igor Krivokon and Will Rusch and Marc Pickett and Pranesh Srinivasan and Laichee Man and Kathleen Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Soraker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark Diaz and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravi Rajakumar and Alena Butryna and Matthew Lamm and Viktoriya Kuzmina and Joe Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
      year={2022},
      eprint={2201.08239},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```