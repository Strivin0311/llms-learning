# Miscellaneous Models
*Here's some resources about Miscellaneous Models*


#### Gemini: A Family of Highly Capable Multimodal Models

paper link: [here](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)

citation:
```bibtex
 @article{gemini23google, 
    title={Gemini: A Family of Highly Capable Multimodal Models}, 
    url={https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf}, 
    author={Gemini Team, Google},
    year={2023}, 
    month={Dec}, 
    pages={62}
 }
```


#### Mistral 7B [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2310.06825.pdf)

citation: 
```bibtex
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
```
    


#### Baize: An open-source chat model with parameter-efficient tuning on self-chat data [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2304.01196)

citation: 
```bibtex
@article{xu2023baize,
  title={Baize: An open-source chat model with parameter-efficient tuning on self-chat data},
  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  journal={arXiv preprint arXiv:2304.01196},
  year={2023}
}
```


#### Flamingo: a visual language model for few-shot learning [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf)

citation: 
```bibtex
@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}
```


#### Opt: Open pre-trained transformer language models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR1_0YiQKgxIsy8unzoLvL9E2OA41_kze-H0YvhoCzIQUp_gk-MR9dUs2ZE)

citation: 
```bibtex
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
```
    

#### Pangu-weather: A 3d high-resolution model for fast and accurate global weather forecast [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2211.02556)

citation: 
```bibtex
@article{bi2022pangu,
  title={Pangu-weather: A 3d high-resolution model for fast and accurate global weather forecast},
  author={Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
  journal={arXiv preprint arXiv:2211.02556},
  year={2022}
}
```

