# Miscellaneous Models
*Here's some resources about Miscellaneous Models*



#### Orca 2: Teaching Small Language Models How to Reason ['UNREAD']

paper link: [here](https://arxiv.org/pdf/2311.11045.pdf)

model links: 

|model name|link|
|-|-|
|Orca-2-13b|[here](https://huggingface.co/microsoft/Orca-2-13b)|

citation:
```bibtex
@misc{mitra2023orca,
      title={Orca 2: Teaching Small Language Models How to Reason}, 
      author={Arindam Mitra and Luciano Del Corro and Shweti Mahajan and Andres Codas and Clarisse Simoes and Sahaj Agarwal and Xuxi Chen and Anastasia Razdaibiedina and Erik Jones and Kriti Aggarwal and Hamid Palangi and Guoqing Zheng and Corby Rosset and Hamed Khanpour and Ahmed Awadallah},
      year={2023},
      eprint={2311.11045},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
```


#### Gemini: A Family of Highly Capable Multimodal Models ['UNREAD']

paper link: [here](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)

citation:
```bibtex
 @article{gemini23google, 
    title={Gemini: A Family of Highly Capable Multimodal Models}, 
    url={https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf}, 
    author={Gemini Team, Google},
    year={2023}, 
    month={Dec}, 
    pages={62}
 }
```


#### Starling-7B- Increasing LLM Helpfulness & Harmlessness with RLAIF ['UNREAD']

blog link: [here](https://starling.cs.berkeley.edu/)

model links: 

|model name|link|
|-|-|
|Starling-RM-7B-alpha|[here](https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha)|
|||

citation: 
```bibtex
@misc{starling2023,
    title = {Starling-7B: Improving LLM Helpfulness & Harmlessness with RLAIF},
    author = {Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Jiao, Jiantao},
    month = {November},
    year = {2023}
}
```


#### Mistral 7B [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2310.06825.pdf)

model links: 

|model name|link|
|-|-|
|Mistral-7B-v0.1|[here](https://huggingface.co/mistralai/Mistral-7B-v0.1)|
|Mistral-7B-Instruct-v0.1|[here](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)|
|Neural-Chat-7b-v3-1|[here](https://huggingface.co/Intel/neural-chat-7b-v3-1)|
|OpenHermes-2.5-Mistral-7B|[here](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)|
|OpenHermes-2.5-neural-chat-7b-v3-1-7B|[here](https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-7b-v3-1-7B)|
|Zephyr-7B-beta|[here](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)|


citation: 
```bibtex
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
```


#### OpenChat: Advancing Open-source Language Models with Mixed-Quality Data ['UNREAD']

paper link: [here](https://arxiv.org/pdf/2309.11235.pdf)

model link: 

|model name|link|
|-|-|
|OpenChat3.5|[here](https://huggingface.co/openchat/openchat_3.5)|
|||


citation:
```bibtex
@article{wang2023openchat,
  title={Openchat: Advancing open-source language models with mixed-quality data},
  author={Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
  journal={arXiv preprint arXiv:2309.11235},
  year={2023}
}
```
    


#### Baize: An open-source chat model with parameter-efficient tuning on self-chat data [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2304.01196)

citation: 
```bibtex
@article{xu2023baize,
  title={Baize: An open-source chat model with parameter-efficient tuning on self-chat data},
  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  journal={arXiv preprint arXiv:2304.01196},
  year={2023}
}
```


#### Flamingo: a visual language model for few-shot learning [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf)

citation: 
```bibtex
@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}
```


#### Opt: Open pre-trained transformer language models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR1_0YiQKgxIsy8unzoLvL9E2OA41_kze-H0YvhoCzIQUp_gk-MR9dUs2ZE)

citation: 
```bibtex
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
```
    

#### Pangu-weather: A 3d high-resolution model for fast and accurate global weather forecast [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2211.02556)

citation: 
```bibtex
@article{bi2022pangu,
  title={Pangu-weather: A 3d high-resolution model for fast and accurate global weather forecast},
  author={Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
  journal={arXiv preprint arXiv:2211.02556},
  year={2022}
}
```


#### Learning transferable visual models from natural language supervision (CLIP) [`UNREAD`]

paper link: [here](http://proceedings.mlr.press/v139/radford21a/radford21a.pdf)

citation: 
```bibtex
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
```
    