# GPT
*Here're some resources about GPT*



#### Sparks of artificial general intelligence: Early experiments with gpt-4 [`READ`]

paper link: [here](https://arxiv.org/pdf/2303.12712.pdf?utm_source=webtekno)

citation: 
```bibtex
@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}
```
    
    

#### GPT-4 Technical Report [`READ`]

paper link: [here](https://arxiv.org/pdf/2303.08774.pdf)


citation: 
```bibtex
@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

#### Reasearch on GPT4  [`READ`]

blog link: [here](https://openai.com/research/gpt-4)

citation:
```bibtex
@online{openai-blog-gpt4,
  author={OpenAI},
  title={OpenAI: GPT-4, 2023},
  year={2023},
  url={https://openai.com/research/gpt-4},
}
```


#### Introducing ChatGPT  [`READ`]

blog link: [here](https://openai.com/blog/chatgpt)

[here](../../notebooks/tutorial_openai_assistant_api.ipynb) is a tutorial notebook about how to use the basic openai assistant API

citation:
```bibtex
@online{openai-blog-chatgpt,
  author={OpenAI},
  title={OpenAI: Introducing ChatGPT},
  year={2022},
  url={https://openai.com/blog/chatgpt},
}
```



#### Language models are unsupervised multitask learners (GPT3) [`READ`]

paper link: [here](https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf)

citation: 
```bibtex
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
```
    


#### Language models are few-shot learners (GPT2) [`READ`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)

citation: 
```bibtex
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
```
    


#### Improving language understanding by generative pre-training (GPT1) [`READ`]

paper link: [here](https://www.mikecaptain.com/resources/pdf/GPT-1.pdf)

citation: 
```bibtex
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}
```
    