# ERNIE
*Here're some resources about ERNIE*


#### Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2107.02137)

citation: 
```bibtex
@article{sun2021ernie,
  title={Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation},
  author={Sun, Yu and Wang, Shuohuan and Feng, Shikun and Ding, Siyu and Pang, Chao and Shang, Junyuan and Liu, Jiaxiang and Chen, Xuyi and Zhao, Yanbin and Lu, Yuxiang and others},
  journal={arXiv preprint arXiv:2107.02137},
  year={2021}
}
```
    

#### Ernie 2.0: A continual pre-training framework for language understanding [`UNREAD`]

paper link: [here](https://ojs.aaai.org/index.php/AAAI/article/download/6428/6284)

citation: 
```bibtex
@inproceedings{sun2020ernie,
  title={Ernie 2.0: A continual pre-training framework for language understanding},
  author={Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={8968--8975},
  year={2020}
}
```
    

#### ERNIE: Enhanced language representation with informative entities [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1905.07129)

citation: 
```bibtex
@article{zhang2019ernie,
  title={ERNIE: Enhanced language representation with informative entities},
  author={Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  journal={arXiv preprint arXiv:1905.07129},
  year={2019}
}
```