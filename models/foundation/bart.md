# BART
*Here're some resources about BART*


#### Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension [`READ`]

paper link: [here](https://arxiv.org/pdf/1910.13461)

model links: 

|model name|link|
|-|-|
|bart-large|[here](https://huggingface.co/facebook/bart-large)|
|bart-base|[here](https://huggingface.co/facebook/bart-base)|

citation: 
```bibtex
@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}
```