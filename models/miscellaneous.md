# Miscellaneous Models
*Here's some resources about Miscellaneous Models*


## Decoder-only


#### Opt: Open pre-trained transformer language models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR1_0YiQKgxIsy8unzoLvL9E2OA41_kze-H0YvhoCzIQUp_gk-MR9dUs2ZE)

citation: 
```bibtex
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
```
    


#### Flamingo: a visual language model for few-shot learning [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf)

citation: 
```bibtex
@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}
```


    
### PaLM

#### Palm 2 technical report [`READ`]

paper link: [here](https://arxiv.org/pdf/2305.10403)

citation: 
```bibtex
@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}
```
    

#### Palm: Scaling language modeling with pathways [`READ`]

paper link: [here](https://arxiv.org/pdf/2204.02311)

citation: 
```bibtex
@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}
```


#### Pangu-weather: A 3d high-resolution model for fast and accurate global weather forecast [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2211.02556)

citation: 
```bibtex
@article{bi2022pangu,
  title={Pangu-weather: A 3d high-resolution model for fast and accurate global weather forecast},
  author={Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
  journal={arXiv preprint arXiv:2211.02556},
  year={2022}
}
```
    
    


## Encoder-Decoder

### T5

#### Exploring the limits of transfer learning with a unified text-to-text transformer [`READ`]

paper link: [here](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf)

citation: 
```bibtex
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}
```
    

### BART

#### Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension [`READ`]

paper link: [here](https://arxiv.org/pdf/1910.13461)

citation: 
```bibtex
@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}
```
    


## Encoder-only


