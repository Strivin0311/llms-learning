{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Test Toy for Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_WITH_REF = False # NOTE: toggle this flag to `True` to enable testing with running the cells with ref\n",
    "# TEST_WITH_REF = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" # NOTE: you had better use \"cuda\", otherwise it might be very slow\n",
    "# device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./model/llama_3.2_1b_instruct/\"\n",
    "num_shards = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step0. set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.testing import assert_close\n",
    "\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_WITH_REF:\n",
    "    from ref.modeling import (\n",
    "        BatchLayout as BatchLayoutRef,\n",
    "        PaddingSide as PaddingSideRef,\n",
    "        TruncateSide as TruncateSideRef,\n",
    "        \n",
    "        PromptType as PromptTypeRef,\n",
    "        PromptTemplate as PromptTemplateRef,\n",
    "    )\n",
    "    from ref.modeling.models import (\n",
    "        LlamaConfig as LlamaConfigRef,\n",
    "        LlamaTokenizer as LlamaTokenizerRef,\n",
    "        LlamaModel as LlamaModelRef,\n",
    "    )\n",
    "    from ref.inference import (\n",
    "        DecodeStrategy as DecodeStrategyRef,\n",
    "        InferenceConfig as InferenceConfigRef,\n",
    "        InferenceAgent as InferenceAgentRef,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modeling import (\n",
    "    BatchLayout,\n",
    "    PaddingSide,\n",
    "    TruncateSide,\n",
    "    \n",
    "    PromptType,\n",
    "    PromptTemplate,\n",
    ")\n",
    "from src.modeling.models import (\n",
    "    LlamaConfig,\n",
    "    LlamaTokenizer,\n",
    "    LlamaModel,\n",
    ")\n",
    "from src.inference import (\n",
    "    DecodeStrategy,\n",
    "    InferenceConfig,\n",
    "    InferenceAgent,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1. load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = os.path.join(model_dir, \"config.json\")\n",
    "params_files = os.path.join(model_dir, \"model.safetensors\")\n",
    "\n",
    "with open(config_file, \"r\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_config_ref = None\n",
    "if TEST_WITH_REF:\n",
    "    llama_config_ref: LlamaConfigRef = LlamaModelRef.load_config(\n",
    "        config_file, \n",
    "        param_device=device,\n",
    "    )\n",
    "llama_config_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "********************   LlamaConfig   ********************\n",
       "activation_type: MLPActivationType.SILU\n",
       "apply_qk_norm: False\n",
       "causal: True\n",
       "eps: 1e-05\n",
       "ffh_size: 8192\n",
       "gate_init_mean: 0.0\n",
       "gate_init_std: 1.0\n",
       "group_size: None\n",
       "head_dim: 64\n",
       "hidden_size: 2048\n",
       "init_base_seed: 42\n",
       "lm_head_tied: True\n",
       "lora_alpha: None\n",
       "lora_dropout_rate: 0.0\n",
       "lora_dropout_seed: 42\n",
       "lora_init_base_seed: 42\n",
       "lora_rank: 0\n",
       "max_seq_len: 8192\n",
       "moe_topk: 1\n",
       "norm_init_range: (-1.0, 1.0)\n",
       "num_experts: None\n",
       "num_kv_head: 8\n",
       "num_layers: 16\n",
       "num_q_head: 32\n",
       "online_attn_block_size: None\n",
       "param_device: 'cpu'\n",
       "param_dtype: torch.bfloat16\n",
       "process_group: None\n",
       "proj_init_mean: 0.0\n",
       "proj_init_seed: 42\n",
       "proj_init_std: 1.0\n",
       "qk_norm_group_size: None\n",
       "qkv_layout: AttnQKVLayout.BSHD\n",
       "qkv_pack_format: AttnQKVPackFormat.Q_K_V\n",
       "rank: 0\n",
       "rope_base: 500000.0\n",
       "rope_dynamic: False\n",
       "rope_ratio: 1\n",
       "softmax_cap: None\n",
       "softmax_clip_range: (0.0, 1.0)\n",
       "softmax_dropout_rate: 0.0\n",
       "softmax_dropout_seed: 42\n",
       "softmax_scale: None\n",
       "softmax_temp: 1.0\n",
       "vocab_init_mean: 0.0\n",
       "vocab_init_std: 1.0\n",
       "vocab_size: 128256\n",
       "window_size: None\n",
       "world_size: 1\n",
       "*********************************************************"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_config: LlamaConfig = LlamaModel.load_config(\n",
    "    config_file, \n",
    "    param_device=device,\n",
    ")\n",
    "llama_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_hf = LlamaForCausalLM.from_pretrained(model_dir, torch_dtype=llama_config.param_dtype).to(device)\n",
    "llama_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model_ref = None\n",
    "if TEST_WITH_REF:\n",
    "    llama_model_ref = LlamaModelRef(llama_config_ref)\n",
    "    llama_model_ref.load_parameters(params_files)\n",
    "llama_model_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (block): TransformerDecoderBlock(\n",
       "    (vocab_emb): ParallelVocabEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x TransformerDecoderLayer(\n",
       "        (attn_pre_norm): GroupRMSNorm()\n",
       "        (rope): NTKAwareRoPE()\n",
       "        (attn): OfflineSlidingWindowAttn(\n",
       "          (softmax_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp_pre_norm): GroupRMSNorm()\n",
       "        (mlp): DenseMLPWithLoRA()\n",
       "      )\n",
       "    )\n",
       "    (kv_cache): TransformerDecoderKVCache()\n",
       "    (final_norm): GroupRMSNorm()\n",
       "    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_model = LlamaModel(llama_config)\n",
    "llama_model.load_parameters(params_files)\n",
    "llama_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2. load the pretrained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_file = os.path.join(model_dir, \"tokenizer.json\")\n",
    "tokenizer_config_file = os.path.join(model_dir, \"tokenizer_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The key to life is not to be afraid to take risks and try new',\n",
       " 'The key to life is not to be happy, but to be content')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"The key to life is\"\n",
    "response1 = \" not to be afraid to take risks and try new\"\n",
    "response2 = \" not to be happy, but to be content\"\n",
    "prompt1 = query + response1\n",
    "prompt2 = query + response2\n",
    "prompt1, prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|eot_id|> 128009 <|eot_id|> 128009 <|begin_of_text|> 128000\n",
      "[[128000, 791, 1401, 311, 2324, 374, 539, 311, 387, 16984, 311, 1935, 15635, 323, 1456, 502], [128000, 791, 1401, 311, 2324, 374, 539, 311, 387, 6380, 11, 719, 311, 387, 2262]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The key to life is not to be afraid to take risks and try new',\n",
       " 'The key to life is not to be happy, but to be content']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_hf_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "llama_hf_tokenizer.pad_token_id = llama_hf_tokenizer.eos_token_id # NOTE: it pads right by default\n",
    "print(llama_hf_tokenizer.pad_token, llama_hf_tokenizer.pad_token_id, llama_hf_tokenizer.eos_token, llama_hf_tokenizer.eos_token_id, llama_hf_tokenizer.bos_token, llama_hf_tokenizer.bos_token_id)\n",
    "\n",
    "encoded_ids_hf = llama_hf_tokenizer([prompt1, prompt2]).input_ids\n",
    "print(encoded_ids_hf)\n",
    "\n",
    "decoded_prompts = llama_hf_tokenizer.batch_decode(encoded_ids_hf, skip_special_tokens=True)\n",
    "decoded_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_WITH_REF:\n",
    "    llama_tokenizer_ref = LlamaTokenizerRef(\n",
    "        vocab_file=tokenizer_file,\n",
    "        config_file=tokenizer_config_file,\n",
    "    )\n",
    "    print(llama_tokenizer_ref)\n",
    "\n",
    "    encoded_ids_ref = llama_tokenizer_ref.encode([prompt1, prompt2])\n",
    "    print(encoded_ids_ref)\n",
    "    \n",
    "    decoded_prompts_ref = llama_tokenizer_ref.decode(encoded_ids_ref)\n",
    "    print(decoded_prompts_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizer()\n",
      "[tensor([128000,    791,   1401,    311,   2324,    374,    539,    311,    387,\n",
      "         16984,    311,   1935,  15635,    323,   1456,    502]), tensor([128000,    791,   1401,    311,   2324,    374,    539,    311,    387,\n",
      "          6380,     11,    719,    311,    387,   2262])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The key to life is not to be afraid to take risks and try new',\n",
       " 'The key to life is not to be happy, but to be content']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer = LlamaTokenizer(\n",
    "    vocab_file=tokenizer_file,\n",
    "    config_file=tokenizer_config_file,\n",
    ")\n",
    "print(llama_tokenizer)\n",
    "\n",
    "encoded_ids = llama_tokenizer.encode([prompt1, prompt2])\n",
    "print(encoded_ids)\n",
    "\n",
    "decoded_prompts = llama_tokenizer.decode(encoded_ids)\n",
    "decoded_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3. test PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_WITH_REF:\n",
    "    prompt_template_ref = PromptTemplateRef(\n",
    "        template_str=\"I am a {profession} named {name}, with the age of {age}.\",\n",
    "    )\n",
    "\n",
    "    prompt_template_ref.set_default(name=\"John\", age=\"24\")\n",
    "    print(prompt_template_ref.keys())\n",
    "    prompt_str_ref = prompt_template_ref(profession=\"programmer\")\n",
    "    print(prompt_str_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'profession': None, 'name': 'John', 'age': '24'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am a programmer named John, with the age of 24.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template_str=\"I am a {profession} named {name}, with the age of {age}.\",\n",
    ")\n",
    "\n",
    "prompt_template.set_default(name=\"John\", age=\"24\")\n",
    "print(prompt_template.keys())\n",
    "prompt_str = prompt_template(profession=\"programmer\")\n",
    "prompt_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step4. load the InferenceConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config_file = os.path.join(model_dir, \"generation_config.json\")\n",
    "max_new_tokens = 20\n",
    "sampling_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_WITH_REF:\n",
    "    inf_config_loadded_ref = InferenceAgentRef.load_generation_config(\n",
    "        generation_config_file, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        sampling_seed=sampling_seed,\n",
    "        device=device,\n",
    "    )\n",
    "    print(inf_config_loadded_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "********************   InferenceConfig   ********************\n",
       "batch_layout: BatchLayout.STACK\n",
       "decode_strategy: DecodeStrategy.SAMPLING\n",
       "device: 'cpu'\n",
       "max_new_tokens: 20\n",
       "pad_to_multiple_of: 1\n",
       "padding_side: PaddingSide.LEFT\n",
       "sampling_seed: 42\n",
       "streaming: False\n",
       "temperature: 0.6\n",
       "top_k: 50\n",
       "top_p: 0.9\n",
       "truncate_length: None\n",
       "truncate_side: TruncateSide.RIGHT\n",
       "*************************************************************"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_config_loadded = InferenceAgent.load_generation_config(\n",
    "    generation_config_file, \n",
    "    max_new_tokens=max_new_tokens, \n",
    "    sampling_seed=sampling_seed,\n",
    "    device=device,\n",
    ")\n",
    "inf_config_loadded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_WITH_REF:\n",
    "    inf_config_ref = InferenceConfigRef(\n",
    "        decode_strategy=DecodeStrategyRef.GREEDY,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=1.0,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        sampling_seed=sampling_seed,\n",
    "        padding_side=PaddingSideRef.LEFT,\n",
    "        pad_to_multiple_of=1,\n",
    "        truncate_length=None,\n",
    "        truncate_side=TruncateSideRef.RIGHT,\n",
    "        device=device,\n",
    "    )\n",
    "    print(inf_config_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "********************   InferenceConfig   ********************\n",
       "batch_layout: BatchLayout.STACK\n",
       "decode_strategy: DecodeStrategy.GREEDY\n",
       "device: 'cpu'\n",
       "max_new_tokens: 20\n",
       "pad_to_multiple_of: 1\n",
       "padding_side: PaddingSide.LEFT\n",
       "sampling_seed: 42\n",
       "streaming: False\n",
       "temperature: 1.0\n",
       "top_k: 50\n",
       "top_p: 0.9\n",
       "truncate_length: None\n",
       "truncate_side: TruncateSide.RIGHT\n",
       "*************************************************************"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_config = InferenceConfig(\n",
    "    decode_strategy=DecodeStrategy.GREEDY,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=1.0,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    sampling_seed=sampling_seed,\n",
    "    padding_side=PaddingSide.LEFT,\n",
    "    pad_to_multiple_of=1,\n",
    "    truncate_length=None,\n",
    "    truncate_side=TruncateSide.RIGHT,\n",
    "    device=device,\n",
    ")\n",
    "inf_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step5. load the InferenceAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_generation.TextGenerationPipeline at 0x7f31f8cf54f0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_hf = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llama_hf,\n",
    "    tokenizer=llama_hf_tokenizer,\n",
    "    device=device,\n",
    "    do_sample=inf_config.decode_strategy == DecodeStrategy.SAMPLING,\n",
    "    max_new_tokens=inf_config.max_new_tokens,\n",
    "    temperature=inf_config.temperature,\n",
    "    top_p=inf_config.top_p,\n",
    "    top_k=inf_config.top_k,\n",
    ")\n",
    "pipe_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_WITH_REF:\n",
    "    inf_agent_ref = InferenceAgentRef(\n",
    "        config=inf_config_ref,\n",
    "        # config=inf_config_loadded_ref,\n",
    "        model=llama_model_ref,\n",
    "        tokenizer=llama_tokenizer_ref,\n",
    "    )\n",
    "    print(inf_agent_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InferenceAgent(\n",
       "  (model): LlamaModel(\n",
       "    (block): TransformerDecoderBlock(\n",
       "      (vocab_emb): ParallelVocabEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x TransformerDecoderLayer(\n",
       "          (attn_pre_norm): GroupRMSNorm()\n",
       "          (rope): NTKAwareRoPE()\n",
       "          (attn): OfflineSlidingWindowAttn(\n",
       "            (softmax_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (mlp_pre_norm): GroupRMSNorm()\n",
       "          (mlp): DenseMLPWithLoRA()\n",
       "        )\n",
       "      )\n",
       "      (kv_cache): TransformerDecoderKVCache()\n",
       "      (final_norm): GroupRMSNorm()\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (tokenizer): LlamaTokenizer()\n",
       "  (_system_prompt_template): PromptTemplate()\n",
       "  (_context_prompt_template): PromptTemplate()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_agent = InferenceAgent(\n",
    "    config=inf_config,\n",
    "    # config=inf_config_loadded,\n",
    "    model=llama_model,\n",
    "    tokenizer=llama_tokenizer,\n",
    ")\n",
    "inf_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step6. test InferenceAgent on text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"You're a helpful assitant on life.\\n\",\n",
       " 'Fill the sentence below for you to make it reasonable.\\n')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt_template = PromptTemplate(\n",
    "    template_str=\"You're a helpful assitant on {subject}.\\n\",\n",
    ")\n",
    "context_prompt_template = PromptTemplate(\n",
    "    template_str=\"Fill the sentence below for you to make it {adjective}.\\n\",\n",
    ")\n",
    "\n",
    "subject = \"life\"\n",
    "adjective = \"reasonable\"\n",
    "\n",
    "system_prompt_str = system_prompt_template(subject=subject)\n",
    "context_prompt_str = context_prompt_template(adjective=adjective)\n",
    "system_prompt_str, context_prompt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"You're a helpful assitant on life.\\nFill the sentence below for you to make it reasonable.\\nThe key to life is\",\n",
       " \"You're a helpful assitant on life.\\nFill the sentence below for you to make it reasonable.\\nThe only thing we have to fear is\",\n",
       " \"You're a helpful assitant on life.\\nFill the sentence below for you to make it reasonable.\\nThe cat jumped on the keyboard and accidentally\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "querys = [\n",
    "    \"The key to life is\",\n",
    "    \"The only thing we have to fear is\",\n",
    "    \"The cat jumped on the keyboard and accidentally\",\n",
    "]\n",
    "\n",
    "prompts = [\n",
    "    system_prompt_str + context_prompt_str + q\n",
    "    for q in querys\n",
    "]\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyp/anaconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================= The 0-th sample in the batch =========================\n",
      "[generated_text]: You're a helpful assitant on life.\n",
      "Fill the sentence below for you to make it reasonable.\n",
      "The key to life is to be happy and content with what you have, and to never let anyone else's opinions dictate your\n",
      "\n",
      "========================= The 1-th sample in the batch =========================\n",
      "[generated_text]: You're a helpful assitant on life.\n",
      "Fill the sentence below for you to make it reasonable.\n",
      "The only thing we have to fear is fear itself. (Franklin D. Roosevelt)\n",
      "This is a famous quote from the 32nd\n",
      "\n",
      "========================= The 2-th sample in the batch =========================\n",
      "[generated_text]: You're a helpful assitant on life.\n",
      "Fill the sentence below for you to make it reasonable.\n",
      "The cat jumped on the keyboard and accidentally typed out a message that said \"I'm purr-fectly happy to be stuck in this\n"
     ]
    }
   ],
   "source": [
    "prompt_dicts = pipe_hf(prompts)\n",
    "for i, prompt_dict in enumerate(prompt_dicts):\n",
    "    print(f\"\\n{'='*25} The {i}-th sample in the batch {'='*25}\")\n",
    "    generated_text = prompt_dict[0][\"generated_text\"]\n",
    "    print(f\"[generated_text]: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_WITH_REF:\n",
    "    inf_agent_ref.set_prompt(\n",
    "        prompt_template=system_prompt_template,\n",
    "        prompt_type=PromptTypeRef.SYSTEM,\n",
    "    )\n",
    "    inf_agent_ref.set_prompt(\n",
    "        prompt_template=context_prompt_template,\n",
    "        prompt_type=PromptTypeRef.CONTEXT,\n",
    "    )\n",
    "    \n",
    "    prompt_dicts = inf_agent_ref(querys, subject=subject, adjective=adjective)\n",
    "    for i, prompt_dict in enumerate(prompt_dicts):\n",
    "        print(f\"\\n{'='*25} The {i}-th sample in the batch {'='*25}\")\n",
    "        for prompt_type, promp in prompt_dict.items():\n",
    "            print(f\"\\n[{prompt_type}]: {promp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================= The 0-th sample in the batch =========================\n",
      "\n",
      "[PromptType.SYSTEM]: You're a helpful assitant on life.\n",
      "\n",
      "\n",
      "[PromptType.CONTEXT]: Fill the sentence below for you to make it reasonable.\n",
      "\n",
      "\n",
      "[PromptType.QUERY]: The key to life is\n",
      "\n",
      "[PromptType.RESPONSE]:  to be a good friend to others. \n",
      "The best way to do this is to be a good\n",
      "\n",
      "[PromptType.PROMPT]: You're a helpful assitant on life.\n",
      "Fill the sentence below for you to make it reasonable.\n",
      "The key to life is\n",
      "\n",
      "[PromptType.ALL]: You're a helpful assitant on life.\n",
      "Fill the sentence below for you to make it reasonable.\n",
      "The key to life is to be a good friend to others. \n",
      "The best way to do this is to be a good\n",
      "\n",
      "========================= The 1-th sample in the batch =========================\n",
      "\n",
      "[PromptType.SYSTEM]: You're a helpful assitant on life.\n",
      "\n",
      "\n",
      "[PromptType.CONTEXT]: Fill the sentence below for you to make it reasonable.\n",
      "\n",
      "\n",
      "[PromptType.QUERY]: The only thing we have to fear is\n",
      "\n",
      "[PromptType.RESPONSE]:  fear itself. (Franklin D. Roosevelt)\n",
      "This is a famous quote from the 32nd\n",
      "\n",
      "[PromptType.PROMPT]: You're a helpful assitant on life.\n",
      "Fill the sentence below for you to make it reasonable.\n",
      "The only thing we have to fear is\n",
      "\n",
      "[PromptType.ALL]: You're a helpful assitant on life.\n",
      "Fill the sentence below for you to make it reasonable.\n",
      "The only thing we have to fear is fear itself. (Franklin D. Roosevelt)\n",
      "This is a famous quote from the 32nd\n",
      "\n",
      "========================= The 2-th sample in the batch =========================\n",
      "\n",
      "[PromptType.SYSTEM]: You're a helpful assitant on life.\n",
      "\n",
      "\n",
      "[PromptType.CONTEXT]: Fill the sentence below for you to make it reasonable.\n",
      "\n",
      "\n",
      "[PromptType.QUERY]: The cat jumped on the keyboard and accidentally\n",
      "\n",
      "[PromptType.RESPONSE]:  typed out a message that said \"I'm purr-fectly happy to be stuck in this\n",
      "\n",
      "[PromptType.PROMPT]: You're a helpful assitant on life.\n",
      "Fill the sentence below for you to make it reasonable.\n",
      "The cat jumped on the keyboard and accidentally\n",
      "\n",
      "[PromptType.ALL]: You're a helpful assitant on life.\n",
      "Fill the sentence below for you to make it reasonable.\n",
      "The cat jumped on the keyboard and accidentally typed out a message that said \"I'm purr-fectly happy to be stuck in this\n"
     ]
    }
   ],
   "source": [
    "inf_agent.set_prompt(\n",
    "    prompt_template=system_prompt_template,\n",
    "    prompt_type=PromptType.SYSTEM,\n",
    ")\n",
    "inf_agent.set_prompt(\n",
    "    prompt_template=context_prompt_template,\n",
    "    prompt_type=PromptType.CONTEXT,\n",
    ")\n",
    "prompt_dicts = inf_agent(querys, subject=subject, adjective=adjective)\n",
    "for i, prompt_dict in enumerate(prompt_dicts):\n",
    "    print(f\"\\n{'='*25} The {i}-th sample in the batch {'='*25}\")\n",
    "    for prompt_type, promp in prompt_dict.items():\n",
    "        print(f\"\\n[{prompt_type}]: {promp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
