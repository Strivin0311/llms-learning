{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Toy for Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_WITH_REF = False # NOTE: toggle this flag to `True` to enable testing with running the cells with ref\n",
    "# TEST_WITH_REF = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" # NOTE: you had better use \"cuda\", otherwise it might be very slow\n",
    "# device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./model/llama_3.2_1b_instruct/\"\n",
    "num_shards = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step0. set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.testing import assert_close\n",
    "\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_WITH_REF:\n",
    "    from ref.modeling.models import (\n",
    "        LlamaConfig as LlamaConfigRef,\n",
    "        LlamaModel as LlamaModelRef,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modeling.models import (\n",
    "    LlamaConfig,\n",
    "    LlamaModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1. test LlamaConfig loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/llama_3.2_1b_instruct/config.json',\n",
       " './model/llama_3.2_1b_instruct/model.safetensors')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file = os.path.join(model_dir, \"config.json\")\n",
    "params_files = os.path.join(model_dir, \"model.safetensors\")\n",
    "config_file, params_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architectures': ['LlamaForCausalLM'],\n",
       " 'attention_bias': False,\n",
       " 'attention_dropout': 0.0,\n",
       " 'bos_token_id': 128000,\n",
       " 'eos_token_id': [128001, 128008, 128009],\n",
       " 'head_dim': 64,\n",
       " 'hidden_act': 'silu',\n",
       " 'hidden_size': 2048,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 8192,\n",
       " 'max_position_embeddings': 131072,\n",
       " 'mlp_bias': False,\n",
       " 'model_type': 'llama',\n",
       " 'num_attention_heads': 32,\n",
       " 'num_hidden_layers': 16,\n",
       " 'num_key_value_heads': 8,\n",
       " 'pretraining_tp': 1,\n",
       " 'rms_norm_eps': 1e-05,\n",
       " 'rope_scaling': {'factor': 32.0,\n",
       "  'high_freq_factor': 4.0,\n",
       "  'low_freq_factor': 1.0,\n",
       "  'original_max_position_embeddings': 8192,\n",
       "  'rope_type': 'llama3'},\n",
       " 'rope_theta': 500000.0,\n",
       " 'tie_word_embeddings': True,\n",
       " 'torch_dtype': 'bfloat16',\n",
       " 'transformers_version': '4.45.0.dev0',\n",
       " 'use_cache': True,\n",
       " 'vocab_size': 128256}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(config_file, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_config_ref = None\n",
    "if TEST_WITH_REF:\n",
    "    llama_config_ref: LlamaConfigRef = LlamaModelRef.load_config(\n",
    "        config_file, \n",
    "        param_device=device,\n",
    "    )\n",
    "llama_config_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "********************   LlamaConfig   ********************\n",
       "activation_type: MLPActivationType.SILU\n",
       "apply_qk_norm: False\n",
       "causal: True\n",
       "eps: 1e-05\n",
       "ffh_size: 8192\n",
       "gate_init_mean: 0.0\n",
       "gate_init_std: 1.0\n",
       "group_size: None\n",
       "head_dim: 64\n",
       "hidden_size: 2048\n",
       "init_base_seed: 42\n",
       "lm_head_tied: True\n",
       "lora_alpha: None\n",
       "lora_dropout_rate: 0.0\n",
       "lora_dropout_seed: 42\n",
       "lora_init_base_seed: 42\n",
       "lora_rank: 0\n",
       "max_seq_len: 8192\n",
       "moe_topk: 1\n",
       "norm_init_range: (-1.0, 1.0)\n",
       "num_experts: None\n",
       "num_kv_head: 8\n",
       "num_layers: 16\n",
       "num_q_head: 32\n",
       "online_attn_block_size: None\n",
       "param_device: 'cpu'\n",
       "param_dtype: torch.bfloat16\n",
       "process_group: None\n",
       "proj_init_mean: 0.0\n",
       "proj_init_seed: 42\n",
       "proj_init_std: 1.0\n",
       "qk_norm_group_size: None\n",
       "qkv_layout: AttnQKVLayout.BSHD\n",
       "qkv_pack_format: AttnQKVPackFormat.Q_K_V\n",
       "rank: 0\n",
       "rope_base: 500000.0\n",
       "rope_dynamic: False\n",
       "rope_ratio: 1\n",
       "softmax_cap: None\n",
       "softmax_clip_range: (0.0, 1.0)\n",
       "softmax_dropout_rate: 0.0\n",
       "softmax_dropout_seed: 42\n",
       "softmax_scale: None\n",
       "softmax_temp: 1.0\n",
       "vocab_init_mean: 0.0\n",
       "vocab_init_std: 1.0\n",
       "vocab_size: 128256\n",
       "window_size: None\n",
       "world_size: 1\n",
       "*********************************************************"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_config: LlamaConfig = LlamaModel.load_config(\n",
    "    config_file, \n",
    "    param_device=device,\n",
    ")\n",
    "llama_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2. test LlamaModel loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "model.embed_tokens.weight torch.Size([128256, 2048]) torch.bfloat16\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.0.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.1.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.2.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.3.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.4.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.5.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.6.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.7.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.8.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.9.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.10.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.11.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.12.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.13.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.14.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([512, 2048]) torch.bfloat16\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([8192, 2048]) torch.bfloat16\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([2048, 8192]) torch.bfloat16\n",
      "model.layers.15.input_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([2048]) torch.bfloat16\n",
      "model.norm.weight torch.Size([2048]) torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "llama_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id\n",
    "\n",
    "llama_hf = LlamaForCausalLM.from_pretrained(model_dir, torch_dtype=llama_config.param_dtype).to(device)\n",
    "print(llama_hf)\n",
    "\n",
    "for name, param in llama_hf.named_parameters():\n",
    "    print(name, param.shape, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_WITH_REF:\n",
    "    llama_model_ref = LlamaModelRef(llama_config_ref)\n",
    "    llama_model_ref.load_parameters(params_files)\n",
    "    print(llama_model_ref)\n",
    "\n",
    "    for name, param in llama_model_ref.named_parameters():\n",
    "        print(name, param.shape, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaModel(\n",
      "  (block): TransformerDecoderBlock(\n",
      "    (vocab_emb): ParallelVocabEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x TransformerDecoderLayer(\n",
      "        (attn_pre_norm): GroupRMSNorm()\n",
      "        (rope): NTKAwareRoPE()\n",
      "        (attn): OfflineSlidingWindowAttn(\n",
      "          (softmax_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp_pre_norm): GroupRMSNorm()\n",
      "        (mlp): DenseMLPWithLoRA()\n",
      "      )\n",
      "    )\n",
      "    (kv_cache): TransformerDecoderKVCache()\n",
      "    (final_norm): GroupRMSNorm()\n",
      "    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      "  )\n",
      ")\n",
      "block.vocab_emb.weight torch.Size([128256, 2048]) torch.bfloat16\n",
      "block.layers.0.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.0.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.0.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.0.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.0.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.0.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.0.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.layers.1.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.1.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.1.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.1.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.1.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.1.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.1.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.layers.2.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.2.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.2.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.2.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.2.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.2.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.2.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.layers.3.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.3.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.3.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.3.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.3.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.3.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.3.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.layers.4.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.4.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.4.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.4.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.4.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.4.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.4.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.layers.5.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.5.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.5.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.5.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.5.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.5.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.5.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.layers.6.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.6.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.6.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.6.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.6.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.6.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.6.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.layers.7.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.7.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.7.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.7.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.7.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.7.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.7.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.layers.8.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.8.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.8.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.8.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.8.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.8.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.8.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.layers.9.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.9.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.9.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.9.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.9.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.9.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.9.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.layers.10.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.10.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.10.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.10.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.10.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.10.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.10.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.layers.11.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.11.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.11.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.11.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.11.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.11.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.11.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.layers.12.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.12.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.12.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.12.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.12.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.12.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.12.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.layers.13.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.13.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.13.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.13.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.13.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.13.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.13.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.layers.14.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.14.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.14.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.14.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.14.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.14.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.14.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.layers.15.qkv_proj torch.Size([2048, 3072]) torch.bfloat16\n",
      "block.layers.15.out_proj torch.Size([2048, 2048]) torch.bfloat16\n",
      "block.layers.15.attn_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.15.mlp_pre_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n",
      "block.layers.15.mlp.up_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.15.mlp.gate_proj torch.Size([2048, 8192]) torch.bfloat16\n",
      "block.layers.15.mlp.down_proj torch.Size([8192, 2048]) torch.bfloat16\n",
      "block.final_norm.weight torch.Size([1, 1, 1, 2048]) torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "llama_model = LlamaModel(llama_config)\n",
    "llama_model.load_parameters(params_files)\n",
    "print(llama_model)\n",
    "\n",
    "for name, param in llama_model.named_parameters():\n",
    "    print(name, param.shape, param.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3. test LlamaModel statistics APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "punit, munit = \"B\", \"GB\"\n",
    "pmap = {\n",
    "    \"B\": 1000**3,\n",
    "    \"M\": 1000**2,\n",
    "    \"K\": 1000,\n",
    "    \"1\": 1,\n",
    "}\n",
    "mmap = {\n",
    "    \"GB\": 1024**3,\n",
    "    \"MB\": 1024**2,\n",
    "    \"KB\": 1024,\n",
    "    \"1\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1.24 B\n",
      "Memory footprint: 2.30 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total parameters: {sum(p.numel() for p in llama_hf.parameters()) / pmap[punit]:.2f} {punit}\")\n",
    "print(f\"Memory footprint: {llama_hf.get_memory_footprint() / mmap[munit]:.2f} {munit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_WITH_REF:\n",
    "    total_params_b, memory_gb = llama_model_ref.num_parameters(unit=punit), llama_model_ref.num_memory_footprint(unit=munit)\n",
    "    print(f\"Total parameters: {total_params_b:.2f} {punit}\")\n",
    "    print(f\"Memory footprint: {memory_gb:.2f} {munit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1.24 B\n",
      "Memory footprint: 2.30 GB\n"
     ]
    }
   ],
   "source": [
    "total_params_b, memory_gb = llama_model.num_parameters(unit=punit), llama_model.num_memory_footprint(unit=munit)\n",
    "print(f\"Total parameters: {total_params_b:.2f} {punit}\")\n",
    "print(f\"Memory footprint: {memory_gb:.2f} {munit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step4. test LlamaModel forward in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6]),\n",
       " tensor([[128000,    791,   1401,    311,   2324,    374]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"The key to life is\"\n",
    "input_ids = llama_tokenizer(query, return_tensors=\"pt\").input_ids.to(device)\n",
    "input_ids.shape, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128256]),\n",
       " torch.bfloat16,\n",
       " tensor([[1.0133e-06, 6.7428e-07, 1.4901e-08,  ..., 1.3733e-10, 1.3733e-10,\n",
       "          1.3733e-10]], dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_hf.eval()\n",
    "with torch.no_grad():\n",
    "    outpu_hf = llama_hf.model(input_ids, return_dict=False)[0]\n",
    "    logits_hf = llama_hf.lm_head(outpu_hf)\n",
    "logits_hf = logits_hf[:, -1, :]\n",
    "probs_hf = F.softmax(logits_hf, dim=-1)\n",
    "\n",
    "probs_hf.shape, probs_hf.dtype, probs_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_WITH_REF:\n",
    "    llama_model_ref.eval()\n",
    "    llama_model_ref.reset_kv_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs_ref = llama_model_ref(input_ids)\n",
    "\n",
    "    print(probs_ref.shape, probs_ref.dtype, probs_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128256]),\n",
       " torch.bfloat16,\n",
       " tensor([[9.5367e-07, 6.3330e-07, 1.3795e-08,  ..., 1.3097e-10, 1.3097e-10,\n",
       "          1.3097e-10]], dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_model.eval()\n",
    "llama_model.reset_kv_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    probs = llama_model(input_ids)\n",
    "\n",
    "probs.shape, probs.dtype, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 126 / 128256 (0.1%)\n",
      "Greatest absolute difference: 0.02734375 at index (0, 539) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 0.12255859375 at index (0, 1202) (up to 0.016 allowed)\n"
     ]
    }
   ],
   "source": [
    "try: assert_close(probs, probs_hf)\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_WITH_REF:\n",
    "    try: assert_close(probs, probs_ref)\n",
    "    except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step5. test LlamaModel forward in training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9]),\n",
       " tensor([[128000,    791,   1401,    311,   2324,    374,    311,    387,   6380]]),\n",
       " torch.Size([1, 9]),\n",
       " tensor([[128000,    791,   1401,    311,   2324,    374,    311,    387,   6380]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"The key to life is to be happy\"\n",
    "input_ids = llama_tokenizer(query, return_tensors=\"pt\").input_ids.to(device)\n",
    "labels = input_ids.clone()\n",
    "input_ids.shape, input_ids, labels.shape, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3770, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_hf.train()\n",
    "loss_hf = llama_hf(input_ids, labels=labels).loss\n",
    "loss_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ref = None\n",
    "if TEST_WITH_REF:\n",
    "    llama_model_ref.train()\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        loss_ref = llama_model_ref(input_ids, labels=labels)\n",
    "\n",
    "loss_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3784, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_model.train()\n",
    "\n",
    "with torch.enable_grad():\n",
    "    loss = llama_model(input_ids, labels=labels)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
