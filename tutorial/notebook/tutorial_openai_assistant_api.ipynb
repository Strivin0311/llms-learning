{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial on OpenAI assistants API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is based on the previous one [here](https://blog.gopenai.com/openai-assistants-api-a-to-z-practitioners-guide-to-code-interpreter-knowledge-retrieval-and-33c1979c5d7d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial runs successfully on **openai==1.6.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step1.1 basic chat completion api with the openai models that have got the chat endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The dotenv library's load_dotenv function reads a .env file to load environment variables like \"OPENAI_API_KEY\" into the process environment. \n",
    "# This is a common method to handle configuration settings securely.\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# first of all, we have to construct a client with the own openai api_key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "# client = OpenAI() # NOTE: no need to set since it is in the environment already by load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "=> query from the user:\n",
      " hi, how are you doing ?\n",
      "=> response from gpt-3.5-turbo: \n",
      " Hello! As an AI, I don't have feelings, but I'm here to help you. How can I assist you today?\n",
      "==================================================\n",
      "=> query from the user:\n",
      " could you please give me some ideas to write a paper in the field of machine learning?\n",
      "=> response from gpt-3.5-turbo: \n",
      " Certainly! Here are some ideas for a paper in the field of machine learning:\n",
      "\n",
      "1. Explainable AI: Discuss the importance of interpretability in machine learning models and explore various techniques and approaches to make AI systems more transparent and understandable.\n",
      "\n",
      "2. Transfer Learning: Investigate the concept of transfer learning and its applications in different domains. Explore how pre-trained models can be utilized to improve performance on new tasks with limited data.\n",
      "\n",
      "3. Adversarial Attacks and Defenses: Explore the vulnerabilities of machine learning models to adversarial attacks and discuss different defense mechanisms to enhance robustness against such attacks.\n",
      "\n",
      "4. Reinforcement Learning: Provide an overview of reinforcement learning algorithms and their applications in solving complex problems. Discuss recent advancements and challenges in this area.\n",
      "\n",
      "5. Deep Learning Architectures: Compare and contrast different deep learning architectures such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers. Analyze their strengths, weaknesses, and applications.\n",
      "\n",
      "6. Generative Models: Investigate generative models like generative adversarial networks (GANs) and variational autoencoders (VAEs). Discuss their applications in image synthesis, text generation, and other creative domains.\n",
      "\n",
      "7. Bias and Fairness in Machine Learning: Examine the issue of bias in machine learning algorithms and its impact on decision-making processes. Discuss techniques to mitigate bias and ensure fairness in AI systems.\n",
      "\n",
      "8. Time Series Analysis: Explore machine learning techniques for analyzing and predicting time series data. Discuss applications in finance, weather forecasting, and other domains.\n",
      "\n",
      "9. Natural Language Processing: Discuss the challenges and advancements in natural language processing (NLP) and sentiment analysis. Explore techniques like word embeddings, attention mechanisms, and transformer models.\n",
      "\n",
      "10. Machine Learning in Healthcare: Investigate the applications of machine learning in healthcare, such as disease diagnosis, personalized medicine, and medical image analysis. Discuss the ethical considerations and challenges in implementing AI in healthcare settings.\n",
      "\n",
      "Remember to choose a topic that aligns with your interests and expertise. Good luck with your paper!\n",
      "==================================================\n",
      "=> query from the user:\n",
      " thx, helped a lot!\n",
      "=> response from gpt-3.5-turbo: \n",
      " You're welcome! I'm glad I could help. If you have any more questions, feel free to ask.\n",
      "==================================================\n",
      "=> Thanks, and good bye!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt-3.5-turbo'\n",
    "while True:\n",
    "    print(\"=\"*50)\n",
    "    prompt = input(\"=> Please input your prompt(input 'quit' to quit): \\n\")\n",
    "    if prompt == 'quit':\n",
    "        print(\"=> Thanks, and good bye!\")\n",
    "        break\n",
    "    print(f\"=> query from the user:\\n {prompt}\")\n",
    "    responses = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    response = responses.choices[0].message.content # fist choice message's text content\n",
    "    print(f\"=> response from {model_name}: \\n {response}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step2.1 use openai as a code interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the example code below with the file name `factorial.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def factorial(n):\n",
    "#     # Intentional bug: the base case for recursion is not correctly defined\n",
    "#     if n == 0:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return n * factorial(n - 1)\n",
    "\n",
    "# # Test the function\n",
    "# try:\n",
    "#     number = 5\n",
    "#     result = factorial(number)\n",
    "#     print(f\"The factorial of {number} is {result}\")\n",
    "# except RecursionError:\n",
    "#     print(\"Error: This function caused a recursion error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_file = client.files.create( # create a file handler to load and assign a file id for the assistant\n",
    "    file=open('./data/factorial.py', 'rb'),\n",
    "    purpose='assistants'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_assistant = client.beta.assistants.create(\n",
    "    name = \"Coding Assistant v1.0.0\",\n",
    "    instructions = \"You are a personal coding assistant. When asked a coding question, write and run code to answer the question.\",\n",
    "    model = \"gpt-4-1106-preview\",\n",
    "    tools = [{\"type\": \"code_interpreter\"}],\n",
    "    file_ids = [code_file.id]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asst_4oOlibiFwpKRoruSzXXyIRpj\n",
      "SyncCursorPage[AssistantFile](data=[AssistantFile(id='file-S1Aw9c4sqJMoJs6e2kMY26Lk', assistant_id='asst_4oOlibiFwpKRoruSzXXyIRpj', created_at=1703866580, object='assistant.file')], object='list', first_id='file-S1Aw9c4sqJMoJs6e2kMY26Lk', last_id='file-S1Aw9c4sqJMoJs6e2kMY26Lk', has_more=False)\n",
      "SyncCursorPage[Assistant](data=[Assistant(id='asst_4oOlibiFwpKRoruSzXXyIRpj', created_at=1703866579, description=None, file_ids=['file-S1Aw9c4sqJMoJs6e2kMY26Lk'], instructions='You are a personal coding assistant. When asked a coding question, write and run code to answer the question.', metadata={}, model='gpt-4-1106-preview', name='Coding Assistant v1.0.0', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')]), Assistant(id='asst_GZ8p7xT5KxuM9P9GgofqOxXo', created_at=1703866237, description=None, file_ids=[], instructions='You are a personal coding assistant.     When asked a coding question, write and run code to answer the question.', metadata={}, model='gpt-4-1106-preview', name='Coding Assistant 1.0.0', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')]), Assistant(id='asst_98YmfkrgzlDBDrSG5LaspFpO', created_at=1703865992, description=None, file_ids=[], instructions='You are a personal coding assistant.     When asked a coding question, write and run code to answer the question.', metadata={}, model='gpt-4-1106-preview', name='Coding Assistant 1.0.0', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')]), Assistant(id='asst_GqLzmdJx7fPhxMLQX7dgHlOu', created_at=1703865885, description=None, file_ids=[], instructions='You are a personal coding assistant.     When asked a coding question, write and run code to answer the question.', metadata={}, model='gpt-4-1106-preview', name='Coding Assistant 1.0.0', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')])], object='list', first_id='asst_4oOlibiFwpKRoruSzXXyIRpj', last_id='asst_GqLzmdJx7fPhxMLQX7dgHlOu', has_more=False)\n"
     ]
    }
   ],
   "source": [
    "print(code_assistant.id) # every assistant has a unique id\n",
    "print(client.beta.assistants.files.list(code_assistant.id)) # to list the files that the assistant can be aware of\n",
    "print(client.beta.assistants.list()) # to list existing Assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread_5vfe4VHb9HnY6QWI38O8q4ET\n"
     ]
    }
   ],
   "source": [
    "# to use the llm in a conversation session between an assistant and a user, you need to create a thread\n",
    "# threads store messages and automatically handle truncation to fit content into a model’s context.\n",
    "code_assist_thread = client.beta.threads.create(\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': \"What's wrong with my implementation of factorial function?\",\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(code_assist_thread.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you put all the messages as the context you need from your user in the thread, \n",
    "# you can create a run object from the thread with an assistant of your choice.\n",
    "# i.e. {a run} <- {a thread, an assistant}\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=code_assist_thread.id,\n",
    "    assistant_id=code_assistant.id,\n",
    "    instructions=\"Please address the user as Gunnar. The user has a premium account.\"\n",
    ") # When the run object is created, it will be in a `queued` status, and it will take some time to be completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_FixPkr9ADyvuammPboSilycj', assistant_id='asst_4oOlibiFwpKRoruSzXXyIRpj', content=[MessageContentText(text=Text(annotations=[], value='The contents of your file show a factorial function written in Python with a comment mentioning that there\\'s an \"intentional bug\" relating to the base case of recursion. The base case in your factorial function is `if n == 0: return 0`, which is incorrect. The correct base case for the factorial function should return 1 when n is 0, as the factorial of 0 (0!) is defined to be 1.\\n\\nHere\\'s the content of your `factorial` function with the bug:\\n\\n```python\\ndef factorial(n):\\n    # Intentional bug: the base case for recursion is not correctly defined\\n    if n == 0:\\n        return 0\\n    else:\\n        return n * factorial(n - 1)\\n\\n# Test the function\\ntry:\\n    number = 5\\n    result = factorial(number)\\n    print(f\"The factorial of {number} is {result}\")\\nexcept RecursionError:\\n    print(\"Error: This function caused a recursion error.\")\\n```\\n\\nThe line `if n == 0: return 0` should be `if n == 0: return 1` to correctly calculate the factorial.\\n\\nWould you like me to correct the base case and run the test again with the fixed function?'), type='text')], created_at=1703867582, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_O2DPoqfOfT8PMNUVXNdxAeww', thread_id='thread_5vfe4VHb9HnY6QWI38O8q4ET'), ThreadMessage(id='msg_p3TzdXotXF7GQFFj8x6oN752', assistant_id='asst_4oOlibiFwpKRoruSzXXyIRpj', content=[MessageContentText(text=Text(annotations=[], value=\"To help you assess what might be wrong with your implementation of the factorial function, let's take a look at the contents of the file you uploaded and try to execute the code (if it is indeed a Python file). I'll read the file first and display its contents.\"), type='text')], created_at=1703867571, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_O2DPoqfOfT8PMNUVXNdxAeww', thread_id='thread_5vfe4VHb9HnY6QWI38O8q4ET'), ThreadMessage(id='msg_B7dYlk8WsFpclEcO5XXH8XL7', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value=\"What's wrong with my implementation of factorial function?\"), type='text')], created_at=1703867344, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_5vfe4VHb9HnY6QWI38O8q4ET')], object='list', first_id='msg_FixPkr9ADyvuammPboSilycj', last_id='msg_B7dYlk8WsFpclEcO5XXH8XL7', has_more=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait a couple of seconds and then run, otherwise if you run it before the completion, you will see the message you just added\n",
    "# if completed, you can retrieve the response as well as the chat history from the stored messages in the thread\n",
    "\n",
    "code_assist_messages = client.beta.threads.messages.list(thread_id=code_assist_thread.id)\n",
    "code_assist_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThreadMessage(id='msg_FixPkr9ADyvuammPboSilycj', assistant_id='asst_4oOlibiFwpKRoruSzXXyIRpj', content=[MessageContentText(text=Text(annotations=[], value='The contents of your file show a factorial function written in Python with a comment mentioning that there\\'s an \"intentional bug\" relating to the base case of recursion. The base case in your factorial function is `if n == 0: return 0`, which is incorrect. The correct base case for the factorial function should return 1 when n is 0, as the factorial of 0 (0!) is defined to be 1.\\n\\nHere\\'s the content of your `factorial` function with the bug:\\n\\n```python\\ndef factorial(n):\\n    # Intentional bug: the base case for recursion is not correctly defined\\n    if n == 0:\\n        return 0\\n    else:\\n        return n * factorial(n - 1)\\n\\n# Test the function\\ntry:\\n    number = 5\\n    result = factorial(number)\\n    print(f\"The factorial of {number} is {result}\")\\nexcept RecursionError:\\n    print(\"Error: This function caused a recursion error.\")\\n```\\n\\nThe line `if n == 0: return 0` should be `if n == 0: return 1` to correctly calculate the factorial.\\n\\nWould you like me to correct the base case and run the test again with the fixed function?'), type='text')], created_at=1703867582, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_O2DPoqfOfT8PMNUVXNdxAeww', thread_id='thread_5vfe4VHb9HnY6QWI38O8q4ET')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_message = code_assist_messages.data[0] # NOTE: the message list works as a stack, so the index 0 corresponds to the latest one\n",
    "last_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contents of your file show a factorial function written in Python with a comment mentioning that there's an \"intentional bug\" relating to the base case of recursion. The base case in your factorial function is `if n == 0: return 0`, which is incorrect. The correct base case for the factorial function should return 1 when n is 0, as the factorial of 0 (0!) is defined to be 1.\n",
      "\n",
      "Here's the content of your `factorial` function with the bug:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    # Intentional bug: the base case for recursion is not correctly defined\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    else:\n",
      "        return n * factorial(n - 1)\n",
      "\n",
      "# Test the function\n",
      "try:\n",
      "    number = 5\n",
      "    result = factorial(number)\n",
      "    print(f\"The factorial of {number} is {result}\")\n",
      "except RecursionError:\n",
      "    print(\"Error: This function caused a recursion error.\")\n",
      "```\n",
      "\n",
      "The line `if n == 0: return 0` should be `if n == 0: return 1` to correctly calculate the factorial.\n",
      "\n",
      "Would you like me to correct the base case and run the test again with the fixed function?\n"
     ]
    }
   ],
   "source": [
    "response = last_message.content[0].text.value\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the code assistant's response, you can know that it has found the bug and offers a help to fix it\n",
    "# so continue to chat with it through appending new messages into the thread\n",
    "\n",
    "new_message = client.beta.threads.messages.create(\n",
    "    thread_id=code_assist_thread.id,\n",
    "    role='user',\n",
    "    content='Yes, please!'\n",
    ")\n",
    "\n",
    "# then run another call\n",
    "run = client.beta.threads.runs.create(thread_id=code_assist_thread.id, assistant_id=code_assistant.id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# to avoid repeated code, we create a function to retrieve the response, \n",
    "# which is the last message's content in one specific thread\n",
    "def retrieve_response(thread_id):\n",
    "    messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
    "    last_message = messages.data[0]\n",
    "    response = last_message.content[0].text.value\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The factorial function has been corrected to return 1 when n is 0, and the test ran successfully. The factorial of 5 is calculated to be 120, which is the correct result for 5!.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this will NOT modify the original code in the file, but only said so in the messages\n",
    "print(retrieve_response(code_assist_thread.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncCursorPage[RunStep](data=[RunStep(id='step_tuFgx5scJFK2pQcIhGEdiG8g', assistant_id='asst_4oOlibiFwpKRoruSzXXyIRpj', cancelled_at=None, completed_at=1703868029, created_at=1703868027, expired_at=None, failed_at=None, last_error=None, metadata=None, object='thread.run.step', run_id='run_rUcwrBczaKqxFklJhgGgZGe5', status='completed', step_details=MessageCreationStepDetails(message_creation=MessageCreation(message_id='msg_oTqVtfnswdRjWnTuEciEeHYB'), type='message_creation'), thread_id='thread_5vfe4VHb9HnY6QWI38O8q4ET', type='message_creation', expires_at=None), RunStep(id='step_E7krK1MXgxe1ExNzPjw36Y9w', assistant_id='asst_4oOlibiFwpKRoruSzXXyIRpj', cancelled_at=None, completed_at=1703868027, created_at=1703868016, expired_at=None, failed_at=None, last_error=None, metadata=None, object='thread.run.step', run_id='run_rUcwrBczaKqxFklJhgGgZGe5', status='completed', step_details=ToolCallsStepDetails(tool_calls=[CodeToolCall(id='call_t8nDsE7IDelu1N0lGJE5oyGa', code_interpreter=CodeInterpreter(input='# Correcting the implementation of the factorial function and running the test\\ndef factorial(n):\\n    # Corrected base case: the base case for recursion should return 1 when n is 0\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n - 1)\\n\\n# Test the function\\ntry:\\n    number = 5\\n    result = factorial(number)\\n    print(f\"The factorial of {number} is {result}\")\\nexcept RecursionError:\\n    print(\"Error: This function caused a recursion error.\")', outputs=[CodeInterpreterOutputLogs(logs='The factorial of 5 is 120\\n', type='logs')]), type='code_interpreter')], type='tool_calls'), thread_id='thread_5vfe4VHb9HnY6QWI38O8q4ET', type='tool_calls', expires_at=None)], object='list', first_id='step_tuFgx5scJFK2pQcIhGEdiG8g', last_id='step_E7krK1MXgxe1ExNzPjw36Y9w', has_more=False)\n"
     ]
    }
   ],
   "source": [
    "# to understand the reasoning steps during one specific run:\n",
    "run_steps = client.beta.threads.runs.steps.list(\n",
    "    thread_id=code_assist_thread.id,\n",
    "    run_id=run.id\n",
    ")\n",
    "run_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileDeleteResponse(id='file-S1Aw9c4sqJMoJs6e2kMY26Lk', deleted=True, object='assistant.file.deleted')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# once you complete the debugging task, you can delete the file binding to the assistant by providing file id\n",
    "file_del_status = client.beta.assistants.files.delete(\n",
    "    assistant_id=code_assistant.id,\n",
    "    file_id=code_file.id\n",
    ")\n",
    "file_del_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step3.1 use openai as a knowledge retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the paper pdf file used below can be downloaded [here](https://arxiv.org/pdf/2311.12351.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_file = client.files.create(\n",
    "    file=open('./data/2311.12351.pdf', 'rb'),\n",
    "    purpose='assistants'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_assistant = client.beta.assistants.create(\n",
    "    name = \"Research Assistant v1.0.0\",\n",
    "    instructions=\"You are a helpful research assistant. YOur role is to assist in navigating and understanding research papers from arxiv.\\\n",
    "    SUmmarize papers, clarify terminology within context, and extract key figures and data.\\\n",
    "    Cross-reference information for additional insights and answer related questions comprehensively.\\\n",
    "    Analyze the papers, noting strengths and limitations. Respond to queries effectively, incorporating feedback to enhance your accuracy.\\\n",
    "    Handle data securely and update your knowledge base with the latest research.\\\n",
    "    Adhere to ethical standards, respect intellectual property, and provide users with guidance on any limitations.\\\n",
    "    Maintain a feedback loop for continuous improvement and user support. YOur ultimate goal is to facilitate a deeper understanding of complex scientific material, making it more accessible and comprehensible.\",\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    tools = [{\"type\": \"retrieval\"}],\n",
    "    file_ids=[paper_file.id]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thread_wQLm7wssS7DYF11JT6WFxVQf'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_assist_thread = client.beta.threads.create(\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'Can you explain the NTK-based methods for extrapolative positional embeddings in the paper?',\n",
    "    }]\n",
    ")\n",
    "paper_assist_thread.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=paper_assist_thread.id,\n",
    "    assistant_id=paper_assistant.id,\n",
    "    instructions='Please address the user as Gunnar. The user has a premium account. And if there exist some equations, use $$ to wrap them'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def wait_for_run_completion(client, thread_id, run_id, sleep_interval=2):\n",
    "    import time\n",
    "    while True:\n",
    "        try:\n",
    "            run_status = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)\n",
    "            if run_status.completed_at: # NOTE: this flag is set when the asynchronous running process is done\n",
    "                print(\"Done!\")\n",
    "                break\n",
    "        except Exception as e: # noqa\n",
    "            print(f\"Error happened during the running process with the id {run_id} in the thread {thread_id}:\\n{e}\")\n",
    "            break\n",
    "        time.sleep(sleep_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "wait_for_run_completion(client, paper_assist_thread.id, run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In the paper you provided, the NTK-based methods for extrapolative positional embeddings refer to several strategies inspired by the Neural Tangent Kernel theory (NTK) which aim to enhance the extrapolation capabilities of positional encoding schemes in Transformer-based large language models (LLMs). Here is a summary of the various methods and their core concepts mentioned in the paper:\n",
       "\n",
       "1. **NTK-aware Scaling RoPE (NTK-RoPE)**: This approach addresses the difficulty that deep neural networks have in learning high-frequency information when input dimensions are low and lack high-frequency components. It includes a nonlinear scaling strategy that interpolates low-frequency terms more than high-frequency ones, using a coefficient \\( c_{\\kappa} \\) associated with a scaling factor \\( \\kappa \\). This is reflected in the following equation:\n",
       "   \\[\n",
       "   \\beta := c_{\\kappa} \\cdot \\beta \\quad \\text{s.t.} \\quad \\frac{n}{\\beta_{d/2-1}} = \\frac{n/\\kappa}{\\beta_{d/2-1}} \\quad \\Rightarrow \\quad c_{\\kappa} = \\kappa^{2/(d-2)}\n",
       "   \\]\n",
       "   This approach can be applied directly to LLMs that have been pre-trained with RoPE without additional fine-tuning.\n",
       "\n",
       "2. **Dynamic-NTK**: It delays scaling until the context length \\( L \\) exceeds the currently supported length, and then dynamically increases the scaling ratio \\( \\kappa \\) as \\( L \\) continues to grow. This incremental scaling helps avoid performance degradation within the original \"max length.\"\n",
       "\n",
       "3. **NTK-mix RoPE**: It introduces multiple coefficients for the scaling factor, with the coefficients decreasing as the frequency increases. This allows lesser interpolation for higher frequency dimensions.\n",
       "\n",
       "4. **NTK-by-parts**: This method scales lower frequency dimensions always but does not interpolate higher frequency dimensions at all, countering the issue that might arise from the NTK-mix RoPE strategy.\n",
       "\n",
       "5. **YaRN**: This model combines the NTK-by-parts strategy with a \"length scaling\" trick that scales query and key matrices by a constant temperature factor \\( t \\). It claims to outperform previous methods whether or not they were fine-tuned.\n",
       "\n",
       "6. **Power Scaling**: Described as another scaling strategy where the exponent \\( \\kappa > 0 \\) controls the decay ratio of low frequencies, making sure high-frequency elements are less affected than low-frequency elements, which are not well-learned.\n",
       "\n",
       "These methods stem from a common observation that simple extrapolation and scaling strategies might be suboptimal because they could compress distances between tokens excessively, hindering the model's ability to distinguish the order and relative positions of tokens in certain contexts. The NTK-inspired methods aim to provide a more nuanced scaling that considers the different importance and difficulty of learning across frequency bands.\n",
       "\n",
       "The paper suggests that these extrapolative strategies based on NTK theory for positional embeddings contribute to more efficient and effective length extrapolation in Transformers, allowing them to handle longer sequence contexts without losing performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieve_response(paper_assist_thread.id)\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4.1 use openai to call json-like functions without actual codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_assistant = client.beta.assistants.create(\n",
    "    name=\"Weather Bot v1.0.0\",\n",
    "    instructions=\"You are a weather bot. Use the provided functions to answer questions about the weather.\",\n",
    "    model='gpt-4-1106-preview',\n",
    "    tools=[{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather for a given city\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\", \n",
    "                        \"description\": \"The name of the city, e.g. Beijing, Paris, Los Angeles or San Francisco\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"c\", \"f\"], \"description\": \"The unit of the temperature\"},\n",
    "                },\n",
    "                \"required\": [\"city\"],\n",
    "            }\n",
    "        }\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thread_dqVSOKugbTw7KugyLet4Hp18'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_assist_thread = client.beta.threads.create(\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': \"What's the weather like in Barcelona in Celsius?\",\n",
    "    }]\n",
    ")\n",
    "weather_assist_thread.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=weather_assist_thread.id,\n",
    "    assistant_id=weather_assistant.id,\n",
    "    instructions=\"Please address the user as Gunnar. The user has a premium account.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_axh4Hs8aEMgROGFBg6TlYezS', assistant_id='asst_PJ21tsPyLhzGTCG0Pgx6EpM4', cancelled_at=None, completed_at=None, created_at=1703912976, expires_at=1703913576, failed_at=None, file_ids=[], instructions='Please address the user as Gunnar. The user has a premium account.', last_error=None, metadata={}, model='gpt-4-1106-preview', object='thread.run', required_action=RequiredAction(submit_tool_outputs=RequiredActionSubmitToolOutputs(tool_calls=[RequiredActionFunctionToolCall(id='call_RqnMGbeFW1xYZguRfBWFPw4a', function=Function(arguments='{\"city\":\"Barcelona\",\"unit\":\"c\"}', name='get_current_weather'), type='function')]), type='submit_tool_outputs'), started_at=1703912976, status='requires_action', thread_id='thread_dqVSOKugbTw7KugyLet4Hp18', tools=[ToolAssistantToolsFunction(function=FunctionDefinition(name='get_current_weather', description='Get the current weather for a given city', parameters={'type': 'object', 'properties': {'city': {'type': 'string', 'description': 'The name of the city, e.g. Beijing, Paris, Los Angeles or San Francisco'}, 'unit': {'type': 'string', 'enum': ['c', 'f'], 'description': 'The unit of the temperature'}}, 'required': ['city']}), type='function')])\n"
     ]
    }
   ],
   "source": [
    "# NOTE: do NOT wait until if is completed_at, since the running process will enter a 'required_action' state\n",
    "# to wait for you to run again to call the tool it has found \n",
    "run_status = client.beta.threads.runs.retrieve(thread_id=weather_assist_thread.id, run_id=run.id)\n",
    "print(run_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('call_RqnMGbeFW1xYZguRfBWFPw4a', '{\"city\":\"Barcelona\",\"unit\":\"c\"}')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the tool call id and the arguments out\n",
    "tool_call_id = run_status.required_action.submit_tool_outputs.dict()['tool_calls'][0]['id']\n",
    "arguments = run_status.required_action.submit_tool_outputs.dict()['tool_calls'][0]['function']['arguments']\n",
    "tool_call_id, arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.submit_tool_outputs(\n",
    "    thread_id=weather_assist_thread.id,\n",
    "    run_id=run.id,\n",
    "    tool_outputs=[{\n",
    "        'tool_call_id': tool_call_id,\n",
    "        'output': arguments\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "wait_for_run_completion(client, weather_assist_thread.id, run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Barcelona, in Celsius, is as follows:\n",
      "\n",
      "- Temperature: 16°C\n",
      "- Weather description: Few clouds\n",
      "- Wind speed: 3.6 km/h\n",
      "- Wind direction: 30 degrees\n",
      "- Humidity: 77%\n",
      "- Pressure: 1020 hPa\n",
      "\n",
      "Please note that weather conditions can change rapidly, so it's always a good idea to check for the most up-to-date information before making any plans that depend on the weather.\n"
     ]
    }
   ],
   "source": [
    "print(retrieve_response(weather_assist_thread.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
