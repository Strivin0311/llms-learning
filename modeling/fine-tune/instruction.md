# Instruction Following Fine-Tuning (IFT)
*Here're some resources about Instruction Following Fine-Tuning (IFT) for LLMs*


#### Self-QA: Unsupervised Knowledge Guided Language Model Alignment [`READ`]

paper link: [here](https://arxiv.org/pdf/2305.11952)

citation: 
```bibtex
@article{zhang2023self,
  title={Self-QA: Unsupervised Knowledge Guided Language Model Alignment},
  author={Zhang, Xuanyu and Yang, Qing},
  journal={arXiv preprint arXiv:2305.11952},
  year={2023}
}
```
    
    

#### Principle-driven self-alignment of language models from scratch with minimal human supervision [`READ`]

paper link: [here](https://arxiv.org/pdf/2305.03047)

citation: 
```bibtex
@article{sun2023principle,
  title={Principle-driven self-alignment of language models from scratch with minimal human supervision},
  author={Sun, Zhiqing and Shen, Yikang and Zhou, Qinhong and Zhang, Hongxin and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  journal={arXiv preprint arXiv:2305.03047},
  year={2023}
}
```

#### WizardLM: Empowering Large Language Models to Follow Complex Instructions [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2304.12244.pdf)

github link: [here](https://github.com/nlpxucan/WizardLM)

model links: 

|model name|link|
|-|-|
|WizardLM-7B-V1.0|[here](https://huggingface.co/WizardLM/WizardLM-7B-V1.0)|
|WizardLM-13B-V1.1|[here](https://huggingface.co/WizardLM/WizardLM-13B-V1.1)|
|WizardLM-13B-V1.2|[here](https://huggingface.co/WizardLM/WizardLM-13B-V1.2)|
|WizardLM-70B-V1.0|[here](https://huggingface.co/WizardLM/WizardLM-70B-V1.0)|

citation:
```bibtex
@misc{xu2023wizardlm,
      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, 
      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
      year={2023},
      eprint={2304.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

#### Visual instruction tuning [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2304.08485)

citation: 
```bibtex
@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}
```
    


#### Baize: An open-source chat model with parameter-efficient tuning on self-chat data [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2304.01196)

citation: 
```bibtex
@article{xu2023baize,
  title={Baize: An open-source chat model with parameter-efficient tuning on self-chat data},
  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  journal={arXiv preprint arXiv:2304.01196},
  year={2023}
}
```


#### Self-Prompting Large Language Models for Zero-Shot Open-Domain QA ['READ']

paper link: [here](https://arxiv.org/pdf/2212.08635.pdf)

citation: 
```bibtex
@misc{li2023selfprompting,
  title={Self-Prompting Large Language Models for Zero-Shot Open-Domain QA}, 
  author={Junlong Li and Zhuosheng Zhang and Hai Zhao},
  year={2023},
  eprint={2212.08635},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
```


#### Self-instruct: Aligning language model with self generated instructions [`READ`]

paper link: [here](https://arxiv.org/pdf/2212.10560)

citation: 
```bibtex
@article{wang2022self,
  title={Self-instruct: Aligning language model with self generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}
```

#### Training language models to follow instructions with human feedback (InstructGPT) [`READ`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)

citation: 
```bibtex
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
```


#### Scaling Instruction-Finetuned Language Models (FLAN-T5, FLAN-PaLM) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2210.11416.pdf)

citation: 
```bibtex
@misc{chung2022scaling,
      title={Scaling Instruction-Finetuned Language Models}, 
      author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
      year={2022},
      eprint={2210.11416},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Finetuned language models are zero-shot learners (FLAN) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2109.01652)

citation: 
```bibtex
@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}
```
    

