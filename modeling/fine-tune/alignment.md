# Alignment
*Here's some resources about Alignment of LLMs, especically to ethics, cultures, politicis, laws in human society*


#### FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained Models in Few-Shot Learning [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2310.15105)

citation: 
```bibtex
@article{song2023fd,
  title={FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained Models in Few-Shot Learning},
  author={Song, Kun and Ma, Huimin and Zou, Bochao and Zhang, Huishuai and Huang, Weiran},
  journal={arXiv preprint arXiv:2310.15105},
  year={2023}
}
```
    


#### Direct preference optimization: Your language model is secretly a reward model (DPO) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2305.18290.pdf)

citation: 
```bibtex
@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}
```

#### Lima: Less is more for alignment [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2305.11206)

citation: 
```bibtex
@article{zhou2023lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={arXiv preprint arXiv:2305.11206},
  year={2023}
}
```
    

#### RRHF: Rank responses to align language models with human feedback without tears [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2304.05302)

citation: 
```bibtex
@article{yuan2023rrhf,
  title={Rrhf: Rank responses to align language models with human feedback without tears},
  author={Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei},
  journal={arXiv preprint arXiv:2304.05302},
  year={2023}
}
```

#### Openagi: When llm meets domain experts (RLTF) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2304.04370.pdf)

citation: 
```bibtex
@article{ge2023openagi,
  title={Openagi: When llm meets domain experts},
  author={Ge, Yingqiang and Hua, Wenyue and Ji, Jianchao and Tan, Juntao and Xu, Shuyuan and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2304.04370},
  year={2023}
}
```
    



#### Illustrating Reinforcement Learning from Human Feedback

blog link: [here](https://huggingface.co/blog/rlhf)

citation:
```bibtex
@article{lambert2022illustrating,
  title={Illustrating reinforcement learning from human feedback (RLHF)},
  author={Lambert, Nathan and von Werra, L},
  journal={Hugging Face Blog},
  year={2022}
  url={https://huggingface.co/blog/rlhf}
}
```


#### Fine-tuning language models from human preferences (RLHF) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1909.08593.pdf)

citation: 
```bibtex
@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}
```
       

#### Constitutional ai: Harmlessness from ai feedback (RLAIF) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2212.08073.pdf?trk=public_post_comment-text)

citation: 
```bibtex
@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}
```
    







