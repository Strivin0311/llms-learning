
# Instruction Following
*Here's some resources about Instruction Following for LLMs*


#### Toolllm: Facilitating large language models to master 16000+ real-world apis [`READ`]

paper link: [here](https://arxiv.org/pdf/2307.16789.pdf)

citation: 
```bibtex
@article{qin2023toolllm,
  title={Toolllm: Facilitating large language models to master 16000+ real-world apis},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others},
  journal={arXiv preprint arXiv:2307.16789},
  year={2023}
}
```

#### Self-QA: Unsupervised Knowledge Guided Language Model Alignment [`READ`]

paper link: [here](https://arxiv.org/pdf/2305.11952)

citation: 
```bibtex
@article{zhang2023self,
  title={Self-QA: Unsupervised Knowledge Guided Language Model Alignment},
  author={Zhang, Xuanyu and Yang, Qing},
  journal={arXiv preprint arXiv:2305.11952},
  year={2023}
}
```
    
    

#### Principle-driven self-alignment of language models from scratch with minimal human supervision [`READ`]

paper link: [here](https://arxiv.org/pdf/2305.03047)

citation: 
```bibtex
@article{sun2023principle,
  title={Principle-driven self-alignment of language models from scratch with minimal human supervision},
  author={Sun, Zhiqing and Shen, Yikang and Zhou, Qinhong and Zhang, Hongxin and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  journal={arXiv preprint arXiv:2305.03047},
  year={2023}
}
```




#### Baize: An open-source chat model with parameter-efficient tuning on self-chat data [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2304.01196)

citation: 
```bibtex
@article{xu2023baize,
  title={Baize: An open-source chat model with parameter-efficient tuning on self-chat data},
  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  journal={arXiv preprint arXiv:2304.01196},
  year={2023}
}
```


#### Self-Prompting Large Language Models for Zero-Shot Open-Domain QA ['READ']

paper link: [here](https://arxiv.org/pdf/2212.08635.pdf)

citation: 
```bibtex
@misc{li2023selfprompting,
  title={Self-Prompting Large Language Models for Zero-Shot Open-Domain QA}, 
  author={Junlong Li and Zhuosheng Zhang and Hai Zhao},
  year={2023},
  eprint={2212.08635},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
```


#### Self-instruct: Aligning language model with self generated instructions [`READ`]

paper link: [here](https://arxiv.org/pdf/2212.10560)

citation: 
```bibtex
@article{wang2022self,
  title={Self-instruct: Aligning language model with self generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}
```

#### Training language models to follow instructions with human feedback [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)

citation: 
```bibtex
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
```


#### Finetuned language models are zero-shot learners (FLAN) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2109.01652)

citation: 
```bibtex
@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}
```
    

