# Alignment
*Here's some resources about Alignment of LLMs, especically to ethics, cultures, politicis, laws in human society*


#### Direct preference optimization: Your language model is secretly a reward model [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2305.18290.pdf?trk=public_post_comment-text)

citation: 
```bibtex
@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}
```


#### Fine-tuning language models from human preferences [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1909.08593.pdf)

citation: 
```bibtex
@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}
```
       



#### Constitutional ai: Harmlessness from ai feedback [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2212.08073.pdf?trk=public_post_comment-text)

citation: 
```bibtex
@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}
```
    







