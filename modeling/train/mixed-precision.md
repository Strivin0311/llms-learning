# Mixed-Precision Training
*Here're some resources about Mixed-Precision strategies for LLMs training*


#### FP8-LM: Training FP8 Large Language Models [`READ`]

paper link: [here](https://arxiv.org/pdf/2310.18313.pdf)

github link: [here](https://github.com/Azure/MS-AMP)

citation:
```bibtex
@misc{peng2023fp8lm,
      title={FP8-LM: Training FP8 Large Language Models}, 
      author={Houwen Peng and Kan Wu and Yixuan Wei and Guoshuai Zhao and Yuxiang Yang and Ze Liu and Yifan Xiong and Ziyue Yang and Bolin Ni and Jingcheng Hu and Ruihang Li and Miaosen Zhang and Chen Li and Jia Ning and Ruizhe Wang and Zheng Zhang and Shuguang Liu and Joe Chau and Han Hu and Peng Cheng},
      year={2023},
      eprint={2310.18313},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### BitNet: Scaling 1-bit Transformers for Large Language Models [`READ`]

paper link: [here](https://arxiv.org/pdf/2310.11453)

blog link: [here](https://thegenerality.com/agi/)

github link: [here](https://github.com/microsoft/unilm)

citation:
```bibtex
@misc{wang2023bitnet,
      title={BitNet: Scaling 1-bit Transformers for Large Language Models}, 
      author={Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei},
      year={2023},
      eprint={2310.11453},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### Training and inference of large language models using 8-bit floating point [`READ`]

paper link: [here](https://arxiv.org/pdf/2309.17224)

citation:
```bibtex
@misc{perez2023training,
      title={Training and inference of large language models using 8-bit floating point}, 
      author={Sergio P. Perez and Yan Zhang and James Briggs and Charlie Blake and Josh Levy-Kramer and Paul Balanca and Carlo Luschi and Stephen Barlow and Andrew William Fitzgibbon},
      year={2023},
      eprint={2309.17224},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### NVIDIA Transformer Engine: Accelerating PyTorch Training Workloads with FP8 (TE) [`READ`]

blog link: [here](https://towardsdatascience.com/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7)

docs link: [here](https://docs.nvidia.com/deeplearning/transformer-engine/index.html)

github link: [here](https://github.com/NVIDIA/TransformerEngine)

citation:
```bibtex
@misc{NVIDIA2023TransformerEngine,
  title={NVIDIA Transformer Engine: Accelerating PyTorch Training Workloads with FP8 (TE)},
  author={Chaim Rand, and NVIDIA},
  howpublished = {\url{https://github.com/NVIDIA/TransformerEngine}},
  year={2023},
}
```


#### 8-bit Optimizers via Block-wise Quantization [`READ`]

paper link: [here](https://arxiv.org/pdf/2110.02861)

citation:
```bibtex
@misc{dettmers20228bit,
      title={8-bit Optimizers via Block-wise Quantization}, 
      author={Tim Dettmers and Mike Lewis and Sam Shleifer and Luke Zettlemoyer},
      year={2022},
      eprint={2110.02861},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks (HFP8) [`READ`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2019/file/65fc9fb4897a89789352e211ca2d398f-Paper.pdf)

citation:
```bibtex
@inproceedings{NEURIPS2019_65fc9fb4,
      author = {Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi (Viji) and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
      pages = {},
      publisher = {Curran Associates, Inc.},
      title = {Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks},
      url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/65fc9fb4897a89789352e211ca2d398f-Paper.pdf},
      volume = {32},
      year = {2019}
}  
```


#### Training Deep Neural Networks with 8-bit Floating Point Numbers (FP8) [`READ`]

paper link: [here](https://arxiv.org/pdf/1812.08011)

citation:
```bibtex
@misc{wang2018training,
      title={Training Deep Neural Networks with 8-bit Floating Point Numbers}, 
      author={Naigang Wang and Jungwook Choi and Daniel Brand and Chia-Yu Chen and Kailash Gopalakrishnan},
      year={2018},
      eprint={1812.08011},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### Mixed Precision Training (FP16) [`READ`]

paper link: [here](https://arxiv.org/pdf/1710.03740)

citation:
```bibtex
@misc{micikevicius2018mixed,
      title={Mixed Precision Training}, 
      author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
      year={2018},
      eprint={1710.03740},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
```