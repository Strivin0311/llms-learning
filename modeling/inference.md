# Inference on LLMs
*Here's some resources about Inference on LLMs*


#### AirLLM: inference 70B LLM with 4GB single GPU [`READ`]

github link: [here](https://github.com/lyogavin/Anima/tree/main/air_llm)

tutorial links:

|tutorial name|public date|main-lib version|notebook link|
|-|-|-|-|
|tutorial_airllm|2024.01|airllm=2.8.3|[here](../notebooks/tutorial_airllm.ipynb)|


#### PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU [`UNREAD`]

paper link: [here](https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf)

citation:
```bibtex
@misc{song2023powerinfer,
      title={PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU}, 
      author={Yixin Song and Zeyu Mi and Haotong Xie and Haibo Chen},
      year={2023},
      eprint={2312.12456},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


#### High-throughput generative inference of large language models with a single gpu [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2303.06865)

citation: 
```bibtex
@article{sheng2023high,
  title={High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Fu, Daniel Y and Xie, Zhiqiang and Chen, Beidi and Barrett, Clark and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2303.06865},
  year={2023}
}
```


#### Fast transformer decoding: One write-head is all you need [`READ`]

paper link: [here](https://arxiv.org/pdf/1911.02150.pdf?trk=public_post_comment-text)

citation: 
```bibtex
@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}
```
    
    