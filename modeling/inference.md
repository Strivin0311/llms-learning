# Inference on LLMs
*Here's some resources about Inference on LLMs*


#### Fast Inference of Mixture-of-Experts Language Models with Offloading [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2312.17238.pdf)

github link: [here](https://github.com/dvmazur/mixtral-offloading)

citation: 
```bibtex
@misc{eliseev2023fast,
      title={Fast Inference of Mixture-of-Experts Language Models with Offloading}, 
      author={Artyom Eliseev and Denis Mazur},
      year={2023},
      eprint={2312.17238},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU [`UNREAD`]

paper link: [here](https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf)

citation:
```bibtex
@misc{song2023powerinfer,
      title={PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU}, 
      author={Yixin Song and Zeyu Mi and Haotong Xie and Haibo Chen},
      year={2023},
      eprint={2312.12456},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### AirLLM: inference 70B LLM with 4GB single GPU [`READ`]

github link: [here](https://github.com/lyogavin/Anima/tree/main/air_llm)

tutorial links:

|tutorial name|public date|main-lib version|notebook link|
|-|-|-|-|
|tutorial_airllm|2024.01|airllm=2.8.3|[here](../notebooks/tutorial_airllm.ipynb)|

citation
```bibtex
@software{airllm2023,
  author = {Gavin Li},
  title = {AirLLM: scaling large language models on low-end commodity computers},
  url = {https://github.com/lyogavin/Anima/tree/main/air_llm},
  version = {0.0},
  year = {2023},
}
```


#### High-throughput generative inference of large language models with a single gpu [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2303.06865)

citation: 
```bibtex
@article{sheng2023high,
  title={High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Fu, Daniel Y and Xie, Zhiqiang and Chen, Beidi and Barrett, Clark and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2303.06865},
  year={2023}
}
```


#### A BetterTransformer for Fast Transformer Inference [`READ`]

blog link: [here](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)

homepage link: [here](https://huggingface.co/docs/optimum/bettertransformer/overview)

tutorial link: [here](../notebooks/BetterTransformerDemo.ipynb)

supported model list link: [here](https://huggingface.co/docs/optimum/bettertransformer/overview#supported-models)

citation:
```bibtex
@online{bettertransformer,
  author = {Michael Gschwind, Eric Han, Scott Wolchok, Rui Zhu, Christian Puhrsch},
  title = {A Better Transformer for Fast Transformer Inference},
  year = {2022},
  month = {July},
  url = {\url{https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/}}
}
```


#### Fast transformer decoding: One write-head is all you need [`READ`]

paper link: [here](https://arxiv.org/pdf/1911.02150.pdf?trk=public_post_comment-text)

citation: 
```bibtex
@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}
```
    
    