# Inference on LLMs
*Here's some resources about Inference on LLMs*



#### High-throughput generative inference of large language models with a single gpu [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2303.06865)

citation: 
```bibtex
@article{sheng2023high,
  title={High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Fu, Daniel Y and Xie, Zhiqiang and Chen, Beidi and Barrett, Clark and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2303.06865},
  year={2023}
}
```


#### Fast transformer decoding: One write-head is all you need [`READ`]

paper link: [here](https://arxiv.org/pdf/1911.02150.pdf?trk=public_post_comment-text)

citation: 
```bibtex
@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}
```
    
    