# Mixture-of-Experts (MoE)
*Here's some resources about Mixture-of-Experts (MoE)*


#### Fast Inference of Mixture-of-Experts Language Models with Offloading [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2312.17238.pdf)

github link: [here](https://github.com/dvmazur/mixtral-offloading)

citation: 
```bibtex
@misc{eliseev2023fast,
      title={Fast Inference of Mixture-of-Experts Language Models with Offloading}, 
      author={Artyom Eliseev and Denis Mazur},
      year={2023},
      eprint={2312.17238},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### Mixtral of experts: A high quality Sparse Mixture-of-Experts [`UNREAD`]

blog link: [here](https://mistral.ai/news/mixtral-of-experts/)

model links: 

|model name|link|
|-|-|
|Mixtral-SlimOrca-8x7B|[here](https://huggingface.co/Open-Orca/Mixtral-SlimOrca-8x7B)|
|Mixtral-8x7B-Instruct-v0.1|[here](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)|
|Mixtral-8x7B-v0.1|[here](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)|

citation:
```bibtex
@online{mixtral_model,
  author = {Mistral AI},
  title = {Mixtral of Experts: A High-Quality Sparse Mixture-of-Experts},
  year = {2023},
  url = {\url{https://mistral.ai/news/mixtral-of-experts/}}
}
```


#### QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2310.16795.pdf)

citation:
```bibtex
@misc{frantar2023qmoe,
      title={QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models}, 
      author={Elias Frantar and Dan Alistarh},
      year={2023},
      eprint={2310.16795},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### OpenMoE: A family of open-sourced Mixture-of-Experts (MoE) Large Language Models [`READ`]

blog link: [here](https://xuefuzhao.notion.site/Aug-2023-OpenMoE-v0-2-Release-43808efc0f5845caa788f2db52021879)

github link: [here](https://github.com/XueFuzhao/OpenMoE)

citation:
```bibtex
@misc{openmoe2023,
  author = {Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou and Yang You},
  title = {OpenMoE: Open Mixture-of-Experts Language Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/XueFuzhao/OpenMoE}},
}
```


#### AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts [`UNREAD`]

paper link: [here](http://openaccess.thecvf.com/content/ICCV2023/papers/Chen_AdaMV-MoE_Adaptive_Multi-Task_Vision_Mixture-of-Experts_ICCV_2023_paper.pdf)

citation:
```bibtex
@inproceedings{chen2023adamv,
  title={AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts},
  author={Chen, Tianlong and Chen, Xuxi and Du, Xianzhi and Rashwan, Abdullah and Yang, Fan and Chen, Huizhong and Wang, Zhangyang and Li, Yeqing},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={17346--17357},
  year={2023}
}
```

#### MegaBlocks: Efficient Sparse Training with Mixture-of-Experts (dMoE) [`READ`]

paper link: [here](https://arxiv.org/pdf/2211.15841.pdf)

github link: [here](https://github.com/stanford-futuredata/megablocks)

citation:
```bibtex
@misc{gale2022megablocks,
      title={MegaBlocks: Efficient Sparse Training with Mixture-of-Experts}, 
      author={Trevor Gale and Deepak Narayanan and Cliff Young and Matei Zaharia},
      year={2022},
      eprint={2211.15841},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2204.07689.pdf)

citation:
```bibtex
@misc{gupta2022sparsely,
      title={Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners}, 
      author={Shashank Gupta and Subhabrata Mukherjee and Krishan Subudhi and Eduardo Gonzalez and Damien Jose and Ahmed H. Awadallah and Jianfeng Gao},
      year={2022},
      eprint={2204.07689},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

#### Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity (SMoE) [`READ`]

paper link: [here](https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf)

citation:
```bibtex
@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022},
  publisher={JMLRORG}
}
```

#### Gshard: Scaling giant models with conditional computation and automatic sharding [`READ`]

paper link: [here](https://arxiv.org/pdf/2006.16668.pdf)

citation:
```bibtex
@article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}
```

#### Outrageously large neural networks: The sparsely-gated mixture-of-experts layer [`READ`]

paper link: [here](https://arxiv.org/pdf/1701.06538.pdf)

citation:
```bibtex
@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}
```

#### Adaptive Mixture of Local Experts [`UNREAD`]

paper link: [here](http://www.cs.utoronto.ca/~hinton/absps/jjnh91.ps)

citation:
```bibtex
@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}
```